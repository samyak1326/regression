{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1_Sa7Fl2cjD"
      },
      "source": [
        "# CS 584 :: Data Mining :: George Mason University :: Spring 2024\n",
        "\n",
        "\n",
        "# Homework 2: Linear Regression&Neural Networks\n",
        "\n",
        "- **100 points [8% of your final grade]**\n",
        "- **Due Sunday, March 10 by 11:59pm**\n",
        "\n",
        "- *Goals of this homework:* (1) implement the linear regression model; (2) implement the multi-layer perceptron neural network; (3) tune the hyperparameters of MLP model to produce classification result as good as possible.\n",
        "\n",
        "- *Submission instructions:* for this homework, you need to submit to two different platforms. First, you should submit your notebook file to Blackboard (look for the homework 2 assignment there). Please name your submission **FirstName_Lastname_hw2.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw2.ipynb**. Your notebook should be **fully executed** so that we can see all outputs. Then, you need to submit a output file from this notebook (you will see later in this notebook) to the HW2 page in the http://miner2.vsnet.gmu.edu website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpvBEEA22cjF"
      },
      "source": [
        "## Part 1: Linear Regression (40 points)\n",
        "\n",
        "Recent studies have found that novel mobile games can lead to increased physical activity. A notable example is Pokemon Go, a mobile game combining the Pokemon world through augmented reality with the real world requiring players to physically move around. Specifically, in the following study, researchers have found that Pokemon Go leads to increased levels of physical activity for the most engaged players! https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5174727/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJJiIkyn2cjF"
      },
      "source": [
        "In this part, our goal is to predict the combat point of each pokemon in the 2017 Pokemon Go mobile game. Each pokemon has its own unique attributes that can help predicting its combat points. These include:\n",
        "\n",
        "- Stamina\n",
        "- Attack value\n",
        "- Defense value\n",
        "- Capture rate\n",
        "- Flee rate\n",
        "- Spawn chance\n",
        "- Primary strength\n",
        "\n",
        "The file pokemon_data.csv contains data of 146 pokemons to be used in this homework. The rows of these files refer to the data samples (i.e., pokemon samples), while the columns denote the name of the pokemon (column 1), its attributes (columns 2-8), and the combat point outcome (column 9). You can ignore column 1 for the rest of this problem.\n",
        "\n",
        "First, let's load the data by excuting the following code.\n",
        "\n",
        "**Note: you need to install the pandas library beforehand**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x7m_w8q4bcj",
        "outputId": "b8f0a554-5e0f-404f-db45-1d908d6aa149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "YtmKFgnh2cjF",
        "outputId": "bab49afd-d9d2-42bf-8afa-461d0968b673"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"data_frame\",\n  \"rows\": 146,\n  \"fields\": [\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 145,\n        \"samples\": [\n          \"Victreebel\",\n          \"Aerodactyl\",\n          \"Sandslash\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stamina\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56,\n        \"min\": 20,\n        \"max\": 500,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          180,\n          70,\n          110\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attack_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40,\n        \"min\": 40,\n        \"max\": 250,\n        \"num_unique_values\": 67,\n        \"samples\": [\n          134,\n          102,\n          160\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"defense_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37,\n        \"min\": 54,\n        \"max\": 222,\n        \"num_unique_values\": 65,\n        \"samples\": [\n          168,\n          134,\n          126\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"capture_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12254951858892368,\n        \"min\": 0.04,\n        \"max\": 0.56,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.1,\n          0.16,\n          0.32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flee_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0824460717516844,\n        \"min\": 0.05,\n        \"max\": 0.99,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.07,\n          0.06,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spawn_chance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 198.50127307201146,\n        \"min\": 0.01,\n        \"max\": 1598.0,\n        \"num_unique_values\": 104,\n        \"samples\": [\n          131.0,\n          3.6,\n          105.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"primary_strength\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Fighting\",\n          \"Rock\",\n          \"Grass\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combat_point\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 689,\n        \"min\": 264,\n        \"max\": 3525,\n        \"num_unique_values\": 145,\n        \"samples\": [\n          2548,\n          2180,\n          1823\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "data_frame"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e1294496-7f6e-4306-b345-6bddc4cecaea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>stamina</th>\n",
              "      <th>attack_value</th>\n",
              "      <th>defense_value</th>\n",
              "      <th>capture_rate</th>\n",
              "      <th>flee_rate</th>\n",
              "      <th>spawn_chance</th>\n",
              "      <th>primary_strength</th>\n",
              "      <th>combat_point</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bulbasaur</td>\n",
              "      <td>90</td>\n",
              "      <td>126</td>\n",
              "      <td>126</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.10</td>\n",
              "      <td>69.0</td>\n",
              "      <td>Grass</td>\n",
              "      <td>1079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ivysaur</td>\n",
              "      <td>120</td>\n",
              "      <td>156</td>\n",
              "      <td>158</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.07</td>\n",
              "      <td>4.2</td>\n",
              "      <td>Grass</td>\n",
              "      <td>1643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Venusaur</td>\n",
              "      <td>160</td>\n",
              "      <td>198</td>\n",
              "      <td>200</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.7</td>\n",
              "      <td>Grass</td>\n",
              "      <td>2598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Charmander</td>\n",
              "      <td>78</td>\n",
              "      <td>128</td>\n",
              "      <td>108</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.10</td>\n",
              "      <td>25.3</td>\n",
              "      <td>Fire</td>\n",
              "      <td>962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Charmeleon</td>\n",
              "      <td>116</td>\n",
              "      <td>160</td>\n",
              "      <td>140</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.2</td>\n",
              "      <td>Fire</td>\n",
              "      <td>1568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1294496-7f6e-4306-b345-6bddc4cecaea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1294496-7f6e-4306-b345-6bddc4cecaea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1294496-7f6e-4306-b345-6bddc4cecaea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6688e114-a48f-4e95-9ff2-5c9b7a3a0175\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6688e114-a48f-4e95-9ff2-5c9b7a3a0175')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6688e114-a48f-4e95-9ff2-5c9b7a3a0175 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         name  stamina  attack_value  defense_value  capture_rate  flee_rate  \\\n",
              "0   Bulbasaur       90           126            126          0.16       0.10   \n",
              "1     Ivysaur      120           156            158          0.08       0.07   \n",
              "2    Venusaur      160           198            200          0.04       0.05   \n",
              "3  Charmander       78           128            108          0.16       0.10   \n",
              "4  Charmeleon      116           160            140          0.08       0.07   \n",
              "\n",
              "   spawn_chance primary_strength  combat_point  \n",
              "0          69.0            Grass          1079  \n",
              "1           4.2            Grass          1643  \n",
              "2           1.7            Grass          2598  \n",
              "3          25.3             Fire           962  \n",
              "4           1.2             Fire          1568  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data_frame = pd.read_csv('/content/drive/MyDrive/CS_584/pokemon_data.csv')\n",
        "data_frame.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFb74VWd2cjH",
        "outputId": "1ebe7a02-4aab-42e9-8ee2-4052e0ec0caf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "name                 object\n",
              "stamina               int64\n",
              "attack_value          int64\n",
              "defense_value         int64\n",
              "capture_rate        float64\n",
              "flee_rate           float64\n",
              "spawn_chance        float64\n",
              "primary_strength     object\n",
              "combat_point          int64\n",
              "dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_frame.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l64nr1uy2cjH"
      },
      "source": [
        "Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of Python. By excuting the following code, let's create one Numpy array to contain the feature data without the name column and one array to contain the combat point ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui6QylBx2cjH",
        "outputId": "c1a3b781-95f8-4ce1-fe1d-c62f989b92fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "array of labels: shape (146,)\n",
            "array of feature matrix: shape (146, 7)\n"
          ]
        }
      ],
      "source": [
        "features = data_frame.values[:, 1:-1]\n",
        "labels = data_frame.values[:, -1]\n",
        "print('array of labels: shape ' + str(np.shape(labels)))\n",
        "print('array of feature matrix: shape ' + str(np.shape(features)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_PtSAbt2cjI"
      },
      "source": [
        "Now, you may find out that we have a categorical feature 'primary_strength' in our data. Categorical features require special attention because usually they cannot be the input of regression models as they are. A potential way to treat categorical features is to simply convert each value of the feature to a separate number. However, this might impute non-existent relative associations between the features, which might not always be representative of the data (e.g., if we assign “1” to the value “green” and “2” to the value “red”, the regression algorithm will assume that “red” is greater than “green,” which is not necessarily the case). For this reason, we can use a “one hot encoding” to represent categorical features. According to this, we will create a binary column for each category of the categorical feature, which will take a value of 1 if the sample belongs to that category, and 0 otherwise. For each categorical feature of the problem, count the number of different values and implement the one hot encoding. For the remaining of the problem, you will be working with the one hot encoding of the categorical features.\n",
        "\n",
        "\n",
        "In the next cell, write your code to replace the categorical feature 'primary_strength' with **one-hot encoding** and generate the new version of the Numpy array 'features'.\n",
        "\n",
        "**Hint: if you don't remember one hot encoding, review the slides of our first-week lecture.**\n",
        "\n",
        "**Note: do not use sklearn to automatically generate one hot encoding.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNdVGdyeK5s9",
        "outputId": "c599d14c-d312-42eb-c230-5d6ed8f82648"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Grass', 'Fire', 'Water', 'Bug', 'Normal', 'Poison', 'Electric',\n",
              "       'Ground', 'Fairy', 'Fighting', 'Psychic', 'Rock', 'Ghost', 'Ice',\n",
              "       'Dragon'], dtype=object)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_feat = data_frame['primary_strength'].unique()\n",
        "unique_feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "jSTgZUuD2cjI",
        "outputId": "3aa7a109-0595-405c-d9ed-3c5ce2ee6382"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"not_cat_feat\",\n  \"rows\": 146,\n  \"fields\": [\n    {\n      \"column\": \"stamina\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56,\n        \"min\": 20,\n        \"max\": 500,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          180,\n          70,\n          110\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attack_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40,\n        \"min\": 40,\n        \"max\": 250,\n        \"num_unique_values\": 67,\n        \"samples\": [\n          134,\n          102,\n          160\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"defense_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37,\n        \"min\": 54,\n        \"max\": 222,\n        \"num_unique_values\": 65,\n        \"samples\": [\n          168,\n          134,\n          126\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"capture_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12254951858892368,\n        \"min\": 0.04,\n        \"max\": 0.56,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.1,\n          0.16,\n          0.32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flee_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0824460717516844,\n        \"min\": 0.05,\n        \"max\": 0.99,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.07,\n          0.06,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spawn_chance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 198.50127307201146,\n        \"min\": 0.01,\n        \"max\": 1598.0,\n        \"num_unique_values\": 104,\n        \"samples\": [\n          131.0,\n          3.6,\n          105.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "not_cat_feat"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e0ed3733-001c-435a-a4b3-a1da835be745\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stamina</th>\n",
              "      <th>attack_value</th>\n",
              "      <th>defense_value</th>\n",
              "      <th>capture_rate</th>\n",
              "      <th>flee_rate</th>\n",
              "      <th>spawn_chance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90</td>\n",
              "      <td>126</td>\n",
              "      <td>126</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.10</td>\n",
              "      <td>69.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>120</td>\n",
              "      <td>156</td>\n",
              "      <td>158</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.07</td>\n",
              "      <td>4.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>160</td>\n",
              "      <td>198</td>\n",
              "      <td>200</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>78</td>\n",
              "      <td>128</td>\n",
              "      <td>108</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.10</td>\n",
              "      <td>25.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>116</td>\n",
              "      <td>160</td>\n",
              "      <td>140</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>160</td>\n",
              "      <td>182</td>\n",
              "      <td>162</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.09</td>\n",
              "      <td>1.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>320</td>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.09</td>\n",
              "      <td>1.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>82</td>\n",
              "      <td>128</td>\n",
              "      <td>110</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.09</td>\n",
              "      <td>30.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>122</td>\n",
              "      <td>170</td>\n",
              "      <td>152</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.06</td>\n",
              "      <td>2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>182</td>\n",
              "      <td>250</td>\n",
              "      <td>212</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>146 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0ed3733-001c-435a-a4b3-a1da835be745')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e0ed3733-001c-435a-a4b3-a1da835be745 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e0ed3733-001c-435a-a4b3-a1da835be745');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-89c24dd0-d641-4f6f-92c8-ead9c371e61f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-89c24dd0-d641-4f6f-92c8-ead9c371e61f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-89c24dd0-d641-4f6f-92c8-ead9c371e61f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_955a2f5b-5ca0-4bbb-aec7-206bc7fc2a24\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('not_cat_feat')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_955a2f5b-5ca0-4bbb-aec7-206bc7fc2a24 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('not_cat_feat');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     stamina  attack_value  defense_value  capture_rate  flee_rate  \\\n",
              "0         90           126            126          0.16       0.10   \n",
              "1        120           156            158          0.08       0.07   \n",
              "2        160           198            200          0.04       0.05   \n",
              "3         78           128            108          0.16       0.10   \n",
              "4        116           160            140          0.08       0.07   \n",
              "..       ...           ...            ...           ...        ...   \n",
              "141      160           182            162          0.16       0.09   \n",
              "142      320           180            180          0.16       0.09   \n",
              "143       82           128            110          0.32       0.09   \n",
              "144      122           170            152          0.08       0.06   \n",
              "145      182           250            212          0.04       0.05   \n",
              "\n",
              "     spawn_chance  \n",
              "0           69.00  \n",
              "1            4.20  \n",
              "2            1.70  \n",
              "3           25.30  \n",
              "4            1.20  \n",
              "..            ...  \n",
              "141          1.80  \n",
              "142          1.60  \n",
              "143         30.00  \n",
              "144          2.00  \n",
              "145          0.11  \n",
              "\n",
              "[146 rows x 6 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#removing categorical features\n",
        "not_cat_feat = data_frame.drop(['name','primary_strength' ,'combat_point'], axis =1)\n",
        "not_cat_feat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCQob2Kd2cjI",
        "outputId": "17e5c87d-e4e4-43c7-aa62-3d16d20e44b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stamina            int64\n",
            "attack_value       int64\n",
            "defense_value      int64\n",
            "capture_rate     float64\n",
            "flee_rate        float64\n",
            "spawn_chance     float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(not_cat_feat.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPExK0Es2cjI",
        "outputId": "74a2f751-f05b-4a3f-8f6e-ac2248f3af17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stamina            int64\n",
            "attack_value       int64\n",
            "defense_value      int64\n",
            "capture_rate     float64\n",
            "flee_rate        float64\n",
            "spawn_chance     float64\n",
            "Grass              int64\n",
            "Fire               int64\n",
            "Water              int64\n",
            "Bug                int64\n",
            "Normal             int64\n",
            "Poison             int64\n",
            "Electric           int64\n",
            "Ground             int64\n",
            "Fairy              int64\n",
            "Fighting           int64\n",
            "Psychic            int64\n",
            "Rock               int64\n",
            "Ghost              int64\n",
            "Ice                int64\n",
            "Dragon             int64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "#one hot encoding\n",
        "\n",
        "encode_ps = pd.DataFrame(index = data_frame.index)\n",
        "for category in unique_feat:\n",
        "    encode_ps[category] = data_frame['primary_strength'].apply(lambda x: 1 if x == category else 0)\n",
        "encode_feat = pd.concat([not_cat_feat, encode_ps], axis =1)\n",
        "\n",
        "# Convert boolean columns to integers in encode_feat\n",
        "encode_feat = encode_feat.astype({col: 'int64' for col in encode_feat.select_dtypes(include='bool').columns})\n",
        "print(encode_feat.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nwPh73U2cjJ",
        "outputId": "217c3c78-cc25-4e65-9b69-6c4159dd88da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Index(['stamina', 'attack_value', 'defense_value', 'capture_rate', 'flee_rate',\n",
              "       'spawn_chance', 'Grass', 'Fire', 'Water', 'Bug', 'Normal', 'Poison',\n",
              "       'Electric', 'Ground', 'Fairy', 'Fighting', 'Psychic', 'Rock', 'Ghost',\n",
              "       'Ice', 'Dragon'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feat_arr_onehot = encode_feat.to_numpy()\n",
        "print(feat_arr_onehot.dtype)\n",
        "\n",
        "# feat_arr_onehot.shape\n",
        "encode_feat.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoqMiieU2cjJ"
      },
      "source": [
        "Besides, you may also notice that other features have different scales. So, you need to standardize them: $({x-\\mu})/{\\sigma}$, where $\\mu$ is the mean and $\\sigma$ is the standard deviation. Write your code below.\n",
        "\n",
        "**Hint: details about feature standardization is also in slides of our first-week lecture.**\n",
        "\n",
        "**Note: You do not need to do standardize for one-hot encodings.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aykaOhmp2cjJ",
        "outputId": "4ecaca32-3717-4643-af3c-cb2d4cca567e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((146, 21),\n",
              " Index(['stamina', 'attack_value', 'defense_value', 'capture_rate', 'flee_rate',\n",
              "        'spawn_chance', 'Grass', 'Fire', 'Water', 'Bug', 'Normal', 'Poison',\n",
              "        'Electric', 'Ground', 'Fairy', 'Fighting', 'Psychic', 'Rock', 'Ghost',\n",
              "        'Ice', 'Dragon'],\n",
              "       dtype='object'))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_features = not_cat_feat.columns\n",
        "num_data = encode_feat[num_features]\n",
        "\n",
        "sn = (num_data - num_data.mean()) / num_data.std()\n",
        "sf = pd.concat([sn, encode_ps], axis=1)\n",
        "\n",
        "\n",
        "sf_arr = sf.to_numpy()\n",
        "sf_arr.shape, sf.columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t0TMbXf2cjJ"
      },
      "source": [
        "Now, in the next cell, you need to implement your own **linear regression model** using the **Ordinary Least Square (OLS)** solution **without regularization**. And here, you should adopt the **5-fold cross-validation** method. For each fold compute and print out the **square root** of the residual sum of squares error (RSS) between the actual and predicted outcome variable. Also compute and print out the **average** square root of the RSS over all folds.\n",
        "\n",
        "**Note: You should implement the algorithm by yourself. You are NOT allowed to use Machine Learning libraries like Sklearn.**\n",
        "\n",
        "**Hint: Use numpy.linalg.pinv() for calculating the inverse of a matrix.**\n",
        "\n",
        "**Hint: details about cross-validation is on page 40-42 in slides of KNN lecture.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdCtIFv62cjJ",
        "outputId": "9b541441-60ae-4a3e-b743-f213a1a48e6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(146,)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cp_arr = data_frame['combat_point'].to_numpy()\n",
        "cp_arr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF4uYftC2cjK",
        "outputId": "20cd1202-e8e6-4c9a-bfe4-7fd395a98107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float64\n"
          ]
        }
      ],
      "source": [
        "from numpy.linalg import pinv\n",
        "\n",
        "X = sf_arr\n",
        "X_b = np.c_[np.ones((feat_arr_onehot.shape[0], 1)), feat_arr_onehot]\n",
        "print(X_b.dtype)\n",
        "\n",
        "y = cp_arr\n",
        "#target var 'y' has combat point of Pokemon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhX9dczB2cjK",
        "outputId": "54703224-2f3e-4a5f-f9f6-6d85a59ea2a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Square Root of RSS: \n",
            " [568.330798318255, 537.301056102393, 537.301056102393, 537.301056102393, 537.301056102393]\n"
          ]
        }
      ],
      "source": [
        "#calculating rss\n",
        "def calculate_rss(y_actual, y_pred):\n",
        "    rss = np.sqrt(np.sum((y_actual - y_pred) ** 2))\n",
        "    return rss\n",
        "\n",
        "# 5-fold cross validation\n",
        "k = 5\n",
        "n = X_b.shape[0]\n",
        "\n",
        "indices = np.arange(n)\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "fold_sizes = np.full(k,n // k, dtype=int)\n",
        "fold_sizes[:n % k]+= 1\n",
        "\n",
        "start = 0\n",
        "rss_score = []\n",
        "\n",
        "for fold_size in fold_sizes:\n",
        "    stop = start + fold_size\n",
        "    test_indices = indices[start:stop]\n",
        "    train_indices = np.concatenate([indices[:start], indices[stop:]])\n",
        "\n",
        "\n",
        "    X_train, X_test = X_b[train_indices], X_b[test_indices]\n",
        "    y_train, y_test = y[train_indices], y[test_indices]\n",
        "\n",
        "#     print(f\"Data type of X_train: {X_train.dtype}\")\n",
        "#     print(f\"Data type of X_train.T @ X_train: {(X_train.T @ X_train).dtype}\")\n",
        "#     print(f\"Data type of y_train: {y_train.dtype}\")\n",
        "\n",
        "    #computing the OLS solution\n",
        "    best_fit = pinv(X_train.T @ X_train) @ X_train.T @ y_train\n",
        "\n",
        "\n",
        "    #making prediction on the test set\n",
        "    y_prediction = X_test @ best_fit\n",
        "\n",
        "    rss_value = calculate_rss(y_test, y_prediction)\n",
        "    rss_score.append(rss_value)\n",
        "\n",
        "\n",
        "rss_value_avg = np.mean(rss_score)\n",
        "\n",
        "print(\"Square Root of RSS: \\n\",rss_score)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPLFLjd6aiJ7",
        "outputId": "9199f81a-917f-4497-ba5d-acfedecab599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Square Root of RSS:  543.5070045455653\n"
          ]
        }
      ],
      "source": [
        "print(\"Average Square Root of RSS: \",rss_value_avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASIRyMGT2cjK"
      },
      "source": [
        "At the end in this part, please repeat the same experiment as in the previous step, but instead of linear regression, implement linear regression **with L2-norm regularization**. Experiment and report your results (average square root of RSS over 5-fold cross-validation) with different values of the regularization term $\\lambda=\\{1, 0.1, 0.01, 0.001, 0.0001\\}$.\n",
        "\n",
        "**Hint: details about the closed-form solution with regularization is on page 76 in slides of our linear regression lecture.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Q6rtY_WP2cjK"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import pinv\n",
        "\n",
        "def closed_form_regression(X, y, lamb):\n",
        "    identity = np.eye(X.shape[1])\n",
        "    identity[0,0] = 0\n",
        "\n",
        "    theta = pinv(X.T @ X + lamb * identity) @ X.T @ y\n",
        "    return theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtvwSPk52cjL",
        "outputId": "2d25a470-bc92-4751-f51f-fe7b9de69a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lambda: 1, Average RSS: 504.7817365156885\n",
            "Lambda: 0.1, Average RSS: 534.9791954758791\n",
            "Lambda: 0.01, Average RSS: 542.5230312411506\n",
            "Lambda: 0.001, Average RSS: 543.4070502071031\n",
            "Lambda: 0.0001, Average RSS: 543.4969932520249\n"
          ]
        }
      ],
      "source": [
        "lambdas = [1,0.1,0.01,0.001,0.0001]\n",
        "\n",
        "avg_rss_scores = []\n",
        "\n",
        "for lamb in lambdas:\n",
        "    rss_scores = []\n",
        "    for fold_size in fold_sizes:\n",
        "        stop = start + fold_size\n",
        "        test_indices = indices[start:stop]\n",
        "        train_indices = np.concatenate([indices[:start], indices[stop:]])\n",
        "\n",
        "        X_train, X_test = X_b[train_indices], X_b[test_indices]\n",
        "        y_train, y_test = y[train_indices], y[test_indices]\n",
        "\n",
        "        best_fit = closed_form_regression(X_train, y_train, lamb)\n",
        "\n",
        "        y_prediction = X_test @ best_fit\n",
        "\n",
        "        rss_value = calculate_rss(y_test, y_prediction)\n",
        "        rss_scores.append(rss_value)\n",
        "\n",
        "    avg_rss = np.mean(rss_scores)\n",
        "    avg_rss_scores.append(avg_rss)\n",
        "\n",
        "    print(f\"Lambda: {lamb}, Average RSS: {avg_rss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgBiNEDU2cjL"
      },
      "source": [
        "## Part 2: Neural Networks (40 points)\n",
        "\n",
        "In this part, you are going to implement your multi-layer perceptron model by the Pytorch library. You will still use the same handwritten digit image dataset from HW1. So, in the next few cells, please run the provided code to load and process the data, and creat dataset objects for further use by Pytorch.\n",
        "\n",
        "**Note: you need to install Pytorch beforehand. Or, you can use Google Colab for this homework, which is recommended.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hme3Nkp12cjL"
      },
      "outputs": [],
      "source": [
        "# load data from file and split into training and validation sets\n",
        "import numpy as np\n",
        "data = np.loadtxt(\"/content/drive/MyDrive/CS_584/train.txt\", delimiter=',')\n",
        "perm_idx = np.random.permutation(data.shape[0])\n",
        "vali_num = int(data.shape[0] * 0.2)\n",
        "vali_idx = perm_idx[:vali_num]\n",
        "train_idx = perm_idx[vali_num:]\n",
        "train_data = data[train_idx]\n",
        "vali_data = data[vali_idx]\n",
        "train_features = train_data[:, 1:].astype(np.float32)\n",
        "train_labels = train_data[:, 0].astype(int)\n",
        "vali_features = vali_data[:, 1:].astype(np.float32)\n",
        "vali_labels = vali_data[:, 0].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-LRAOmq2w0g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RJ6IixJ2cjL"
      },
      "outputs": [],
      "source": [
        "# define a Dataset class\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx, :], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sTw62QO2cjL",
        "outputId": "b0bf8173-a67d-4b0b-98bb-006f8f8984cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, F]: torch.Size([64, 784]) torch.float32\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "training_data = MNISTDataset(train_features, train_labels)\n",
        "vali_data = MNISTDataset(vali_features, vali_labels)\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "vali_dataloader = DataLoader(vali_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in train_dataloader:\n",
        "    print(f\"Shape of X [N, F]: {X.shape} {X.dtype}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNtzmqrp2cjL"
      },
      "source": [
        "Now, you should have the train_dataloader and vali_dataloader. Then, you need to build and train your multi-layer perceptron model by Pytorch.\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html gives a comprehansive example how to achieve this. Please read this tutorial closely, and implement the model in the next few cells.\n",
        "\n",
        "https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c30c1dcf2bc20119bcda7e734ce0eb42/quickstart_tutorial.ipynb provides the interactive version, which you can run and edit.\n",
        "\n",
        "**Note: in your implementation:**\n",
        "- you will only have three layers [784 -> 512 -> 10], you need to remove the [512 -> 512] layer in the tutorial.\n",
        "- add 'weight_decay=1e-4' in torch.optim.SGD to add L2 regularization.\n",
        "- train the model for 10 epochs instead of 5 epochs.\n",
        "- keep all other hyper-parameters the same as used in the tutorial.\n",
        "- **You are allowed to resue the code in the tutorial for this homework**\n",
        "\n",
        "\n",
        "**Note: print out the training process and the final accuracy on the validation set.**\n",
        "\n",
        "**Note: you can use Colab for running the code with GPU for free (open a colab notebook, then Runtime->Change runtime type->Hardware accelerator->GPU)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUMSPoKi2cjM",
        "outputId": "5fa1a70f-a697-431d-ec7d-8e25d50da218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Write your code\n",
        "import torch\n",
        "from torch import nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emIeucwQ96fA"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv84jpz3-adn"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRt2Zqov_bXS"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    print(\"\\n-------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-ummTH__QmD",
        "outputId": "da5e6c65-d87e-43c6-f2ee-fcaf4e5a5117"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.173670  [    0/48000]\n",
            "loss: 0.037089  [ 6400/48000]\n",
            "loss: 0.050983  [12800/48000]\n",
            "loss: 0.119967  [19200/48000]\n",
            "loss: 0.119909  [25600/48000]\n",
            "loss: 0.085360  [32000/48000]\n",
            "loss: 0.069345  [38400/48000]\n",
            "loss: 0.105718  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.0%, Avg loss: 0.174242 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.189223  [    0/48000]\n",
            "loss: 0.032542  [ 6400/48000]\n",
            "loss: 0.038819  [12800/48000]\n",
            "loss: 0.109250  [19200/48000]\n",
            "loss: 0.104765  [25600/48000]\n",
            "loss: 0.078278  [32000/48000]\n",
            "loss: 0.037790  [38400/48000]\n",
            "loss: 0.088638  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.3%, Avg loss: 0.168203 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.175764  [    0/48000]\n",
            "loss: 0.033464  [ 6400/48000]\n",
            "loss: 0.028930  [12800/48000]\n",
            "loss: 0.100443  [19200/48000]\n",
            "loss: 0.084009  [25600/48000]\n",
            "loss: 0.062568  [32000/48000]\n",
            "loss: 0.023568  [38400/48000]\n",
            "loss: 0.063581  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.5%, Avg loss: 0.163133 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.178632  [    0/48000]\n",
            "loss: 0.029324  [ 6400/48000]\n",
            "loss: 0.022766  [12800/48000]\n",
            "loss: 0.088594  [19200/48000]\n",
            "loss: 0.083111  [25600/48000]\n",
            "loss: 0.051603  [32000/48000]\n",
            "loss: 0.017734  [38400/48000]\n",
            "loss: 0.049601  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.6%, Avg loss: 0.160416 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.172602  [    0/48000]\n",
            "loss: 0.028510  [ 6400/48000]\n",
            "loss: 0.019977  [12800/48000]\n",
            "loss: 0.079493  [19200/48000]\n",
            "loss: 0.070361  [25600/48000]\n",
            "loss: 0.045210  [32000/48000]\n",
            "loss: 0.014452  [38400/48000]\n",
            "loss: 0.035374  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.158884 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.170439  [    0/48000]\n",
            "loss: 0.022051  [ 6400/48000]\n",
            "loss: 0.017639  [12800/48000]\n",
            "loss: 0.070719  [19200/48000]\n",
            "loss: 0.066159  [25600/48000]\n",
            "loss: 0.043729  [32000/48000]\n",
            "loss: 0.010559  [38400/48000]\n",
            "loss: 0.027217  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 0.158272 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.175864  [    0/48000]\n",
            "loss: 0.019385  [ 6400/48000]\n",
            "loss: 0.016074  [12800/48000]\n",
            "loss: 0.063547  [19200/48000]\n",
            "loss: 0.060420  [25600/48000]\n",
            "loss: 0.037057  [32000/48000]\n",
            "loss: 0.009282  [38400/48000]\n",
            "loss: 0.022702  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 0.158970 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.172006  [    0/48000]\n",
            "loss: 0.019222  [ 6400/48000]\n",
            "loss: 0.014890  [12800/48000]\n",
            "loss: 0.053732  [19200/48000]\n",
            "loss: 0.052852  [25600/48000]\n",
            "loss: 0.037040  [32000/48000]\n",
            "loss: 0.007252  [38400/48000]\n",
            "loss: 0.019865  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 0.159517 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.167933  [    0/48000]\n",
            "loss: 0.015690  [ 6400/48000]\n",
            "loss: 0.014609  [12800/48000]\n",
            "loss: 0.050872  [19200/48000]\n",
            "loss: 0.050965  [25600/48000]\n",
            "loss: 0.032170  [32000/48000]\n",
            "loss: 0.005649  [38400/48000]\n",
            "loss: 0.015864  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 0.160222 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.180086  [    0/48000]\n",
            "loss: 0.015926  [ 6400/48000]\n",
            "loss: 0.013597  [12800/48000]\n",
            "loss: 0.042172  [19200/48000]\n",
            "loss: 0.044847  [25600/48000]\n",
            "loss: 0.028796  [32000/48000]\n",
            "loss: 0.004937  [38400/48000]\n",
            "loss: 0.018482  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.160192 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(vali_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34XVo3Du2cjM"
      },
      "source": [
        "## Part 3: Tune Hyperparameter [Need to submit to Miner2] (20 points)\n",
        "\n",
        "In this part, you need to do your best to tune the hyperparameter in the MLP to build the best model and submit the predictions for the testing data to Miner2 system. First of all, let's load the testing data by excuting the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGHyniJ02cjM",
        "outputId": "fe266391-a453-4e20-cfff-c81952bde65b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "array of testing feature matrix: shape (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "test_features = np.loadtxt(\"/content/drive/MyDrive/CS_584/test.txt\", delimiter=',')\n",
        "print('array of testing feature matrix: shape ' + str(np.shape(test_features)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzKkMXJs2cjM"
      },
      "source": [
        "Now, you should tune four hyperparameters:\n",
        "\n",
        "- the number of layers and the dimension of each layer (explore as much as you can, but choose reasonable settings considering the computational resource you have)\n",
        "- the activation function (choose from sigmoid, tanh, relu, leaky_relu)\n",
        "- weight decay\n",
        "- number of training epochs\n",
        "\n",
        "Rules:\n",
        "\n",
        "- Write your predictions for samples in the testing set into a file, in which each line has one integer indicating the prediction from your best model for the corresponding sample in the test.txt file. Please see the format.txt file in Miner2 as one submission example. Name the submission file hw2_Miner2.txt and submit it to Miner2 HW2 page.\n",
        "- The public leaderboard shows results for 50% of randomly chosen test instances only. This is a standard practice in data mining challenge to avoid gaming of the system. The private leaderboard will be released after the deadline evaluates all the entries in the test set.\n",
        "- You are allowed 5 submissions in a 24 hour cycle.\n",
        "- The final score and ranking will always be based on the last submission.\n",
        "- Grading will only be based on the model performance (based on Accuracy metric) instead of ranking. You'll get full credit as long as your socre is a reasonable number.\n",
        "\n",
        "\n",
        "**Hint: You can tune these hyperparameters by one randomly generated validation set, or you can also use the cross-validation method.**\n",
        "\n",
        "**Note: you can use Colab for running the code with GPU for free**\n",
        "\n",
        "**Hint: use the following two lines of code to generate the label predictions for test data:**\n",
        "- raw_pred = model(torch.tensor(test_features).to(device).float())\n",
        "- pred = np.argmax(raw_pred.to('cpu').detach().numpy(), axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8XrULa_2cjM"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    'num_layers': [2, 3, 4],\n",
        "    'layer_size': [128, 256, 512],\n",
        "    'activation': ['relu', 'sigmoid', 'tanh', 'leaky_relu'],  # Activation functions\n",
        "    'weight_decay': [0, 1e-4, 1e-3],  # Weight decay values\n",
        "    'epochs': [5, 10, 20]  # Number of epochs\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuwSVEmrbnS7"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_layers, layer_size, activation):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        layers = [nn.Flatten()]\n",
        "        for i in range(num_layers - 1):  # Minus 1 because the last layer is the output layer\n",
        "            layers.append(nn.Linear(layer_size if i > 0 else 28*28, layer_size))\n",
        "            if activation == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif activation == 'sigmoid':\n",
        "                layers.append(nn.Sigmoid())\n",
        "            elif activation == 'tanh':\n",
        "                layers.append(nn.Tanh())\n",
        "            elif activation == 'leaky_relu':\n",
        "                layers.append(nn.LeakyReLU())\n",
        "        layers.append(nn.Linear(layer_size, 10))  # Output layer\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.layers(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eABGdUzc_bC"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    accuracy = correct / size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvGQ_pnIfmAO",
        "outputId": "0fe0d71c-2d17-4e42-bc6b-d8d5746586cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=256, Activation=leaky_relu, weight_decay=0.0001, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 8.026017  [    0/48000]\n",
            "loss: 0.647256  [ 6400/48000]\n",
            "loss: 0.399848  [12800/48000]\n",
            "loss: 0.377799  [19200/48000]\n",
            "loss: 0.377808  [25600/48000]\n",
            "loss: 0.265476  [32000/48000]\n",
            "loss: 0.269797  [38400/48000]\n",
            "loss: 0.292416  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.272918  [    0/48000]\n",
            "loss: 0.267895  [ 6400/48000]\n",
            "loss: 0.156759  [12800/48000]\n",
            "loss: 0.208008  [19200/48000]\n",
            "loss: 0.178944  [25600/48000]\n",
            "loss: 0.133296  [32000/48000]\n",
            "loss: 0.118316  [38400/48000]\n",
            "loss: 0.195982  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.206745  [    0/48000]\n",
            "loss: 0.202377  [ 6400/48000]\n",
            "loss: 0.091823  [12800/48000]\n",
            "loss: 0.125680  [19200/48000]\n",
            "loss: 0.120255  [25600/48000]\n",
            "loss: 0.092729  [32000/48000]\n",
            "loss: 0.071164  [38400/48000]\n",
            "loss: 0.146960  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.187284  [    0/48000]\n",
            "loss: 0.170678  [ 6400/48000]\n",
            "loss: 0.063454  [12800/48000]\n",
            "loss: 0.089667  [19200/48000]\n",
            "loss: 0.101548  [25600/48000]\n",
            "loss: 0.076608  [32000/48000]\n",
            "loss: 0.050692  [38400/48000]\n",
            "loss: 0.108007  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.161797  [    0/48000]\n",
            "loss: 0.143823  [ 6400/48000]\n",
            "loss: 0.045081  [12800/48000]\n",
            "loss: 0.070949  [19200/48000]\n",
            "loss: 0.089865  [25600/48000]\n",
            "loss: 0.065692  [32000/48000]\n",
            "loss: 0.038323  [38400/48000]\n",
            "loss: 0.081869  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.142557  [    0/48000]\n",
            "loss: 0.121735  [ 6400/48000]\n",
            "loss: 0.036390  [12800/48000]\n",
            "loss: 0.053022  [19200/48000]\n",
            "loss: 0.083386  [25600/48000]\n",
            "loss: 0.056835  [32000/48000]\n",
            "loss: 0.031594  [38400/48000]\n",
            "loss: 0.063058  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.122847  [    0/48000]\n",
            "loss: 0.109598  [ 6400/48000]\n",
            "loss: 0.030953  [12800/48000]\n",
            "loss: 0.042394  [19200/48000]\n",
            "loss: 0.076599  [25600/48000]\n",
            "loss: 0.048566  [32000/48000]\n",
            "loss: 0.026437  [38400/48000]\n",
            "loss: 0.049285  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.106304  [    0/48000]\n",
            "loss: 0.100413  [ 6400/48000]\n",
            "loss: 0.027226  [12800/48000]\n",
            "loss: 0.035184  [19200/48000]\n",
            "loss: 0.071352  [25600/48000]\n",
            "loss: 0.042651  [32000/48000]\n",
            "loss: 0.021969  [38400/48000]\n",
            "loss: 0.039092  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.092008  [    0/48000]\n",
            "loss: 0.092272  [ 6400/48000]\n",
            "loss: 0.023668  [12800/48000]\n",
            "loss: 0.028855  [19200/48000]\n",
            "loss: 0.063707  [25600/48000]\n",
            "loss: 0.037478  [32000/48000]\n",
            "loss: 0.018766  [38400/48000]\n",
            "loss: 0.031288  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.078735  [    0/48000]\n",
            "loss: 0.083861  [ 6400/48000]\n",
            "loss: 0.020769  [12800/48000]\n",
            "loss: 0.023541  [19200/48000]\n",
            "loss: 0.058841  [25600/48000]\n",
            "loss: 0.033522  [32000/48000]\n",
            "loss: 0.016502  [38400/48000]\n",
            "loss: 0.026410  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.067527  [    0/48000]\n",
            "loss: 0.078380  [ 6400/48000]\n",
            "loss: 0.017011  [12800/48000]\n",
            "loss: 0.019324  [19200/48000]\n",
            "loss: 0.053897  [25600/48000]\n",
            "loss: 0.029960  [32000/48000]\n",
            "loss: 0.014717  [38400/48000]\n",
            "loss: 0.023448  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.059447  [    0/48000]\n",
            "loss: 0.071501  [ 6400/48000]\n",
            "loss: 0.015039  [12800/48000]\n",
            "loss: 0.017069  [19200/48000]\n",
            "loss: 0.048275  [25600/48000]\n",
            "loss: 0.026660  [32000/48000]\n",
            "loss: 0.013225  [38400/48000]\n",
            "loss: 0.020374  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.050612  [    0/48000]\n",
            "loss: 0.064963  [ 6400/48000]\n",
            "loss: 0.013282  [12800/48000]\n",
            "loss: 0.015170  [19200/48000]\n",
            "loss: 0.043445  [25600/48000]\n",
            "loss: 0.024315  [32000/48000]\n",
            "loss: 0.011974  [38400/48000]\n",
            "loss: 0.018787  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.045286  [    0/48000]\n",
            "loss: 0.059299  [ 6400/48000]\n",
            "loss: 0.011812  [12800/48000]\n",
            "loss: 0.013687  [19200/48000]\n",
            "loss: 0.039508  [25600/48000]\n",
            "loss: 0.020643  [32000/48000]\n",
            "loss: 0.010506  [38400/48000]\n",
            "loss: 0.016719  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.040393  [    0/48000]\n",
            "loss: 0.052921  [ 6400/48000]\n",
            "loss: 0.010796  [12800/48000]\n",
            "loss: 0.012340  [19200/48000]\n",
            "loss: 0.034597  [25600/48000]\n",
            "loss: 0.018781  [32000/48000]\n",
            "loss: 0.009340  [38400/48000]\n",
            "loss: 0.015263  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.035988  [    0/48000]\n",
            "loss: 0.047138  [ 6400/48000]\n",
            "loss: 0.010076  [12800/48000]\n",
            "loss: 0.011522  [19200/48000]\n",
            "loss: 0.030891  [25600/48000]\n",
            "loss: 0.016960  [32000/48000]\n",
            "loss: 0.008463  [38400/48000]\n",
            "loss: 0.014147  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.032905  [    0/48000]\n",
            "loss: 0.042770  [ 6400/48000]\n",
            "loss: 0.008885  [12800/48000]\n",
            "loss: 0.010269  [19200/48000]\n",
            "loss: 0.027775  [25600/48000]\n",
            "loss: 0.015441  [32000/48000]\n",
            "loss: 0.007535  [38400/48000]\n",
            "loss: 0.012955  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.029869  [    0/48000]\n",
            "loss: 0.038117  [ 6400/48000]\n",
            "loss: 0.008081  [12800/48000]\n",
            "loss: 0.009599  [19200/48000]\n",
            "loss: 0.025422  [25600/48000]\n",
            "loss: 0.013584  [32000/48000]\n",
            "loss: 0.007282  [38400/48000]\n",
            "loss: 0.012214  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.027389  [    0/48000]\n",
            "loss: 0.034582  [ 6400/48000]\n",
            "loss: 0.007415  [12800/48000]\n",
            "loss: 0.008924  [19200/48000]\n",
            "loss: 0.022765  [25600/48000]\n",
            "loss: 0.012619  [32000/48000]\n",
            "loss: 0.006501  [38400/48000]\n",
            "loss: 0.011506  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.025129  [    0/48000]\n",
            "loss: 0.031314  [ 6400/48000]\n",
            "loss: 0.006892  [12800/48000]\n",
            "loss: 0.008176  [19200/48000]\n",
            "loss: 0.020652  [25600/48000]\n",
            "loss: 0.011499  [32000/48000]\n",
            "loss: 0.006093  [38400/48000]\n",
            "loss: 0.010909  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.109450 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=256, Activation=leaky_relu, weight_decay=0.001, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 4.423669  [    0/48000]\n",
            "loss: 0.715448  [ 6400/48000]\n",
            "loss: 0.278111  [12800/48000]\n",
            "loss: 0.344120  [19200/48000]\n",
            "loss: 0.353097  [25600/48000]\n",
            "loss: 0.188294  [32000/48000]\n",
            "loss: 0.248940  [38400/48000]\n",
            "loss: 0.287802  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.187280  [    0/48000]\n",
            "loss: 0.287762  [ 6400/48000]\n",
            "loss: 0.100248  [12800/48000]\n",
            "loss: 0.183734  [19200/48000]\n",
            "loss: 0.211344  [25600/48000]\n",
            "loss: 0.130173  [32000/48000]\n",
            "loss: 0.149796  [38400/48000]\n",
            "loss: 0.205973  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.128915  [    0/48000]\n",
            "loss: 0.221000  [ 6400/48000]\n",
            "loss: 0.075210  [12800/48000]\n",
            "loss: 0.115103  [19200/48000]\n",
            "loss: 0.171941  [25600/48000]\n",
            "loss: 0.098307  [32000/48000]\n",
            "loss: 0.094579  [38400/48000]\n",
            "loss: 0.170091  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.111822  [    0/48000]\n",
            "loss: 0.181688  [ 6400/48000]\n",
            "loss: 0.060870  [12800/48000]\n",
            "loss: 0.081437  [19200/48000]\n",
            "loss: 0.151952  [25600/48000]\n",
            "loss: 0.079887  [32000/48000]\n",
            "loss: 0.067037  [38400/48000]\n",
            "loss: 0.141501  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.100402  [    0/48000]\n",
            "loss: 0.161738  [ 6400/48000]\n",
            "loss: 0.055196  [12800/48000]\n",
            "loss: 0.067207  [19200/48000]\n",
            "loss: 0.135230  [25600/48000]\n",
            "loss: 0.066980  [32000/48000]\n",
            "loss: 0.050973  [38400/48000]\n",
            "loss: 0.118420  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.6%, Avg loss: 0.148121 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=256, Activation=leaky_relu, weight_decay=0.001, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.494560  [    0/48000]\n",
            "loss: 0.545964  [ 6400/48000]\n",
            "loss: 0.245340  [12800/48000]\n",
            "loss: 0.386909  [19200/48000]\n",
            "loss: 0.410088  [25600/48000]\n",
            "loss: 0.261439  [32000/48000]\n",
            "loss: 0.241534  [38400/48000]\n",
            "loss: 0.307293  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.269558  [    0/48000]\n",
            "loss: 0.178019  [ 6400/48000]\n",
            "loss: 0.102124  [12800/48000]\n",
            "loss: 0.189681  [19200/48000]\n",
            "loss: 0.263566  [25600/48000]\n",
            "loss: 0.145469  [32000/48000]\n",
            "loss: 0.156902  [38400/48000]\n",
            "loss: 0.205324  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.215195  [    0/48000]\n",
            "loss: 0.151844  [ 6400/48000]\n",
            "loss: 0.066387  [12800/48000]\n",
            "loss: 0.106061  [19200/48000]\n",
            "loss: 0.213140  [25600/48000]\n",
            "loss: 0.117268  [32000/48000]\n",
            "loss: 0.125407  [38400/48000]\n",
            "loss: 0.161195  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.189037  [    0/48000]\n",
            "loss: 0.128132  [ 6400/48000]\n",
            "loss: 0.047673  [12800/48000]\n",
            "loss: 0.072436  [19200/48000]\n",
            "loss: 0.184819  [25600/48000]\n",
            "loss: 0.104132  [32000/48000]\n",
            "loss: 0.094853  [38400/48000]\n",
            "loss: 0.128512  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.177871  [    0/48000]\n",
            "loss: 0.110299  [ 6400/48000]\n",
            "loss: 0.039675  [12800/48000]\n",
            "loss: 0.049336  [19200/48000]\n",
            "loss: 0.160024  [25600/48000]\n",
            "loss: 0.087212  [32000/48000]\n",
            "loss: 0.076028  [38400/48000]\n",
            "loss: 0.105201  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.169422  [    0/48000]\n",
            "loss: 0.097520  [ 6400/48000]\n",
            "loss: 0.035491  [12800/48000]\n",
            "loss: 0.033976  [19200/48000]\n",
            "loss: 0.142013  [25600/48000]\n",
            "loss: 0.075270  [32000/48000]\n",
            "loss: 0.060250  [38400/48000]\n",
            "loss: 0.088968  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.159800  [    0/48000]\n",
            "loss: 0.084497  [ 6400/48000]\n",
            "loss: 0.032035  [12800/48000]\n",
            "loss: 0.026573  [19200/48000]\n",
            "loss: 0.123812  [25600/48000]\n",
            "loss: 0.062997  [32000/48000]\n",
            "loss: 0.045250  [38400/48000]\n",
            "loss: 0.078747  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.150688  [    0/48000]\n",
            "loss: 0.074220  [ 6400/48000]\n",
            "loss: 0.028927  [12800/48000]\n",
            "loss: 0.021485  [19200/48000]\n",
            "loss: 0.112233  [25600/48000]\n",
            "loss: 0.052814  [32000/48000]\n",
            "loss: 0.035036  [38400/48000]\n",
            "loss: 0.067299  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.144752  [    0/48000]\n",
            "loss: 0.067003  [ 6400/48000]\n",
            "loss: 0.026141  [12800/48000]\n",
            "loss: 0.018881  [19200/48000]\n",
            "loss: 0.098493  [25600/48000]\n",
            "loss: 0.045325  [32000/48000]\n",
            "loss: 0.028389  [38400/48000]\n",
            "loss: 0.057426  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.135313  [    0/48000]\n",
            "loss: 0.060155  [ 6400/48000]\n",
            "loss: 0.024370  [12800/48000]\n",
            "loss: 0.016255  [19200/48000]\n",
            "loss: 0.085194  [25600/48000]\n",
            "loss: 0.039846  [32000/48000]\n",
            "loss: 0.023344  [38400/48000]\n",
            "loss: 0.050804  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 0.119331 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=256, Activation=leaky_relu, weight_decay=0.001, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 4.612347  [    0/48000]\n",
            "loss: 0.695243  [ 6400/48000]\n",
            "loss: 0.249516  [12800/48000]\n",
            "loss: 0.414946  [19200/48000]\n",
            "loss: 0.206468  [25600/48000]\n",
            "loss: 0.262185  [32000/48000]\n",
            "loss: 0.265182  [38400/48000]\n",
            "loss: 0.243198  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.228246  [    0/48000]\n",
            "loss: 0.339661  [ 6400/48000]\n",
            "loss: 0.097034  [12800/48000]\n",
            "loss: 0.206574  [19200/48000]\n",
            "loss: 0.085946  [25600/48000]\n",
            "loss: 0.157330  [32000/48000]\n",
            "loss: 0.164809  [38400/48000]\n",
            "loss: 0.201297  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.192613  [    0/48000]\n",
            "loss: 0.272934  [ 6400/48000]\n",
            "loss: 0.072109  [12800/48000]\n",
            "loss: 0.131637  [19200/48000]\n",
            "loss: 0.060516  [25600/48000]\n",
            "loss: 0.120498  [32000/48000]\n",
            "loss: 0.131507  [38400/48000]\n",
            "loss: 0.171191  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.191062  [    0/48000]\n",
            "loss: 0.218476  [ 6400/48000]\n",
            "loss: 0.062214  [12800/48000]\n",
            "loss: 0.082589  [19200/48000]\n",
            "loss: 0.045547  [25600/48000]\n",
            "loss: 0.106028  [32000/48000]\n",
            "loss: 0.114786  [38400/48000]\n",
            "loss: 0.136050  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.191013  [    0/48000]\n",
            "loss: 0.173365  [ 6400/48000]\n",
            "loss: 0.055777  [12800/48000]\n",
            "loss: 0.060676  [19200/48000]\n",
            "loss: 0.038532  [25600/48000]\n",
            "loss: 0.092525  [32000/48000]\n",
            "loss: 0.098895  [38400/48000]\n",
            "loss: 0.114660  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.188797  [    0/48000]\n",
            "loss: 0.143554  [ 6400/48000]\n",
            "loss: 0.047159  [12800/48000]\n",
            "loss: 0.050777  [19200/48000]\n",
            "loss: 0.032817  [25600/48000]\n",
            "loss: 0.082424  [32000/48000]\n",
            "loss: 0.080887  [38400/48000]\n",
            "loss: 0.094377  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.186264  [    0/48000]\n",
            "loss: 0.120256  [ 6400/48000]\n",
            "loss: 0.041335  [12800/48000]\n",
            "loss: 0.043828  [19200/48000]\n",
            "loss: 0.027486  [25600/48000]\n",
            "loss: 0.071165  [32000/48000]\n",
            "loss: 0.064033  [38400/48000]\n",
            "loss: 0.077762  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.182491  [    0/48000]\n",
            "loss: 0.100926  [ 6400/48000]\n",
            "loss: 0.036126  [12800/48000]\n",
            "loss: 0.037593  [19200/48000]\n",
            "loss: 0.023305  [25600/48000]\n",
            "loss: 0.062534  [32000/48000]\n",
            "loss: 0.052561  [38400/48000]\n",
            "loss: 0.068046  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.175159  [    0/48000]\n",
            "loss: 0.088039  [ 6400/48000]\n",
            "loss: 0.034011  [12800/48000]\n",
            "loss: 0.032382  [19200/48000]\n",
            "loss: 0.020751  [25600/48000]\n",
            "loss: 0.057201  [32000/48000]\n",
            "loss: 0.044418  [38400/48000]\n",
            "loss: 0.060153  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.167381  [    0/48000]\n",
            "loss: 0.079372  [ 6400/48000]\n",
            "loss: 0.030778  [12800/48000]\n",
            "loss: 0.027676  [19200/48000]\n",
            "loss: 0.018168  [25600/48000]\n",
            "loss: 0.049585  [32000/48000]\n",
            "loss: 0.037588  [38400/48000]\n",
            "loss: 0.054006  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.158453  [    0/48000]\n",
            "loss: 0.070634  [ 6400/48000]\n",
            "loss: 0.027167  [12800/48000]\n",
            "loss: 0.023967  [19200/48000]\n",
            "loss: 0.016077  [25600/48000]\n",
            "loss: 0.040710  [32000/48000]\n",
            "loss: 0.033206  [38400/48000]\n",
            "loss: 0.048176  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.150887  [    0/48000]\n",
            "loss: 0.063409  [ 6400/48000]\n",
            "loss: 0.023246  [12800/48000]\n",
            "loss: 0.021978  [19200/48000]\n",
            "loss: 0.015398  [25600/48000]\n",
            "loss: 0.031878  [32000/48000]\n",
            "loss: 0.027741  [38400/48000]\n",
            "loss: 0.043622  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.144133  [    0/48000]\n",
            "loss: 0.057214  [ 6400/48000]\n",
            "loss: 0.021540  [12800/48000]\n",
            "loss: 0.019322  [19200/48000]\n",
            "loss: 0.014006  [25600/48000]\n",
            "loss: 0.024445  [32000/48000]\n",
            "loss: 0.024634  [38400/48000]\n",
            "loss: 0.039788  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.135280  [    0/48000]\n",
            "loss: 0.051694  [ 6400/48000]\n",
            "loss: 0.019507  [12800/48000]\n",
            "loss: 0.016486  [19200/48000]\n",
            "loss: 0.012676  [25600/48000]\n",
            "loss: 0.020059  [32000/48000]\n",
            "loss: 0.021075  [38400/48000]\n",
            "loss: 0.035881  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.129700  [    0/48000]\n",
            "loss: 0.046108  [ 6400/48000]\n",
            "loss: 0.017853  [12800/48000]\n",
            "loss: 0.014972  [19200/48000]\n",
            "loss: 0.012566  [25600/48000]\n",
            "loss: 0.016471  [32000/48000]\n",
            "loss: 0.019170  [38400/48000]\n",
            "loss: 0.032297  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.123169  [    0/48000]\n",
            "loss: 0.042013  [ 6400/48000]\n",
            "loss: 0.016804  [12800/48000]\n",
            "loss: 0.013338  [19200/48000]\n",
            "loss: 0.011645  [25600/48000]\n",
            "loss: 0.013684  [32000/48000]\n",
            "loss: 0.016490  [38400/48000]\n",
            "loss: 0.028283  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.118439  [    0/48000]\n",
            "loss: 0.038209  [ 6400/48000]\n",
            "loss: 0.014876  [12800/48000]\n",
            "loss: 0.011771  [19200/48000]\n",
            "loss: 0.010574  [25600/48000]\n",
            "loss: 0.012269  [32000/48000]\n",
            "loss: 0.015500  [38400/48000]\n",
            "loss: 0.026109  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.113281  [    0/48000]\n",
            "loss: 0.034595  [ 6400/48000]\n",
            "loss: 0.013462  [12800/48000]\n",
            "loss: 0.011014  [19200/48000]\n",
            "loss: 0.010540  [25600/48000]\n",
            "loss: 0.010673  [32000/48000]\n",
            "loss: 0.013561  [38400/48000]\n",
            "loss: 0.023451  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.107827  [    0/48000]\n",
            "loss: 0.030633  [ 6400/48000]\n",
            "loss: 0.012112  [12800/48000]\n",
            "loss: 0.010535  [19200/48000]\n",
            "loss: 0.010214  [25600/48000]\n",
            "loss: 0.009415  [32000/48000]\n",
            "loss: 0.012027  [38400/48000]\n",
            "loss: 0.021097  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.101304  [    0/48000]\n",
            "loss: 0.027675  [ 6400/48000]\n",
            "loss: 0.010930  [12800/48000]\n",
            "loss: 0.009762  [19200/48000]\n",
            "loss: 0.009287  [25600/48000]\n",
            "loss: 0.008496  [32000/48000]\n",
            "loss: 0.011201  [38400/48000]\n",
            "loss: 0.018956  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 0.107461 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=relu, weight_decay=0, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 7.069376  [    0/48000]\n",
            "loss: 0.560758  [ 6400/48000]\n",
            "loss: 0.269844  [12800/48000]\n",
            "loss: 0.384774  [19200/48000]\n",
            "loss: 0.270224  [25600/48000]\n",
            "loss: 0.164632  [32000/48000]\n",
            "loss: 0.166008  [38400/48000]\n",
            "loss: 0.191972  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.280264  [    0/48000]\n",
            "loss: 0.179306  [ 6400/48000]\n",
            "loss: 0.126047  [12800/48000]\n",
            "loss: 0.262774  [19200/48000]\n",
            "loss: 0.188455  [25600/48000]\n",
            "loss: 0.082433  [32000/48000]\n",
            "loss: 0.072938  [38400/48000]\n",
            "loss: 0.136944  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.213857  [    0/48000]\n",
            "loss: 0.145407  [ 6400/48000]\n",
            "loss: 0.086396  [12800/48000]\n",
            "loss: 0.188721  [19200/48000]\n",
            "loss: 0.160812  [25600/48000]\n",
            "loss: 0.057571  [32000/48000]\n",
            "loss: 0.039934  [38400/48000]\n",
            "loss: 0.108121  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.178442  [    0/48000]\n",
            "loss: 0.118137  [ 6400/48000]\n",
            "loss: 0.069203  [12800/48000]\n",
            "loss: 0.134629  [19200/48000]\n",
            "loss: 0.138293  [25600/48000]\n",
            "loss: 0.045440  [32000/48000]\n",
            "loss: 0.027526  [38400/48000]\n",
            "loss: 0.082895  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.162007  [    0/48000]\n",
            "loss: 0.100855  [ 6400/48000]\n",
            "loss: 0.058472  [12800/48000]\n",
            "loss: 0.098313  [19200/48000]\n",
            "loss: 0.117592  [25600/48000]\n",
            "loss: 0.036464  [32000/48000]\n",
            "loss: 0.020307  [38400/48000]\n",
            "loss: 0.063600  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.1%, Avg loss: 0.128609 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=relu, weight_decay=0, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 3.920603  [    0/48000]\n",
            "loss: 0.562442  [ 6400/48000]\n",
            "loss: 0.251569  [12800/48000]\n",
            "loss: 0.316776  [19200/48000]\n",
            "loss: 0.197435  [25600/48000]\n",
            "loss: 0.232301  [32000/48000]\n",
            "loss: 0.127704  [38400/48000]\n",
            "loss: 0.310275  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.262766  [    0/48000]\n",
            "loss: 0.177094  [ 6400/48000]\n",
            "loss: 0.104221  [12800/48000]\n",
            "loss: 0.143317  [19200/48000]\n",
            "loss: 0.073701  [25600/48000]\n",
            "loss: 0.142265  [32000/48000]\n",
            "loss: 0.074829  [38400/48000]\n",
            "loss: 0.214799  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.228248  [    0/48000]\n",
            "loss: 0.124387  [ 6400/48000]\n",
            "loss: 0.071997  [12800/48000]\n",
            "loss: 0.095292  [19200/48000]\n",
            "loss: 0.049188  [25600/48000]\n",
            "loss: 0.109600  [32000/48000]\n",
            "loss: 0.062407  [38400/48000]\n",
            "loss: 0.153055  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.199697  [    0/48000]\n",
            "loss: 0.095260  [ 6400/48000]\n",
            "loss: 0.047917  [12800/48000]\n",
            "loss: 0.073914  [19200/48000]\n",
            "loss: 0.036984  [25600/48000]\n",
            "loss: 0.090446  [32000/48000]\n",
            "loss: 0.049304  [38400/48000]\n",
            "loss: 0.096404  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.171849  [    0/48000]\n",
            "loss: 0.079400  [ 6400/48000]\n",
            "loss: 0.036970  [12800/48000]\n",
            "loss: 0.063355  [19200/48000]\n",
            "loss: 0.028969  [25600/48000]\n",
            "loss: 0.076717  [32000/48000]\n",
            "loss: 0.039240  [38400/48000]\n",
            "loss: 0.063071  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.147279  [    0/48000]\n",
            "loss: 0.068426  [ 6400/48000]\n",
            "loss: 0.029450  [12800/48000]\n",
            "loss: 0.054236  [19200/48000]\n",
            "loss: 0.022897  [25600/48000]\n",
            "loss: 0.060600  [32000/48000]\n",
            "loss: 0.030225  [38400/48000]\n",
            "loss: 0.048098  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.126730  [    0/48000]\n",
            "loss: 0.059307  [ 6400/48000]\n",
            "loss: 0.023945  [12800/48000]\n",
            "loss: 0.045467  [19200/48000]\n",
            "loss: 0.018861  [25600/48000]\n",
            "loss: 0.047171  [32000/48000]\n",
            "loss: 0.024235  [38400/48000]\n",
            "loss: 0.039577  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.110633  [    0/48000]\n",
            "loss: 0.051644  [ 6400/48000]\n",
            "loss: 0.019087  [12800/48000]\n",
            "loss: 0.039233  [19200/48000]\n",
            "loss: 0.015513  [25600/48000]\n",
            "loss: 0.034903  [32000/48000]\n",
            "loss: 0.019546  [38400/48000]\n",
            "loss: 0.032623  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.093151  [    0/48000]\n",
            "loss: 0.046380  [ 6400/48000]\n",
            "loss: 0.016356  [12800/48000]\n",
            "loss: 0.034406  [19200/48000]\n",
            "loss: 0.012563  [25600/48000]\n",
            "loss: 0.027211  [32000/48000]\n",
            "loss: 0.016079  [38400/48000]\n",
            "loss: 0.028506  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.081839  [    0/48000]\n",
            "loss: 0.039847  [ 6400/48000]\n",
            "loss: 0.013932  [12800/48000]\n",
            "loss: 0.030676  [19200/48000]\n",
            "loss: 0.010533  [25600/48000]\n",
            "loss: 0.020974  [32000/48000]\n",
            "loss: 0.013376  [38400/48000]\n",
            "loss: 0.025505  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.109054 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=relu, weight_decay=0, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 6.436568  [    0/48000]\n",
            "loss: 0.589239  [ 6400/48000]\n",
            "loss: 0.329862  [12800/48000]\n",
            "loss: 0.379296  [19200/48000]\n",
            "loss: 0.263656  [25600/48000]\n",
            "loss: 0.218520  [32000/48000]\n",
            "loss: 0.236056  [38400/48000]\n",
            "loss: 0.264492  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.233738  [    0/48000]\n",
            "loss: 0.310911  [ 6400/48000]\n",
            "loss: 0.138884  [12800/48000]\n",
            "loss: 0.211144  [19200/48000]\n",
            "loss: 0.135064  [25600/48000]\n",
            "loss: 0.161829  [32000/48000]\n",
            "loss: 0.106325  [38400/48000]\n",
            "loss: 0.152351  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.208557  [    0/48000]\n",
            "loss: 0.237643  [ 6400/48000]\n",
            "loss: 0.104441  [12800/48000]\n",
            "loss: 0.124040  [19200/48000]\n",
            "loss: 0.107523  [25600/48000]\n",
            "loss: 0.137370  [32000/48000]\n",
            "loss: 0.061762  [38400/48000]\n",
            "loss: 0.106632  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.201718  [    0/48000]\n",
            "loss: 0.185721  [ 6400/48000]\n",
            "loss: 0.082580  [12800/48000]\n",
            "loss: 0.077406  [19200/48000]\n",
            "loss: 0.092598  [25600/48000]\n",
            "loss: 0.116135  [32000/48000]\n",
            "loss: 0.042102  [38400/48000]\n",
            "loss: 0.079667  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.190981  [    0/48000]\n",
            "loss: 0.159637  [ 6400/48000]\n",
            "loss: 0.060972  [12800/48000]\n",
            "loss: 0.053721  [19200/48000]\n",
            "loss: 0.076261  [25600/48000]\n",
            "loss: 0.097534  [32000/48000]\n",
            "loss: 0.034185  [38400/48000]\n",
            "loss: 0.061866  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.176669  [    0/48000]\n",
            "loss: 0.140117  [ 6400/48000]\n",
            "loss: 0.046185  [12800/48000]\n",
            "loss: 0.039707  [19200/48000]\n",
            "loss: 0.062905  [25600/48000]\n",
            "loss: 0.081883  [32000/48000]\n",
            "loss: 0.028723  [38400/48000]\n",
            "loss: 0.050764  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.163707  [    0/48000]\n",
            "loss: 0.125476  [ 6400/48000]\n",
            "loss: 0.034943  [12800/48000]\n",
            "loss: 0.031722  [19200/48000]\n",
            "loss: 0.053170  [25600/48000]\n",
            "loss: 0.068496  [32000/48000]\n",
            "loss: 0.024474  [38400/48000]\n",
            "loss: 0.042450  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.149256  [    0/48000]\n",
            "loss: 0.113646  [ 6400/48000]\n",
            "loss: 0.026902  [12800/48000]\n",
            "loss: 0.026842  [19200/48000]\n",
            "loss: 0.044485  [25600/48000]\n",
            "loss: 0.057935  [32000/48000]\n",
            "loss: 0.021869  [38400/48000]\n",
            "loss: 0.036521  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.135832  [    0/48000]\n",
            "loss: 0.103672  [ 6400/48000]\n",
            "loss: 0.021513  [12800/48000]\n",
            "loss: 0.023578  [19200/48000]\n",
            "loss: 0.034500  [25600/48000]\n",
            "loss: 0.047787  [32000/48000]\n",
            "loss: 0.018878  [38400/48000]\n",
            "loss: 0.032184  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.122680  [    0/48000]\n",
            "loss: 0.094001  [ 6400/48000]\n",
            "loss: 0.018668  [12800/48000]\n",
            "loss: 0.020825  [19200/48000]\n",
            "loss: 0.029863  [25600/48000]\n",
            "loss: 0.038658  [32000/48000]\n",
            "loss: 0.016833  [38400/48000]\n",
            "loss: 0.027766  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.112475  [    0/48000]\n",
            "loss: 0.086389  [ 6400/48000]\n",
            "loss: 0.016121  [12800/48000]\n",
            "loss: 0.018844  [19200/48000]\n",
            "loss: 0.025905  [25600/48000]\n",
            "loss: 0.031315  [32000/48000]\n",
            "loss: 0.015224  [38400/48000]\n",
            "loss: 0.024025  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.100593  [    0/48000]\n",
            "loss: 0.078988  [ 6400/48000]\n",
            "loss: 0.014044  [12800/48000]\n",
            "loss: 0.016570  [19200/48000]\n",
            "loss: 0.021690  [25600/48000]\n",
            "loss: 0.024894  [32000/48000]\n",
            "loss: 0.013691  [38400/48000]\n",
            "loss: 0.021645  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.089410  [    0/48000]\n",
            "loss: 0.071451  [ 6400/48000]\n",
            "loss: 0.012430  [12800/48000]\n",
            "loss: 0.014455  [19200/48000]\n",
            "loss: 0.018326  [25600/48000]\n",
            "loss: 0.020583  [32000/48000]\n",
            "loss: 0.012140  [38400/48000]\n",
            "loss: 0.019372  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.079818  [    0/48000]\n",
            "loss: 0.064948  [ 6400/48000]\n",
            "loss: 0.010719  [12800/48000]\n",
            "loss: 0.012913  [19200/48000]\n",
            "loss: 0.016117  [25600/48000]\n",
            "loss: 0.017480  [32000/48000]\n",
            "loss: 0.010564  [38400/48000]\n",
            "loss: 0.017420  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.069211  [    0/48000]\n",
            "loss: 0.058807  [ 6400/48000]\n",
            "loss: 0.009402  [12800/48000]\n",
            "loss: 0.011609  [19200/48000]\n",
            "loss: 0.014619  [25600/48000]\n",
            "loss: 0.014889  [32000/48000]\n",
            "loss: 0.009414  [38400/48000]\n",
            "loss: 0.015343  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.059142  [    0/48000]\n",
            "loss: 0.052644  [ 6400/48000]\n",
            "loss: 0.008287  [12800/48000]\n",
            "loss: 0.010276  [19200/48000]\n",
            "loss: 0.012790  [25600/48000]\n",
            "loss: 0.013407  [32000/48000]\n",
            "loss: 0.008058  [38400/48000]\n",
            "loss: 0.013995  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.050075  [    0/48000]\n",
            "loss: 0.046387  [ 6400/48000]\n",
            "loss: 0.007399  [12800/48000]\n",
            "loss: 0.009163  [19200/48000]\n",
            "loss: 0.011556  [25600/48000]\n",
            "loss: 0.011999  [32000/48000]\n",
            "loss: 0.007167  [38400/48000]\n",
            "loss: 0.012479  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.040719  [    0/48000]\n",
            "loss: 0.041001  [ 6400/48000]\n",
            "loss: 0.006675  [12800/48000]\n",
            "loss: 0.008231  [19200/48000]\n",
            "loss: 0.010522  [25600/48000]\n",
            "loss: 0.011014  [32000/48000]\n",
            "loss: 0.006351  [38400/48000]\n",
            "loss: 0.011317  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.032702  [    0/48000]\n",
            "loss: 0.035905  [ 6400/48000]\n",
            "loss: 0.005985  [12800/48000]\n",
            "loss: 0.007470  [19200/48000]\n",
            "loss: 0.009298  [25600/48000]\n",
            "loss: 0.010028  [32000/48000]\n",
            "loss: 0.005650  [38400/48000]\n",
            "loss: 0.010240  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.026226  [    0/48000]\n",
            "loss: 0.031354  [ 6400/48000]\n",
            "loss: 0.005264  [12800/48000]\n",
            "loss: 0.006785  [19200/48000]\n",
            "loss: 0.008622  [25600/48000]\n",
            "loss: 0.009309  [32000/48000]\n",
            "loss: 0.005169  [38400/48000]\n",
            "loss: 0.009346  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 97.1%, Avg loss: 0.104583 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "New Best model: {'num_layers': 4, 'layer_size': 512, 'activation': 'relu', 'weight_decay': 0, 'epochs': 20} with accuracy 0.9706666666666667\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=relu, weight_decay=0.0001, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.616487  [    0/48000]\n",
            "loss: 0.607706  [ 6400/48000]\n",
            "loss: 0.304771  [12800/48000]\n",
            "loss: 0.387192  [19200/48000]\n",
            "loss: 0.417643  [25600/48000]\n",
            "loss: 0.261810  [32000/48000]\n",
            "loss: 0.171806  [38400/48000]\n",
            "loss: 0.284112  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.339979  [    0/48000]\n",
            "loss: 0.272450  [ 6400/48000]\n",
            "loss: 0.134802  [12800/48000]\n",
            "loss: 0.180528  [19200/48000]\n",
            "loss: 0.240413  [25600/48000]\n",
            "loss: 0.201176  [32000/48000]\n",
            "loss: 0.088591  [38400/48000]\n",
            "loss: 0.209372  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.277266  [    0/48000]\n",
            "loss: 0.185027  [ 6400/48000]\n",
            "loss: 0.097632  [12800/48000]\n",
            "loss: 0.095181  [19200/48000]\n",
            "loss: 0.154884  [25600/48000]\n",
            "loss: 0.175206  [32000/48000]\n",
            "loss: 0.055072  [38400/48000]\n",
            "loss: 0.164206  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.242551  [    0/48000]\n",
            "loss: 0.137653  [ 6400/48000]\n",
            "loss: 0.077558  [12800/48000]\n",
            "loss: 0.059599  [19200/48000]\n",
            "loss: 0.113146  [25600/48000]\n",
            "loss: 0.151953  [32000/48000]\n",
            "loss: 0.041196  [38400/48000]\n",
            "loss: 0.122477  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.213689  [    0/48000]\n",
            "loss: 0.107299  [ 6400/48000]\n",
            "loss: 0.064375  [12800/48000]\n",
            "loss: 0.039258  [19200/48000]\n",
            "loss: 0.091795  [25600/48000]\n",
            "loss: 0.128748  [32000/48000]\n",
            "loss: 0.033204  [38400/48000]\n",
            "loss: 0.089123  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.8%, Avg loss: 0.131735 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=relu, weight_decay=0.0001, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 6.992445  [    0/48000]\n",
            "loss: 0.500980  [ 6400/48000]\n",
            "loss: 0.228517  [12800/48000]\n",
            "loss: 0.231860  [19200/48000]\n",
            "loss: 0.263768  [25600/48000]\n",
            "loss: 0.258583  [32000/48000]\n",
            "loss: 0.161333  [38400/48000]\n",
            "loss: 0.202675  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.222057  [    0/48000]\n",
            "loss: 0.230946  [ 6400/48000]\n",
            "loss: 0.120150  [12800/48000]\n",
            "loss: 0.102171  [19200/48000]\n",
            "loss: 0.106735  [25600/48000]\n",
            "loss: 0.158483  [32000/48000]\n",
            "loss: 0.082976  [38400/48000]\n",
            "loss: 0.121991  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.176228  [    0/48000]\n",
            "loss: 0.177277  [ 6400/48000]\n",
            "loss: 0.093102  [12800/48000]\n",
            "loss: 0.058003  [19200/48000]\n",
            "loss: 0.061816  [25600/48000]\n",
            "loss: 0.113191  [32000/48000]\n",
            "loss: 0.056367  [38400/48000]\n",
            "loss: 0.087793  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.144420  [    0/48000]\n",
            "loss: 0.146770  [ 6400/48000]\n",
            "loss: 0.077982  [12800/48000]\n",
            "loss: 0.040478  [19200/48000]\n",
            "loss: 0.041768  [25600/48000]\n",
            "loss: 0.083915  [32000/48000]\n",
            "loss: 0.045382  [38400/48000]\n",
            "loss: 0.067148  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.121573  [    0/48000]\n",
            "loss: 0.125425  [ 6400/48000]\n",
            "loss: 0.066590  [12800/48000]\n",
            "loss: 0.031142  [19200/48000]\n",
            "loss: 0.031975  [25600/48000]\n",
            "loss: 0.068510  [32000/48000]\n",
            "loss: 0.035259  [38400/48000]\n",
            "loss: 0.051751  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.102151  [    0/48000]\n",
            "loss: 0.105017  [ 6400/48000]\n",
            "loss: 0.061751  [12800/48000]\n",
            "loss: 0.025564  [19200/48000]\n",
            "loss: 0.024996  [25600/48000]\n",
            "loss: 0.058299  [32000/48000]\n",
            "loss: 0.029857  [38400/48000]\n",
            "loss: 0.040481  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.084108  [    0/48000]\n",
            "loss: 0.090083  [ 6400/48000]\n",
            "loss: 0.056353  [12800/48000]\n",
            "loss: 0.020710  [19200/48000]\n",
            "loss: 0.021968  [25600/48000]\n",
            "loss: 0.048362  [32000/48000]\n",
            "loss: 0.024367  [38400/48000]\n",
            "loss: 0.033779  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.070695  [    0/48000]\n",
            "loss: 0.078274  [ 6400/48000]\n",
            "loss: 0.047951  [12800/48000]\n",
            "loss: 0.018074  [19200/48000]\n",
            "loss: 0.018335  [25600/48000]\n",
            "loss: 0.040108  [32000/48000]\n",
            "loss: 0.019823  [38400/48000]\n",
            "loss: 0.028890  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.057981  [    0/48000]\n",
            "loss: 0.067937  [ 6400/48000]\n",
            "loss: 0.040007  [12800/48000]\n",
            "loss: 0.015413  [19200/48000]\n",
            "loss: 0.015431  [25600/48000]\n",
            "loss: 0.033620  [32000/48000]\n",
            "loss: 0.016912  [38400/48000]\n",
            "loss: 0.025620  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.047118  [    0/48000]\n",
            "loss: 0.056490  [ 6400/48000]\n",
            "loss: 0.031072  [12800/48000]\n",
            "loss: 0.013046  [19200/48000]\n",
            "loss: 0.012930  [25600/48000]\n",
            "loss: 0.028075  [32000/48000]\n",
            "loss: 0.015469  [38400/48000]\n",
            "loss: 0.022578  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.5%, Avg loss: 0.114456 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=relu, weight_decay=0.0001, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.688332  [    0/48000]\n",
            "loss: 0.541023  [ 6400/48000]\n",
            "loss: 0.306931  [12800/48000]\n",
            "loss: 0.367439  [19200/48000]\n",
            "loss: 0.292895  [25600/48000]\n",
            "loss: 0.340834  [32000/48000]\n",
            "loss: 0.244992  [38400/48000]\n",
            "loss: 0.224576  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.209628  [    0/48000]\n",
            "loss: 0.175862  [ 6400/48000]\n",
            "loss: 0.140875  [12800/48000]\n",
            "loss: 0.202197  [19200/48000]\n",
            "loss: 0.143509  [25600/48000]\n",
            "loss: 0.210784  [32000/48000]\n",
            "loss: 0.129665  [38400/48000]\n",
            "loss: 0.147765  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.199428  [    0/48000]\n",
            "loss: 0.144052  [ 6400/48000]\n",
            "loss: 0.106129  [12800/48000]\n",
            "loss: 0.117022  [19200/48000]\n",
            "loss: 0.089934  [25600/48000]\n",
            "loss: 0.165901  [32000/48000]\n",
            "loss: 0.069597  [38400/48000]\n",
            "loss: 0.111173  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.192927  [    0/48000]\n",
            "loss: 0.129061  [ 6400/48000]\n",
            "loss: 0.074264  [12800/48000]\n",
            "loss: 0.072560  [19200/48000]\n",
            "loss: 0.066951  [25600/48000]\n",
            "loss: 0.138289  [32000/48000]\n",
            "loss: 0.044544  [38400/48000]\n",
            "loss: 0.084709  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.181033  [    0/48000]\n",
            "loss: 0.110730  [ 6400/48000]\n",
            "loss: 0.056978  [12800/48000]\n",
            "loss: 0.050906  [19200/48000]\n",
            "loss: 0.052646  [25600/48000]\n",
            "loss: 0.116668  [32000/48000]\n",
            "loss: 0.032138  [38400/48000]\n",
            "loss: 0.061468  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.170949  [    0/48000]\n",
            "loss: 0.093413  [ 6400/48000]\n",
            "loss: 0.044892  [12800/48000]\n",
            "loss: 0.040354  [19200/48000]\n",
            "loss: 0.040575  [25600/48000]\n",
            "loss: 0.102489  [32000/48000]\n",
            "loss: 0.024373  [38400/48000]\n",
            "loss: 0.044471  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.159953  [    0/48000]\n",
            "loss: 0.080850  [ 6400/48000]\n",
            "loss: 0.036424  [12800/48000]\n",
            "loss: 0.033998  [19200/48000]\n",
            "loss: 0.032926  [25600/48000]\n",
            "loss: 0.086602  [32000/48000]\n",
            "loss: 0.018733  [38400/48000]\n",
            "loss: 0.034980  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.148824  [    0/48000]\n",
            "loss: 0.069018  [ 6400/48000]\n",
            "loss: 0.029817  [12800/48000]\n",
            "loss: 0.028513  [19200/48000]\n",
            "loss: 0.027904  [25600/48000]\n",
            "loss: 0.074191  [32000/48000]\n",
            "loss: 0.014903  [38400/48000]\n",
            "loss: 0.026772  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.137893  [    0/48000]\n",
            "loss: 0.059204  [ 6400/48000]\n",
            "loss: 0.024600  [12800/48000]\n",
            "loss: 0.024477  [19200/48000]\n",
            "loss: 0.023753  [25600/48000]\n",
            "loss: 0.060740  [32000/48000]\n",
            "loss: 0.012373  [38400/48000]\n",
            "loss: 0.021367  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.125973  [    0/48000]\n",
            "loss: 0.051290  [ 6400/48000]\n",
            "loss: 0.020209  [12800/48000]\n",
            "loss: 0.020601  [19200/48000]\n",
            "loss: 0.021271  [25600/48000]\n",
            "loss: 0.049585  [32000/48000]\n",
            "loss: 0.010348  [38400/48000]\n",
            "loss: 0.018536  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.115897  [    0/48000]\n",
            "loss: 0.043567  [ 6400/48000]\n",
            "loss: 0.017753  [12800/48000]\n",
            "loss: 0.017725  [19200/48000]\n",
            "loss: 0.019179  [25600/48000]\n",
            "loss: 0.039717  [32000/48000]\n",
            "loss: 0.009224  [38400/48000]\n",
            "loss: 0.016240  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.104609  [    0/48000]\n",
            "loss: 0.037390  [ 6400/48000]\n",
            "loss: 0.015384  [12800/48000]\n",
            "loss: 0.015575  [19200/48000]\n",
            "loss: 0.018004  [25600/48000]\n",
            "loss: 0.030819  [32000/48000]\n",
            "loss: 0.008361  [38400/48000]\n",
            "loss: 0.014794  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.093473  [    0/48000]\n",
            "loss: 0.032470  [ 6400/48000]\n",
            "loss: 0.014396  [12800/48000]\n",
            "loss: 0.013782  [19200/48000]\n",
            "loss: 0.016322  [25600/48000]\n",
            "loss: 0.023998  [32000/48000]\n",
            "loss: 0.007737  [38400/48000]\n",
            "loss: 0.013576  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.082453  [    0/48000]\n",
            "loss: 0.027861  [ 6400/48000]\n",
            "loss: 0.012408  [12800/48000]\n",
            "loss: 0.012198  [19200/48000]\n",
            "loss: 0.015002  [25600/48000]\n",
            "loss: 0.019053  [32000/48000]\n",
            "loss: 0.007049  [38400/48000]\n",
            "loss: 0.012486  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.070768  [    0/48000]\n",
            "loss: 0.024592  [ 6400/48000]\n",
            "loss: 0.010928  [12800/48000]\n",
            "loss: 0.010922  [19200/48000]\n",
            "loss: 0.013687  [25600/48000]\n",
            "loss: 0.016160  [32000/48000]\n",
            "loss: 0.006491  [38400/48000]\n",
            "loss: 0.011342  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.059620  [    0/48000]\n",
            "loss: 0.021620  [ 6400/48000]\n",
            "loss: 0.010477  [12800/48000]\n",
            "loss: 0.009706  [19200/48000]\n",
            "loss: 0.011332  [25600/48000]\n",
            "loss: 0.013942  [32000/48000]\n",
            "loss: 0.005946  [38400/48000]\n",
            "loss: 0.010553  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.048183  [    0/48000]\n",
            "loss: 0.018936  [ 6400/48000]\n",
            "loss: 0.009177  [12800/48000]\n",
            "loss: 0.008782  [19200/48000]\n",
            "loss: 0.009973  [25600/48000]\n",
            "loss: 0.012242  [32000/48000]\n",
            "loss: 0.005445  [38400/48000]\n",
            "loss: 0.009531  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.037630  [    0/48000]\n",
            "loss: 0.016948  [ 6400/48000]\n",
            "loss: 0.008235  [12800/48000]\n",
            "loss: 0.007954  [19200/48000]\n",
            "loss: 0.008940  [25600/48000]\n",
            "loss: 0.010816  [32000/48000]\n",
            "loss: 0.004943  [38400/48000]\n",
            "loss: 0.008771  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.029742  [    0/48000]\n",
            "loss: 0.015372  [ 6400/48000]\n",
            "loss: 0.007432  [12800/48000]\n",
            "loss: 0.007236  [19200/48000]\n",
            "loss: 0.007664  [25600/48000]\n",
            "loss: 0.009846  [32000/48000]\n",
            "loss: 0.004516  [38400/48000]\n",
            "loss: 0.007889  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.022884  [    0/48000]\n",
            "loss: 0.013817  [ 6400/48000]\n",
            "loss: 0.006917  [12800/48000]\n",
            "loss: 0.006607  [19200/48000]\n",
            "loss: 0.006435  [25600/48000]\n",
            "loss: 0.009083  [32000/48000]\n",
            "loss: 0.004157  [38400/48000]\n",
            "loss: 0.007306  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 0.104460 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=relu, weight_decay=0.001, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.347633  [    0/48000]\n",
            "loss: 0.523017  [ 6400/48000]\n",
            "loss: 0.341034  [12800/48000]\n",
            "loss: 0.334031  [19200/48000]\n",
            "loss: 0.260452  [25600/48000]\n",
            "loss: 0.163602  [32000/48000]\n",
            "loss: 0.177062  [38400/48000]\n",
            "loss: 0.233653  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.288801  [    0/48000]\n",
            "loss: 0.165762  [ 6400/48000]\n",
            "loss: 0.149954  [12800/48000]\n",
            "loss: 0.189270  [19200/48000]\n",
            "loss: 0.144729  [25600/48000]\n",
            "loss: 0.116982  [32000/48000]\n",
            "loss: 0.081361  [38400/48000]\n",
            "loss: 0.158887  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.190064  [    0/48000]\n",
            "loss: 0.121402  [ 6400/48000]\n",
            "loss: 0.105229  [12800/48000]\n",
            "loss: 0.117804  [19200/48000]\n",
            "loss: 0.116960  [25600/48000]\n",
            "loss: 0.105412  [32000/48000]\n",
            "loss: 0.052453  [38400/48000]\n",
            "loss: 0.107389  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.156409  [    0/48000]\n",
            "loss: 0.095458  [ 6400/48000]\n",
            "loss: 0.080767  [12800/48000]\n",
            "loss: 0.084467  [19200/48000]\n",
            "loss: 0.100666  [25600/48000]\n",
            "loss: 0.093446  [32000/48000]\n",
            "loss: 0.038771  [38400/48000]\n",
            "loss: 0.079543  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.136869  [    0/48000]\n",
            "loss: 0.079218  [ 6400/48000]\n",
            "loss: 0.060502  [12800/48000]\n",
            "loss: 0.064587  [19200/48000]\n",
            "loss: 0.083485  [25600/48000]\n",
            "loss: 0.078476  [32000/48000]\n",
            "loss: 0.031070  [38400/48000]\n",
            "loss: 0.059018  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.126435 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=relu, weight_decay=0.001, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.267077  [    0/48000]\n",
            "loss: 0.571323  [ 6400/48000]\n",
            "loss: 0.279975  [12800/48000]\n",
            "loss: 0.326098  [19200/48000]\n",
            "loss: 0.268405  [25600/48000]\n",
            "loss: 0.233166  [32000/48000]\n",
            "loss: 0.184859  [38400/48000]\n",
            "loss: 0.205244  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.276932  [    0/48000]\n",
            "loss: 0.211607  [ 6400/48000]\n",
            "loss: 0.126917  [12800/48000]\n",
            "loss: 0.117239  [19200/48000]\n",
            "loss: 0.128752  [25600/48000]\n",
            "loss: 0.131445  [32000/48000]\n",
            "loss: 0.098526  [38400/48000]\n",
            "loss: 0.137268  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.228836  [    0/48000]\n",
            "loss: 0.174548  [ 6400/48000]\n",
            "loss: 0.082631  [12800/48000]\n",
            "loss: 0.065173  [19200/48000]\n",
            "loss: 0.094859  [25600/48000]\n",
            "loss: 0.094237  [32000/48000]\n",
            "loss: 0.066108  [38400/48000]\n",
            "loss: 0.102806  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.203043  [    0/48000]\n",
            "loss: 0.150016  [ 6400/48000]\n",
            "loss: 0.055491  [12800/48000]\n",
            "loss: 0.046746  [19200/48000]\n",
            "loss: 0.076783  [25600/48000]\n",
            "loss: 0.072172  [32000/48000]\n",
            "loss: 0.048899  [38400/48000]\n",
            "loss: 0.076039  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.176473  [    0/48000]\n",
            "loss: 0.130493  [ 6400/48000]\n",
            "loss: 0.043966  [12800/48000]\n",
            "loss: 0.035814  [19200/48000]\n",
            "loss: 0.061559  [25600/48000]\n",
            "loss: 0.057832  [32000/48000]\n",
            "loss: 0.035982  [38400/48000]\n",
            "loss: 0.056323  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.157796  [    0/48000]\n",
            "loss: 0.109215  [ 6400/48000]\n",
            "loss: 0.033961  [12800/48000]\n",
            "loss: 0.029148  [19200/48000]\n",
            "loss: 0.052108  [25600/48000]\n",
            "loss: 0.047931  [32000/48000]\n",
            "loss: 0.029313  [38400/48000]\n",
            "loss: 0.044561  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.141850  [    0/48000]\n",
            "loss: 0.093890  [ 6400/48000]\n",
            "loss: 0.028145  [12800/48000]\n",
            "loss: 0.024754  [19200/48000]\n",
            "loss: 0.042511  [25600/48000]\n",
            "loss: 0.041333  [32000/48000]\n",
            "loss: 0.023984  [38400/48000]\n",
            "loss: 0.038308  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.130296  [    0/48000]\n",
            "loss: 0.079878  [ 6400/48000]\n",
            "loss: 0.022963  [12800/48000]\n",
            "loss: 0.020901  [19200/48000]\n",
            "loss: 0.035475  [25600/48000]\n",
            "loss: 0.033991  [32000/48000]\n",
            "loss: 0.019180  [38400/48000]\n",
            "loss: 0.032769  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.122049  [    0/48000]\n",
            "loss: 0.069705  [ 6400/48000]\n",
            "loss: 0.020130  [12800/48000]\n",
            "loss: 0.017989  [19200/48000]\n",
            "loss: 0.029242  [25600/48000]\n",
            "loss: 0.028694  [32000/48000]\n",
            "loss: 0.015780  [38400/48000]\n",
            "loss: 0.028930  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.112581  [    0/48000]\n",
            "loss: 0.059521  [ 6400/48000]\n",
            "loss: 0.018156  [12800/48000]\n",
            "loss: 0.016031  [19200/48000]\n",
            "loss: 0.024620  [25600/48000]\n",
            "loss: 0.023518  [32000/48000]\n",
            "loss: 0.014218  [38400/48000]\n",
            "loss: 0.025070  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.6%, Avg loss: 0.109836 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=relu, weight_decay=0.001, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.040874  [    0/48000]\n",
            "loss: 0.479984  [ 6400/48000]\n",
            "loss: 0.247546  [12800/48000]\n",
            "loss: 0.311393  [19200/48000]\n",
            "loss: 0.302314  [25600/48000]\n",
            "loss: 0.232753  [32000/48000]\n",
            "loss: 0.138273  [38400/48000]\n",
            "loss: 0.288563  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.230453  [    0/48000]\n",
            "loss: 0.209520  [ 6400/48000]\n",
            "loss: 0.153637  [12800/48000]\n",
            "loss: 0.149742  [19200/48000]\n",
            "loss: 0.151382  [25600/48000]\n",
            "loss: 0.131478  [32000/48000]\n",
            "loss: 0.063779  [38400/48000]\n",
            "loss: 0.189360  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.193053  [    0/48000]\n",
            "loss: 0.151388  [ 6400/48000]\n",
            "loss: 0.133817  [12800/48000]\n",
            "loss: 0.095441  [19200/48000]\n",
            "loss: 0.106979  [25600/48000]\n",
            "loss: 0.097118  [32000/48000]\n",
            "loss: 0.037397  [38400/48000]\n",
            "loss: 0.133466  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.169284  [    0/48000]\n",
            "loss: 0.129315  [ 6400/48000]\n",
            "loss: 0.112198  [12800/48000]\n",
            "loss: 0.067811  [19200/48000]\n",
            "loss: 0.077631  [25600/48000]\n",
            "loss: 0.077231  [32000/48000]\n",
            "loss: 0.024331  [38400/48000]\n",
            "loss: 0.094993  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.150738  [    0/48000]\n",
            "loss: 0.111777  [ 6400/48000]\n",
            "loss: 0.094697  [12800/48000]\n",
            "loss: 0.052981  [19200/48000]\n",
            "loss: 0.059207  [25600/48000]\n",
            "loss: 0.065824  [32000/48000]\n",
            "loss: 0.018697  [38400/48000]\n",
            "loss: 0.068203  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.129696  [    0/48000]\n",
            "loss: 0.092229  [ 6400/48000]\n",
            "loss: 0.077381  [12800/48000]\n",
            "loss: 0.042965  [19200/48000]\n",
            "loss: 0.046534  [25600/48000]\n",
            "loss: 0.055562  [32000/48000]\n",
            "loss: 0.015323  [38400/48000]\n",
            "loss: 0.051747  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.109146  [    0/48000]\n",
            "loss: 0.078758  [ 6400/48000]\n",
            "loss: 0.060532  [12800/48000]\n",
            "loss: 0.037350  [19200/48000]\n",
            "loss: 0.037971  [25600/48000]\n",
            "loss: 0.047156  [32000/48000]\n",
            "loss: 0.013301  [38400/48000]\n",
            "loss: 0.039680  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.091948  [    0/48000]\n",
            "loss: 0.067620  [ 6400/48000]\n",
            "loss: 0.049532  [12800/48000]\n",
            "loss: 0.032307  [19200/48000]\n",
            "loss: 0.030983  [25600/48000]\n",
            "loss: 0.040428  [32000/48000]\n",
            "loss: 0.011203  [38400/48000]\n",
            "loss: 0.032746  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.075679  [    0/48000]\n",
            "loss: 0.057694  [ 6400/48000]\n",
            "loss: 0.039012  [12800/48000]\n",
            "loss: 0.028302  [19200/48000]\n",
            "loss: 0.025975  [25600/48000]\n",
            "loss: 0.034795  [32000/48000]\n",
            "loss: 0.009901  [38400/48000]\n",
            "loss: 0.028124  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.063152  [    0/48000]\n",
            "loss: 0.050555  [ 6400/48000]\n",
            "loss: 0.031875  [12800/48000]\n",
            "loss: 0.025045  [19200/48000]\n",
            "loss: 0.022345  [25600/48000]\n",
            "loss: 0.029564  [32000/48000]\n",
            "loss: 0.008862  [38400/48000]\n",
            "loss: 0.024127  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.052412  [    0/48000]\n",
            "loss: 0.043644  [ 6400/48000]\n",
            "loss: 0.027442  [12800/48000]\n",
            "loss: 0.022238  [19200/48000]\n",
            "loss: 0.019367  [25600/48000]\n",
            "loss: 0.025707  [32000/48000]\n",
            "loss: 0.007973  [38400/48000]\n",
            "loss: 0.020911  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.044478  [    0/48000]\n",
            "loss: 0.038371  [ 6400/48000]\n",
            "loss: 0.023189  [12800/48000]\n",
            "loss: 0.019757  [19200/48000]\n",
            "loss: 0.017437  [25600/48000]\n",
            "loss: 0.022654  [32000/48000]\n",
            "loss: 0.007386  [38400/48000]\n",
            "loss: 0.018796  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.038622  [    0/48000]\n",
            "loss: 0.033967  [ 6400/48000]\n",
            "loss: 0.020299  [12800/48000]\n",
            "loss: 0.017264  [19200/48000]\n",
            "loss: 0.015689  [25600/48000]\n",
            "loss: 0.019732  [32000/48000]\n",
            "loss: 0.006356  [38400/48000]\n",
            "loss: 0.016641  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.033272  [    0/48000]\n",
            "loss: 0.030104  [ 6400/48000]\n",
            "loss: 0.017758  [12800/48000]\n",
            "loss: 0.015266  [19200/48000]\n",
            "loss: 0.014182  [25600/48000]\n",
            "loss: 0.017353  [32000/48000]\n",
            "loss: 0.005601  [38400/48000]\n",
            "loss: 0.014893  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.028577  [    0/48000]\n",
            "loss: 0.026480  [ 6400/48000]\n",
            "loss: 0.015726  [12800/48000]\n",
            "loss: 0.013348  [19200/48000]\n",
            "loss: 0.012579  [25600/48000]\n",
            "loss: 0.015714  [32000/48000]\n",
            "loss: 0.005099  [38400/48000]\n",
            "loss: 0.013263  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.024742  [    0/48000]\n",
            "loss: 0.023853  [ 6400/48000]\n",
            "loss: 0.014057  [12800/48000]\n",
            "loss: 0.011966  [19200/48000]\n",
            "loss: 0.011326  [25600/48000]\n",
            "loss: 0.013999  [32000/48000]\n",
            "loss: 0.004681  [38400/48000]\n",
            "loss: 0.012051  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.021677  [    0/48000]\n",
            "loss: 0.021450  [ 6400/48000]\n",
            "loss: 0.012252  [12800/48000]\n",
            "loss: 0.010537  [19200/48000]\n",
            "loss: 0.010387  [25600/48000]\n",
            "loss: 0.012875  [32000/48000]\n",
            "loss: 0.004217  [38400/48000]\n",
            "loss: 0.011041  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.018923  [    0/48000]\n",
            "loss: 0.019454  [ 6400/48000]\n",
            "loss: 0.011082  [12800/48000]\n",
            "loss: 0.009606  [19200/48000]\n",
            "loss: 0.009360  [25600/48000]\n",
            "loss: 0.011924  [32000/48000]\n",
            "loss: 0.003962  [38400/48000]\n",
            "loss: 0.010115  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.016720  [    0/48000]\n",
            "loss: 0.017645  [ 6400/48000]\n",
            "loss: 0.010008  [12800/48000]\n",
            "loss: 0.008851  [19200/48000]\n",
            "loss: 0.008712  [25600/48000]\n",
            "loss: 0.010887  [32000/48000]\n",
            "loss: 0.003653  [38400/48000]\n",
            "loss: 0.009254  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.014962  [    0/48000]\n",
            "loss: 0.016349  [ 6400/48000]\n",
            "loss: 0.009162  [12800/48000]\n",
            "loss: 0.008073  [19200/48000]\n",
            "loss: 0.008046  [25600/48000]\n",
            "loss: 0.010073  [32000/48000]\n",
            "loss: 0.003389  [38400/48000]\n",
            "loss: 0.008542  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 97.1%, Avg loss: 0.099436 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=sigmoid, weight_decay=0, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.318236  [    0/48000]\n",
            "loss: 2.298561  [ 6400/48000]\n",
            "loss: 2.299114  [12800/48000]\n",
            "loss: 2.305046  [19200/48000]\n",
            "loss: 2.303201  [25600/48000]\n",
            "loss: 2.305430  [32000/48000]\n",
            "loss: 2.288652  [38400/48000]\n",
            "loss: 2.302955  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.301735  [    0/48000]\n",
            "loss: 2.304326  [ 6400/48000]\n",
            "loss: 2.302222  [12800/48000]\n",
            "loss: 2.303961  [19200/48000]\n",
            "loss: 2.301451  [25600/48000]\n",
            "loss: 2.303654  [32000/48000]\n",
            "loss: 2.286897  [38400/48000]\n",
            "loss: 2.301294  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.299988  [    0/48000]\n",
            "loss: 2.302726  [ 6400/48000]\n",
            "loss: 2.300620  [12800/48000]\n",
            "loss: 2.302199  [19200/48000]\n",
            "loss: 2.299780  [25600/48000]\n",
            "loss: 2.301818  [32000/48000]\n",
            "loss: 2.285156  [38400/48000]\n",
            "loss: 2.299635  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.298190  [    0/48000]\n",
            "loss: 2.301093  [ 6400/48000]\n",
            "loss: 2.299006  [12800/48000]\n",
            "loss: 2.300355  [19200/48000]\n",
            "loss: 2.298189  [25600/48000]\n",
            "loss: 2.299938  [32000/48000]\n",
            "loss: 2.283375  [38400/48000]\n",
            "loss: 2.297956  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.296407  [    0/48000]\n",
            "loss: 2.299475  [ 6400/48000]\n",
            "loss: 2.297288  [12800/48000]\n",
            "loss: 2.298436  [19200/48000]\n",
            "loss: 2.296557  [25600/48000]\n",
            "loss: 2.298095  [32000/48000]\n",
            "loss: 2.281525  [38400/48000]\n",
            "loss: 2.296181  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 10.8%, Avg loss: 2.292755 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=sigmoid, weight_decay=0, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.320879  [    0/48000]\n",
            "loss: 2.313354  [ 6400/48000]\n",
            "loss: 2.304013  [12800/48000]\n",
            "loss: 2.304342  [19200/48000]\n",
            "loss: 2.303540  [25600/48000]\n",
            "loss: 2.305770  [32000/48000]\n",
            "loss: 2.289054  [38400/48000]\n",
            "loss: 2.302834  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.301600  [    0/48000]\n",
            "loss: 2.304468  [ 6400/48000]\n",
            "loss: 2.302221  [12800/48000]\n",
            "loss: 2.302726  [19200/48000]\n",
            "loss: 2.301728  [25600/48000]\n",
            "loss: 2.303773  [32000/48000]\n",
            "loss: 2.287154  [38400/48000]\n",
            "loss: 2.301059  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.299682  [    0/48000]\n",
            "loss: 2.302794  [ 6400/48000]\n",
            "loss: 2.300403  [12800/48000]\n",
            "loss: 2.300763  [19200/48000]\n",
            "loss: 2.299894  [25600/48000]\n",
            "loss: 2.301747  [32000/48000]\n",
            "loss: 2.285242  [38400/48000]\n",
            "loss: 2.299274  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.297736  [    0/48000]\n",
            "loss: 2.301086  [ 6400/48000]\n",
            "loss: 2.298513  [12800/48000]\n",
            "loss: 2.298731  [19200/48000]\n",
            "loss: 2.298031  [25600/48000]\n",
            "loss: 2.299708  [32000/48000]\n",
            "loss: 2.283241  [38400/48000]\n",
            "loss: 2.297471  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.295692  [    0/48000]\n",
            "loss: 2.299357  [ 6400/48000]\n",
            "loss: 2.296530  [12800/48000]\n",
            "loss: 2.296637  [19200/48000]\n",
            "loss: 2.296148  [25600/48000]\n",
            "loss: 2.297687  [32000/48000]\n",
            "loss: 2.281174  [38400/48000]\n",
            "loss: 2.295658  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.293518  [    0/48000]\n",
            "loss: 2.297559  [ 6400/48000]\n",
            "loss: 2.294434  [12800/48000]\n",
            "loss: 2.294497  [19200/48000]\n",
            "loss: 2.294149  [25600/48000]\n",
            "loss: 2.295613  [32000/48000]\n",
            "loss: 2.279086  [38400/48000]\n",
            "loss: 2.293739  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.291296  [    0/48000]\n",
            "loss: 2.295640  [ 6400/48000]\n",
            "loss: 2.292268  [12800/48000]\n",
            "loss: 2.292273  [19200/48000]\n",
            "loss: 2.292072  [25600/48000]\n",
            "loss: 2.293426  [32000/48000]\n",
            "loss: 2.276965  [38400/48000]\n",
            "loss: 2.291659  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.288916  [    0/48000]\n",
            "loss: 2.293603  [ 6400/48000]\n",
            "loss: 2.290087  [12800/48000]\n",
            "loss: 2.289892  [19200/48000]\n",
            "loss: 2.289855  [25600/48000]\n",
            "loss: 2.291108  [32000/48000]\n",
            "loss: 2.274703  [38400/48000]\n",
            "loss: 2.289446  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.286310  [    0/48000]\n",
            "loss: 2.291417  [ 6400/48000]\n",
            "loss: 2.287813  [12800/48000]\n",
            "loss: 2.287333  [19200/48000]\n",
            "loss: 2.287507  [25600/48000]\n",
            "loss: 2.288658  [32000/48000]\n",
            "loss: 2.272219  [38400/48000]\n",
            "loss: 2.287143  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.283612  [    0/48000]\n",
            "loss: 2.289085  [ 6400/48000]\n",
            "loss: 2.285435  [12800/48000]\n",
            "loss: 2.284743  [19200/48000]\n",
            "loss: 2.285056  [25600/48000]\n",
            "loss: 2.286036  [32000/48000]\n",
            "loss: 2.269456  [38400/48000]\n",
            "loss: 2.284678  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 10.8%, Avg loss: 2.280140 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=sigmoid, weight_decay=0, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.398119  [    0/48000]\n",
            "loss: 2.332159  [ 6400/48000]\n",
            "loss: 2.307068  [12800/48000]\n",
            "loss: 2.305519  [19200/48000]\n",
            "loss: 2.304225  [25600/48000]\n",
            "loss: 2.304992  [32000/48000]\n",
            "loss: 2.287697  [38400/48000]\n",
            "loss: 2.303200  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.302326  [    0/48000]\n",
            "loss: 2.304556  [ 6400/48000]\n",
            "loss: 2.301976  [12800/48000]\n",
            "loss: 2.303488  [19200/48000]\n",
            "loss: 2.302179  [25600/48000]\n",
            "loss: 2.303025  [32000/48000]\n",
            "loss: 2.285703  [38400/48000]\n",
            "loss: 2.301382  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.300360  [    0/48000]\n",
            "loss: 2.302699  [ 6400/48000]\n",
            "loss: 2.300229  [12800/48000]\n",
            "loss: 2.301555  [19200/48000]\n",
            "loss: 2.300407  [25600/48000]\n",
            "loss: 2.301186  [32000/48000]\n",
            "loss: 2.283756  [38400/48000]\n",
            "loss: 2.299548  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.298440  [    0/48000]\n",
            "loss: 2.300820  [ 6400/48000]\n",
            "loss: 2.298482  [12800/48000]\n",
            "loss: 2.299520  [19200/48000]\n",
            "loss: 2.298526  [25600/48000]\n",
            "loss: 2.299292  [32000/48000]\n",
            "loss: 2.281793  [38400/48000]\n",
            "loss: 2.297791  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.296476  [    0/48000]\n",
            "loss: 2.298986  [ 6400/48000]\n",
            "loss: 2.296687  [12800/48000]\n",
            "loss: 2.297458  [19200/48000]\n",
            "loss: 2.296562  [25600/48000]\n",
            "loss: 2.297451  [32000/48000]\n",
            "loss: 2.279737  [38400/48000]\n",
            "loss: 2.295965  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.294344  [    0/48000]\n",
            "loss: 2.297101  [ 6400/48000]\n",
            "loss: 2.294832  [12800/48000]\n",
            "loss: 2.295337  [19200/48000]\n",
            "loss: 2.294583  [25600/48000]\n",
            "loss: 2.295568  [32000/48000]\n",
            "loss: 2.277586  [38400/48000]\n",
            "loss: 2.293988  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.292126  [    0/48000]\n",
            "loss: 2.295166  [ 6400/48000]\n",
            "loss: 2.292883  [12800/48000]\n",
            "loss: 2.293190  [19200/48000]\n",
            "loss: 2.292466  [25600/48000]\n",
            "loss: 2.293565  [32000/48000]\n",
            "loss: 2.275336  [38400/48000]\n",
            "loss: 2.291857  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.289925  [    0/48000]\n",
            "loss: 2.293223  [ 6400/48000]\n",
            "loss: 2.290828  [12800/48000]\n",
            "loss: 2.290908  [19200/48000]\n",
            "loss: 2.290303  [25600/48000]\n",
            "loss: 2.291412  [32000/48000]\n",
            "loss: 2.272933  [38400/48000]\n",
            "loss: 2.289644  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.287605  [    0/48000]\n",
            "loss: 2.291227  [ 6400/48000]\n",
            "loss: 2.288640  [12800/48000]\n",
            "loss: 2.288486  [19200/48000]\n",
            "loss: 2.288041  [25600/48000]\n",
            "loss: 2.289101  [32000/48000]\n",
            "loss: 2.270385  [38400/48000]\n",
            "loss: 2.287264  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.285148  [    0/48000]\n",
            "loss: 2.289036  [ 6400/48000]\n",
            "loss: 2.286288  [12800/48000]\n",
            "loss: 2.285947  [19200/48000]\n",
            "loss: 2.285676  [25600/48000]\n",
            "loss: 2.286507  [32000/48000]\n",
            "loss: 2.267616  [38400/48000]\n",
            "loss: 2.284695  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2.282402  [    0/48000]\n",
            "loss: 2.286636  [ 6400/48000]\n",
            "loss: 2.283755  [12800/48000]\n",
            "loss: 2.283221  [19200/48000]\n",
            "loss: 2.283106  [25600/48000]\n",
            "loss: 2.283708  [32000/48000]\n",
            "loss: 2.264568  [38400/48000]\n",
            "loss: 2.281993  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.279466  [    0/48000]\n",
            "loss: 2.284023  [ 6400/48000]\n",
            "loss: 2.280932  [12800/48000]\n",
            "loss: 2.280297  [19200/48000]\n",
            "loss: 2.280338  [25600/48000]\n",
            "loss: 2.280797  [32000/48000]\n",
            "loss: 2.261286  [38400/48000]\n",
            "loss: 2.279154  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2.276186  [    0/48000]\n",
            "loss: 2.281246  [ 6400/48000]\n",
            "loss: 2.277945  [12800/48000]\n",
            "loss: 2.277176  [19200/48000]\n",
            "loss: 2.277359  [25600/48000]\n",
            "loss: 2.277700  [32000/48000]\n",
            "loss: 2.257815  [38400/48000]\n",
            "loss: 2.276102  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 2.272685  [    0/48000]\n",
            "loss: 2.278216  [ 6400/48000]\n",
            "loss: 2.274626  [12800/48000]\n",
            "loss: 2.273673  [19200/48000]\n",
            "loss: 2.274103  [25600/48000]\n",
            "loss: 2.274218  [32000/48000]\n",
            "loss: 2.254084  [38400/48000]\n",
            "loss: 2.272767  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 2.268828  [    0/48000]\n",
            "loss: 2.274831  [ 6400/48000]\n",
            "loss: 2.270989  [12800/48000]\n",
            "loss: 2.269784  [19200/48000]\n",
            "loss: 2.270502  [25600/48000]\n",
            "loss: 2.270449  [32000/48000]\n",
            "loss: 2.249882  [38400/48000]\n",
            "loss: 2.269103  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 2.264536  [    0/48000]\n",
            "loss: 2.271113  [ 6400/48000]\n",
            "loss: 2.266994  [12800/48000]\n",
            "loss: 2.265436  [19200/48000]\n",
            "loss: 2.266495  [25600/48000]\n",
            "loss: 2.266186  [32000/48000]\n",
            "loss: 2.245169  [38400/48000]\n",
            "loss: 2.264903  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 2.259838  [    0/48000]\n",
            "loss: 2.267181  [ 6400/48000]\n",
            "loss: 2.262637  [12800/48000]\n",
            "loss: 2.260703  [19200/48000]\n",
            "loss: 2.262223  [25600/48000]\n",
            "loss: 2.261562  [32000/48000]\n",
            "loss: 2.239838  [38400/48000]\n",
            "loss: 2.260278  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 2.254657  [    0/48000]\n",
            "loss: 2.262757  [ 6400/48000]\n",
            "loss: 2.257801  [12800/48000]\n",
            "loss: 2.255530  [19200/48000]\n",
            "loss: 2.257394  [25600/48000]\n",
            "loss: 2.256396  [32000/48000]\n",
            "loss: 2.234039  [38400/48000]\n",
            "loss: 2.255124  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 2.248752  [    0/48000]\n",
            "loss: 2.257662  [ 6400/48000]\n",
            "loss: 2.252470  [12800/48000]\n",
            "loss: 2.249687  [19200/48000]\n",
            "loss: 2.251919  [25600/48000]\n",
            "loss: 2.250622  [32000/48000]\n",
            "loss: 2.227564  [38400/48000]\n",
            "loss: 2.249443  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 2.242060  [    0/48000]\n",
            "loss: 2.252187  [ 6400/48000]\n",
            "loss: 2.246530  [12800/48000]\n",
            "loss: 2.243068  [19200/48000]\n",
            "loss: 2.245927  [25600/48000]\n",
            "loss: 2.244211  [32000/48000]\n",
            "loss: 2.220301  [38400/48000]\n",
            "loss: 2.243232  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 25.5%, Avg loss: 2.236161 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=sigmoid, weight_decay=0.0001, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.340439  [    0/48000]\n",
            "loss: 2.305444  [ 6400/48000]\n",
            "loss: 2.304065  [12800/48000]\n",
            "loss: 2.304007  [19200/48000]\n",
            "loss: 2.302358  [25600/48000]\n",
            "loss: 2.305007  [32000/48000]\n",
            "loss: 2.287309  [38400/48000]\n",
            "loss: 2.302692  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.302644  [    0/48000]\n",
            "loss: 2.303784  [ 6400/48000]\n",
            "loss: 2.301759  [12800/48000]\n",
            "loss: 2.301607  [19200/48000]\n",
            "loss: 2.300506  [25600/48000]\n",
            "loss: 2.302982  [32000/48000]\n",
            "loss: 2.285498  [38400/48000]\n",
            "loss: 2.300989  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.300601  [    0/48000]\n",
            "loss: 2.301997  [ 6400/48000]\n",
            "loss: 2.299892  [12800/48000]\n",
            "loss: 2.299608  [19200/48000]\n",
            "loss: 2.298531  [25600/48000]\n",
            "loss: 2.300982  [32000/48000]\n",
            "loss: 2.283586  [38400/48000]\n",
            "loss: 2.299118  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.298538  [    0/48000]\n",
            "loss: 2.300156  [ 6400/48000]\n",
            "loss: 2.297991  [12800/48000]\n",
            "loss: 2.297616  [19200/48000]\n",
            "loss: 2.296471  [25600/48000]\n",
            "loss: 2.298809  [32000/48000]\n",
            "loss: 2.281581  [38400/48000]\n",
            "loss: 2.297103  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.296370  [    0/48000]\n",
            "loss: 2.298219  [ 6400/48000]\n",
            "loss: 2.295887  [12800/48000]\n",
            "loss: 2.295542  [19200/48000]\n",
            "loss: 2.294393  [25600/48000]\n",
            "loss: 2.296559  [32000/48000]\n",
            "loss: 2.279439  [38400/48000]\n",
            "loss: 2.295082  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 10.8%, Avg loss: 2.291228 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=sigmoid, weight_decay=0.0001, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.333310  [    0/48000]\n",
            "loss: 2.301375  [ 6400/48000]\n",
            "loss: 2.305903  [12800/48000]\n",
            "loss: 2.304021  [19200/48000]\n",
            "loss: 2.303900  [25600/48000]\n",
            "loss: 2.304821  [32000/48000]\n",
            "loss: 2.288908  [38400/48000]\n",
            "loss: 2.303780  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.301547  [    0/48000]\n",
            "loss: 2.303153  [ 6400/48000]\n",
            "loss: 2.302619  [12800/48000]\n",
            "loss: 2.302076  [19200/48000]\n",
            "loss: 2.302162  [25600/48000]\n",
            "loss: 2.302938  [32000/48000]\n",
            "loss: 2.286851  [38400/48000]\n",
            "loss: 2.301877  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.299459  [    0/48000]\n",
            "loss: 2.301415  [ 6400/48000]\n",
            "loss: 2.300766  [12800/48000]\n",
            "loss: 2.300043  [19200/48000]\n",
            "loss: 2.300148  [25600/48000]\n",
            "loss: 2.300941  [32000/48000]\n",
            "loss: 2.284828  [38400/48000]\n",
            "loss: 2.299973  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.297390  [    0/48000]\n",
            "loss: 2.299650  [ 6400/48000]\n",
            "loss: 2.298889  [12800/48000]\n",
            "loss: 2.297968  [19200/48000]\n",
            "loss: 2.298148  [25600/48000]\n",
            "loss: 2.298928  [32000/48000]\n",
            "loss: 2.282765  [38400/48000]\n",
            "loss: 2.297936  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.295247  [    0/48000]\n",
            "loss: 2.297794  [ 6400/48000]\n",
            "loss: 2.297037  [12800/48000]\n",
            "loss: 2.295856  [19200/48000]\n",
            "loss: 2.296116  [25600/48000]\n",
            "loss: 2.296839  [32000/48000]\n",
            "loss: 2.280687  [38400/48000]\n",
            "loss: 2.295847  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.293047  [    0/48000]\n",
            "loss: 2.295858  [ 6400/48000]\n",
            "loss: 2.295139  [12800/48000]\n",
            "loss: 2.293617  [19200/48000]\n",
            "loss: 2.294045  [25600/48000]\n",
            "loss: 2.294693  [32000/48000]\n",
            "loss: 2.278447  [38400/48000]\n",
            "loss: 2.293757  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.290728  [    0/48000]\n",
            "loss: 2.293806  [ 6400/48000]\n",
            "loss: 2.293040  [12800/48000]\n",
            "loss: 2.291291  [19200/48000]\n",
            "loss: 2.291908  [25600/48000]\n",
            "loss: 2.292469  [32000/48000]\n",
            "loss: 2.276061  [38400/48000]\n",
            "loss: 2.291634  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.288194  [    0/48000]\n",
            "loss: 2.291666  [ 6400/48000]\n",
            "loss: 2.290785  [12800/48000]\n",
            "loss: 2.288863  [19200/48000]\n",
            "loss: 2.289577  [25600/48000]\n",
            "loss: 2.290114  [32000/48000]\n",
            "loss: 2.273569  [38400/48000]\n",
            "loss: 2.289337  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.285550  [    0/48000]\n",
            "loss: 2.289386  [ 6400/48000]\n",
            "loss: 2.288448  [12800/48000]\n",
            "loss: 2.286341  [19200/48000]\n",
            "loss: 2.287042  [25600/48000]\n",
            "loss: 2.287659  [32000/48000]\n",
            "loss: 2.270932  [38400/48000]\n",
            "loss: 2.286823  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.282811  [    0/48000]\n",
            "loss: 2.286978  [ 6400/48000]\n",
            "loss: 2.285890  [12800/48000]\n",
            "loss: 2.283582  [19200/48000]\n",
            "loss: 2.284294  [25600/48000]\n",
            "loss: 2.284941  [32000/48000]\n",
            "loss: 2.268023  [38400/48000]\n",
            "loss: 2.284014  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 10.8%, Avg loss: 2.279396 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=sigmoid, weight_decay=0.0001, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.362983  [    0/48000]\n",
            "loss: 2.316278  [ 6400/48000]\n",
            "loss: 2.306357  [12800/48000]\n",
            "loss: 2.304390  [19200/48000]\n",
            "loss: 2.302564  [25600/48000]\n",
            "loss: 2.305300  [32000/48000]\n",
            "loss: 2.289517  [38400/48000]\n",
            "loss: 2.303891  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.301611  [    0/48000]\n",
            "loss: 2.304271  [ 6400/48000]\n",
            "loss: 2.302598  [12800/48000]\n",
            "loss: 2.303122  [19200/48000]\n",
            "loss: 2.300964  [25600/48000]\n",
            "loss: 2.303384  [32000/48000]\n",
            "loss: 2.287711  [38400/48000]\n",
            "loss: 2.302129  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.299606  [    0/48000]\n",
            "loss: 2.302608  [ 6400/48000]\n",
            "loss: 2.300790  [12800/48000]\n",
            "loss: 2.301057  [19200/48000]\n",
            "loss: 2.299096  [25600/48000]\n",
            "loss: 2.301369  [32000/48000]\n",
            "loss: 2.285758  [38400/48000]\n",
            "loss: 2.300324  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.297595  [    0/48000]\n",
            "loss: 2.300908  [ 6400/48000]\n",
            "loss: 2.298924  [12800/48000]\n",
            "loss: 2.298981  [19200/48000]\n",
            "loss: 2.297216  [25600/48000]\n",
            "loss: 2.299321  [32000/48000]\n",
            "loss: 2.283652  [38400/48000]\n",
            "loss: 2.298473  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.295527  [    0/48000]\n",
            "loss: 2.299140  [ 6400/48000]\n",
            "loss: 2.296930  [12800/48000]\n",
            "loss: 2.296855  [19200/48000]\n",
            "loss: 2.295343  [25600/48000]\n",
            "loss: 2.297134  [32000/48000]\n",
            "loss: 2.281455  [38400/48000]\n",
            "loss: 2.296680  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.293388  [    0/48000]\n",
            "loss: 2.297348  [ 6400/48000]\n",
            "loss: 2.294928  [12800/48000]\n",
            "loss: 2.294639  [19200/48000]\n",
            "loss: 2.293384  [25600/48000]\n",
            "loss: 2.294928  [32000/48000]\n",
            "loss: 2.279168  [38400/48000]\n",
            "loss: 2.294764  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.291202  [    0/48000]\n",
            "loss: 2.295449  [ 6400/48000]\n",
            "loss: 2.292819  [12800/48000]\n",
            "loss: 2.292326  [19200/48000]\n",
            "loss: 2.291275  [25600/48000]\n",
            "loss: 2.292627  [32000/48000]\n",
            "loss: 2.276832  [38400/48000]\n",
            "loss: 2.292755  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.288856  [    0/48000]\n",
            "loss: 2.293466  [ 6400/48000]\n",
            "loss: 2.290638  [12800/48000]\n",
            "loss: 2.289877  [19200/48000]\n",
            "loss: 2.288988  [25600/48000]\n",
            "loss: 2.290159  [32000/48000]\n",
            "loss: 2.274489  [38400/48000]\n",
            "loss: 2.290628  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.286307  [    0/48000]\n",
            "loss: 2.291336  [ 6400/48000]\n",
            "loss: 2.288290  [12800/48000]\n",
            "loss: 2.287280  [19200/48000]\n",
            "loss: 2.286580  [25600/48000]\n",
            "loss: 2.287570  [32000/48000]\n",
            "loss: 2.272032  [38400/48000]\n",
            "loss: 2.288336  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.283489  [    0/48000]\n",
            "loss: 2.289084  [ 6400/48000]\n",
            "loss: 2.285719  [12800/48000]\n",
            "loss: 2.284483  [19200/48000]\n",
            "loss: 2.283990  [25600/48000]\n",
            "loss: 2.284824  [32000/48000]\n",
            "loss: 2.269338  [38400/48000]\n",
            "loss: 2.285874  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2.280442  [    0/48000]\n",
            "loss: 2.286628  [ 6400/48000]\n",
            "loss: 2.282981  [12800/48000]\n",
            "loss: 2.281515  [19200/48000]\n",
            "loss: 2.281198  [25600/48000]\n",
            "loss: 2.281842  [32000/48000]\n",
            "loss: 2.266280  [38400/48000]\n",
            "loss: 2.283207  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.277072  [    0/48000]\n",
            "loss: 2.284014  [ 6400/48000]\n",
            "loss: 2.280056  [12800/48000]\n",
            "loss: 2.278255  [19200/48000]\n",
            "loss: 2.278135  [25600/48000]\n",
            "loss: 2.278680  [32000/48000]\n",
            "loss: 2.262995  [38400/48000]\n",
            "loss: 2.280222  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2.273411  [    0/48000]\n",
            "loss: 2.281200  [ 6400/48000]\n",
            "loss: 2.276833  [12800/48000]\n",
            "loss: 2.274686  [19200/48000]\n",
            "loss: 2.274799  [25600/48000]\n",
            "loss: 2.275283  [32000/48000]\n",
            "loss: 2.259369  [38400/48000]\n",
            "loss: 2.276920  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 2.269460  [    0/48000]\n",
            "loss: 2.278179  [ 6400/48000]\n",
            "loss: 2.273285  [12800/48000]\n",
            "loss: 2.270821  [19200/48000]\n",
            "loss: 2.271222  [25600/48000]\n",
            "loss: 2.271605  [32000/48000]\n",
            "loss: 2.255294  [38400/48000]\n",
            "loss: 2.273328  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 2.265188  [    0/48000]\n",
            "loss: 2.274806  [ 6400/48000]\n",
            "loss: 2.269509  [12800/48000]\n",
            "loss: 2.266597  [19200/48000]\n",
            "loss: 2.267320  [25600/48000]\n",
            "loss: 2.267613  [32000/48000]\n",
            "loss: 2.250695  [38400/48000]\n",
            "loss: 2.269241  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 2.260582  [    0/48000]\n",
            "loss: 2.270964  [ 6400/48000]\n",
            "loss: 2.265265  [12800/48000]\n",
            "loss: 2.261898  [19200/48000]\n",
            "loss: 2.262954  [25600/48000]\n",
            "loss: 2.263197  [32000/48000]\n",
            "loss: 2.245633  [38400/48000]\n",
            "loss: 2.264635  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 2.255388  [    0/48000]\n",
            "loss: 2.266761  [ 6400/48000]\n",
            "loss: 2.260593  [12800/48000]\n",
            "loss: 2.256682  [19200/48000]\n",
            "loss: 2.258129  [25600/48000]\n",
            "loss: 2.258143  [32000/48000]\n",
            "loss: 2.239990  [38400/48000]\n",
            "loss: 2.259517  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 2.249464  [    0/48000]\n",
            "loss: 2.262127  [ 6400/48000]\n",
            "loss: 2.255429  [12800/48000]\n",
            "loss: 2.250695  [19200/48000]\n",
            "loss: 2.252721  [25600/48000]\n",
            "loss: 2.252412  [32000/48000]\n",
            "loss: 2.233621  [38400/48000]\n",
            "loss: 2.253706  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 2.242676  [    0/48000]\n",
            "loss: 2.256975  [ 6400/48000]\n",
            "loss: 2.249484  [12800/48000]\n",
            "loss: 2.243877  [19200/48000]\n",
            "loss: 2.246496  [25600/48000]\n",
            "loss: 2.245759  [32000/48000]\n",
            "loss: 2.226394  [38400/48000]\n",
            "loss: 2.247188  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 2.234924  [    0/48000]\n",
            "loss: 2.251141  [ 6400/48000]\n",
            "loss: 2.242709  [12800/48000]\n",
            "loss: 2.236141  [19200/48000]\n",
            "loss: 2.239508  [25600/48000]\n",
            "loss: 2.238145  [32000/48000]\n",
            "loss: 2.218202  [38400/48000]\n",
            "loss: 2.239982  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 24.5%, Avg loss: 2.229176 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=sigmoid, weight_decay=0.001, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.307626  [    0/48000]\n",
            "loss: 2.309096  [ 6400/48000]\n",
            "loss: 2.303811  [12800/48000]\n",
            "loss: 2.303732  [19200/48000]\n",
            "loss: 2.301644  [25600/48000]\n",
            "loss: 2.304389  [32000/48000]\n",
            "loss: 2.288202  [38400/48000]\n",
            "loss: 2.304480  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.300575  [    0/48000]\n",
            "loss: 2.303721  [ 6400/48000]\n",
            "loss: 2.300945  [12800/48000]\n",
            "loss: 2.301921  [19200/48000]\n",
            "loss: 2.299880  [25600/48000]\n",
            "loss: 2.302524  [32000/48000]\n",
            "loss: 2.286137  [38400/48000]\n",
            "loss: 2.302743  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.298489  [    0/48000]\n",
            "loss: 2.301955  [ 6400/48000]\n",
            "loss: 2.299016  [12800/48000]\n",
            "loss: 2.299950  [19200/48000]\n",
            "loss: 2.297899  [25600/48000]\n",
            "loss: 2.300603  [32000/48000]\n",
            "loss: 2.284130  [38400/48000]\n",
            "loss: 2.300975  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.296544  [    0/48000]\n",
            "loss: 2.300229  [ 6400/48000]\n",
            "loss: 2.297122  [12800/48000]\n",
            "loss: 2.298019  [19200/48000]\n",
            "loss: 2.295938  [25600/48000]\n",
            "loss: 2.298635  [32000/48000]\n",
            "loss: 2.282079  [38400/48000]\n",
            "loss: 2.299186  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.294633  [    0/48000]\n",
            "loss: 2.298433  [ 6400/48000]\n",
            "loss: 2.295235  [12800/48000]\n",
            "loss: 2.295925  [19200/48000]\n",
            "loss: 2.293981  [25600/48000]\n",
            "loss: 2.296583  [32000/48000]\n",
            "loss: 2.279955  [38400/48000]\n",
            "loss: 2.297264  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 10.8%, Avg loss: 2.291428 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=sigmoid, weight_decay=0.001, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.354837  [    0/48000]\n",
            "loss: 2.292353  [ 6400/48000]\n",
            "loss: 2.299688  [12800/48000]\n",
            "loss: 2.306161  [19200/48000]\n",
            "loss: 2.304050  [25600/48000]\n",
            "loss: 2.305643  [32000/48000]\n",
            "loss: 2.288902  [38400/48000]\n",
            "loss: 2.303167  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.302076  [    0/48000]\n",
            "loss: 2.304204  [ 6400/48000]\n",
            "loss: 2.302863  [12800/48000]\n",
            "loss: 2.304177  [19200/48000]\n",
            "loss: 2.302170  [25600/48000]\n",
            "loss: 2.303707  [32000/48000]\n",
            "loss: 2.286885  [38400/48000]\n",
            "loss: 2.301486  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.300034  [    0/48000]\n",
            "loss: 2.302477  [ 6400/48000]\n",
            "loss: 2.301074  [12800/48000]\n",
            "loss: 2.302299  [19200/48000]\n",
            "loss: 2.300340  [25600/48000]\n",
            "loss: 2.301790  [32000/48000]\n",
            "loss: 2.285017  [38400/48000]\n",
            "loss: 2.299791  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.297961  [    0/48000]\n",
            "loss: 2.300824  [ 6400/48000]\n",
            "loss: 2.299278  [12800/48000]\n",
            "loss: 2.300400  [19200/48000]\n",
            "loss: 2.298604  [25600/48000]\n",
            "loss: 2.299837  [32000/48000]\n",
            "loss: 2.283170  [38400/48000]\n",
            "loss: 2.298086  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.295966  [    0/48000]\n",
            "loss: 2.299158  [ 6400/48000]\n",
            "loss: 2.297429  [12800/48000]\n",
            "loss: 2.298464  [19200/48000]\n",
            "loss: 2.296870  [25600/48000]\n",
            "loss: 2.297864  [32000/48000]\n",
            "loss: 2.281287  [38400/48000]\n",
            "loss: 2.296334  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.294007  [    0/48000]\n",
            "loss: 2.297415  [ 6400/48000]\n",
            "loss: 2.295542  [12800/48000]\n",
            "loss: 2.296479  [19200/48000]\n",
            "loss: 2.295018  [25600/48000]\n",
            "loss: 2.295925  [32000/48000]\n",
            "loss: 2.279323  [38400/48000]\n",
            "loss: 2.294409  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.292054  [    0/48000]\n",
            "loss: 2.295549  [ 6400/48000]\n",
            "loss: 2.293601  [12800/48000]\n",
            "loss: 2.294399  [19200/48000]\n",
            "loss: 2.293040  [25600/48000]\n",
            "loss: 2.293961  [32000/48000]\n",
            "loss: 2.277148  [38400/48000]\n",
            "loss: 2.292403  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.290048  [    0/48000]\n",
            "loss: 2.293572  [ 6400/48000]\n",
            "loss: 2.291608  [12800/48000]\n",
            "loss: 2.292247  [19200/48000]\n",
            "loss: 2.291053  [25600/48000]\n",
            "loss: 2.291868  [32000/48000]\n",
            "loss: 2.274890  [38400/48000]\n",
            "loss: 2.290378  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.287867  [    0/48000]\n",
            "loss: 2.291529  [ 6400/48000]\n",
            "loss: 2.289479  [12800/48000]\n",
            "loss: 2.289978  [19200/48000]\n",
            "loss: 2.288925  [25600/48000]\n",
            "loss: 2.289578  [32000/48000]\n",
            "loss: 2.272571  [38400/48000]\n",
            "loss: 2.288276  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.285541  [    0/48000]\n",
            "loss: 2.289427  [ 6400/48000]\n",
            "loss: 2.287199  [12800/48000]\n",
            "loss: 2.287519  [19200/48000]\n",
            "loss: 2.286658  [25600/48000]\n",
            "loss: 2.287183  [32000/48000]\n",
            "loss: 2.270141  [38400/48000]\n",
            "loss: 2.286078  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 10.8%, Avg loss: 2.281892 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=sigmoid, weight_decay=0.001, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.369461  [    0/48000]\n",
            "loss: 2.299932  [ 6400/48000]\n",
            "loss: 2.303685  [12800/48000]\n",
            "loss: 2.303146  [19200/48000]\n",
            "loss: 2.303180  [25600/48000]\n",
            "loss: 2.304304  [32000/48000]\n",
            "loss: 2.288408  [38400/48000]\n",
            "loss: 2.303576  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.301194  [    0/48000]\n",
            "loss: 2.303158  [ 6400/48000]\n",
            "loss: 2.301871  [12800/48000]\n",
            "loss: 2.302106  [19200/48000]\n",
            "loss: 2.301647  [25600/48000]\n",
            "loss: 2.302492  [32000/48000]\n",
            "loss: 2.286580  [38400/48000]\n",
            "loss: 2.301874  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.299260  [    0/48000]\n",
            "loss: 2.301405  [ 6400/48000]\n",
            "loss: 2.299992  [12800/48000]\n",
            "loss: 2.300169  [19200/48000]\n",
            "loss: 2.299902  [25600/48000]\n",
            "loss: 2.300596  [32000/48000]\n",
            "loss: 2.284694  [38400/48000]\n",
            "loss: 2.300181  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.297318  [    0/48000]\n",
            "loss: 2.299692  [ 6400/48000]\n",
            "loss: 2.298031  [12800/48000]\n",
            "loss: 2.298253  [19200/48000]\n",
            "loss: 2.298027  [25600/48000]\n",
            "loss: 2.298719  [32000/48000]\n",
            "loss: 2.282768  [38400/48000]\n",
            "loss: 2.298475  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.295350  [    0/48000]\n",
            "loss: 2.297942  [ 6400/48000]\n",
            "loss: 2.296087  [12800/48000]\n",
            "loss: 2.296335  [19200/48000]\n",
            "loss: 2.296146  [25600/48000]\n",
            "loss: 2.296728  [32000/48000]\n",
            "loss: 2.280754  [38400/48000]\n",
            "loss: 2.296719  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.293315  [    0/48000]\n",
            "loss: 2.296083  [ 6400/48000]\n",
            "loss: 2.294078  [12800/48000]\n",
            "loss: 2.294343  [19200/48000]\n",
            "loss: 2.294259  [25600/48000]\n",
            "loss: 2.294683  [32000/48000]\n",
            "loss: 2.278628  [38400/48000]\n",
            "loss: 2.294946  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.291170  [    0/48000]\n",
            "loss: 2.294120  [ 6400/48000]\n",
            "loss: 2.292020  [12800/48000]\n",
            "loss: 2.292288  [19200/48000]\n",
            "loss: 2.292297  [25600/48000]\n",
            "loss: 2.292633  [32000/48000]\n",
            "loss: 2.276398  [38400/48000]\n",
            "loss: 2.293124  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.288959  [    0/48000]\n",
            "loss: 2.292126  [ 6400/48000]\n",
            "loss: 2.289856  [12800/48000]\n",
            "loss: 2.290096  [19200/48000]\n",
            "loss: 2.290151  [25600/48000]\n",
            "loss: 2.290549  [32000/48000]\n",
            "loss: 2.274134  [38400/48000]\n",
            "loss: 2.291082  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.286678  [    0/48000]\n",
            "loss: 2.290056  [ 6400/48000]\n",
            "loss: 2.287592  [12800/48000]\n",
            "loss: 2.287723  [19200/48000]\n",
            "loss: 2.287801  [25600/48000]\n",
            "loss: 2.288331  [32000/48000]\n",
            "loss: 2.271813  [38400/48000]\n",
            "loss: 2.288709  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.284256  [    0/48000]\n",
            "loss: 2.287872  [ 6400/48000]\n",
            "loss: 2.285141  [12800/48000]\n",
            "loss: 2.285190  [19200/48000]\n",
            "loss: 2.285314  [25600/48000]\n",
            "loss: 2.285885  [32000/48000]\n",
            "loss: 2.269291  [38400/48000]\n",
            "loss: 2.286180  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2.281637  [    0/48000]\n",
            "loss: 2.285527  [ 6400/48000]\n",
            "loss: 2.282381  [12800/48000]\n",
            "loss: 2.282463  [19200/48000]\n",
            "loss: 2.282744  [25600/48000]\n",
            "loss: 2.283219  [32000/48000]\n",
            "loss: 2.266542  [38400/48000]\n",
            "loss: 2.283500  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.278736  [    0/48000]\n",
            "loss: 2.282935  [ 6400/48000]\n",
            "loss: 2.279428  [12800/48000]\n",
            "loss: 2.279422  [19200/48000]\n",
            "loss: 2.279960  [25600/48000]\n",
            "loss: 2.280312  [32000/48000]\n",
            "loss: 2.263394  [38400/48000]\n",
            "loss: 2.280745  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2.275618  [    0/48000]\n",
            "loss: 2.280067  [ 6400/48000]\n",
            "loss: 2.276328  [12800/48000]\n",
            "loss: 2.276140  [19200/48000]\n",
            "loss: 2.277011  [25600/48000]\n",
            "loss: 2.277133  [32000/48000]\n",
            "loss: 2.260036  [38400/48000]\n",
            "loss: 2.277754  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 2.272206  [    0/48000]\n",
            "loss: 2.276953  [ 6400/48000]\n",
            "loss: 2.273067  [12800/48000]\n",
            "loss: 2.272584  [19200/48000]\n",
            "loss: 2.273734  [25600/48000]\n",
            "loss: 2.273683  [32000/48000]\n",
            "loss: 2.256364  [38400/48000]\n",
            "loss: 2.274472  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 2.268448  [    0/48000]\n",
            "loss: 2.273631  [ 6400/48000]\n",
            "loss: 2.269428  [12800/48000]\n",
            "loss: 2.268677  [19200/48000]\n",
            "loss: 2.270070  [25600/48000]\n",
            "loss: 2.270032  [32000/48000]\n",
            "loss: 2.252132  [38400/48000]\n",
            "loss: 2.270862  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 2.264189  [    0/48000]\n",
            "loss: 2.270016  [ 6400/48000]\n",
            "loss: 2.265357  [12800/48000]\n",
            "loss: 2.264362  [19200/48000]\n",
            "loss: 2.265969  [25600/48000]\n",
            "loss: 2.266021  [32000/48000]\n",
            "loss: 2.247474  [38400/48000]\n",
            "loss: 2.266787  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 2.259397  [    0/48000]\n",
            "loss: 2.266077  [ 6400/48000]\n",
            "loss: 2.260947  [12800/48000]\n",
            "loss: 2.259487  [19200/48000]\n",
            "loss: 2.261413  [25600/48000]\n",
            "loss: 2.261463  [32000/48000]\n",
            "loss: 2.242454  [38400/48000]\n",
            "loss: 2.262220  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 2.254044  [    0/48000]\n",
            "loss: 2.261722  [ 6400/48000]\n",
            "loss: 2.256003  [12800/48000]\n",
            "loss: 2.254098  [19200/48000]\n",
            "loss: 2.256428  [25600/48000]\n",
            "loss: 2.256344  [32000/48000]\n",
            "loss: 2.236868  [38400/48000]\n",
            "loss: 2.257075  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 2.248093  [    0/48000]\n",
            "loss: 2.256903  [ 6400/48000]\n",
            "loss: 2.250522  [12800/48000]\n",
            "loss: 2.248011  [19200/48000]\n",
            "loss: 2.250907  [25600/48000]\n",
            "loss: 2.250662  [32000/48000]\n",
            "loss: 2.230549  [38400/48000]\n",
            "loss: 2.251324  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 2.241335  [    0/48000]\n",
            "loss: 2.251642  [ 6400/48000]\n",
            "loss: 2.244226  [12800/48000]\n",
            "loss: 2.241310  [19200/48000]\n",
            "loss: 2.244787  [25600/48000]\n",
            "loss: 2.244269  [32000/48000]\n",
            "loss: 2.223342  [38400/48000]\n",
            "loss: 2.244755  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 27.3%, Avg loss: 2.234756 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=tanh, weight_decay=0, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.241653  [    0/48000]\n",
            "loss: 2.121593  [ 6400/48000]\n",
            "loss: 1.972388  [12800/48000]\n",
            "loss: 1.793145  [19200/48000]\n",
            "loss: 1.704891  [25600/48000]\n",
            "loss: 1.545776  [32000/48000]\n",
            "loss: 1.408826  [38400/48000]\n",
            "loss: 1.345670  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.239590  [    0/48000]\n",
            "loss: 1.201892  [ 6400/48000]\n",
            "loss: 1.040768  [12800/48000]\n",
            "loss: 0.919531  [19200/48000]\n",
            "loss: 0.942142  [25600/48000]\n",
            "loss: 0.857055  [32000/48000]\n",
            "loss: 0.847205  [38400/48000]\n",
            "loss: 0.809984  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.743733  [    0/48000]\n",
            "loss: 0.767318  [ 6400/48000]\n",
            "loss: 0.632144  [12800/48000]\n",
            "loss: 0.580660  [19200/48000]\n",
            "loss: 0.639057  [25600/48000]\n",
            "loss: 0.599897  [32000/48000]\n",
            "loss: 0.627444  [38400/48000]\n",
            "loss: 0.612482  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.539606  [    0/48000]\n",
            "loss: 0.590901  [ 6400/48000]\n",
            "loss: 0.475016  [12800/48000]\n",
            "loss: 0.428641  [19200/48000]\n",
            "loss: 0.507629  [25600/48000]\n",
            "loss: 0.485602  [32000/48000]\n",
            "loss: 0.517388  [38400/48000]\n",
            "loss: 0.511703  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.441600  [    0/48000]\n",
            "loss: 0.487865  [ 6400/48000]\n",
            "loss: 0.378822  [12800/48000]\n",
            "loss: 0.359238  [19200/48000]\n",
            "loss: 0.406007  [25600/48000]\n",
            "loss: 0.416976  [32000/48000]\n",
            "loss: 0.466542  [38400/48000]\n",
            "loss: 0.444863  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 89.8%, Avg loss: 0.397622 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=tanh, weight_decay=0, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.321179  [    0/48000]\n",
            "loss: 2.207347  [ 6400/48000]\n",
            "loss: 2.035943  [12800/48000]\n",
            "loss: 1.868778  [19200/48000]\n",
            "loss: 1.732312  [25600/48000]\n",
            "loss: 1.592588  [32000/48000]\n",
            "loss: 1.468067  [38400/48000]\n",
            "loss: 1.361994  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.301954  [    0/48000]\n",
            "loss: 1.247387  [ 6400/48000]\n",
            "loss: 1.086189  [12800/48000]\n",
            "loss: 0.968563  [19200/48000]\n",
            "loss: 0.996801  [25600/48000]\n",
            "loss: 0.875740  [32000/48000]\n",
            "loss: 0.872236  [38400/48000]\n",
            "loss: 0.836582  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.785981  [    0/48000]\n",
            "loss: 0.777546  [ 6400/48000]\n",
            "loss: 0.694772  [12800/48000]\n",
            "loss: 0.607387  [19200/48000]\n",
            "loss: 0.678069  [25600/48000]\n",
            "loss: 0.612873  [32000/48000]\n",
            "loss: 0.617521  [38400/48000]\n",
            "loss: 0.629160  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.563008  [    0/48000]\n",
            "loss: 0.577500  [ 6400/48000]\n",
            "loss: 0.521521  [12800/48000]\n",
            "loss: 0.449045  [19200/48000]\n",
            "loss: 0.536691  [25600/48000]\n",
            "loss: 0.482402  [32000/48000]\n",
            "loss: 0.518788  [38400/48000]\n",
            "loss: 0.507110  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.461387  [    0/48000]\n",
            "loss: 0.484091  [ 6400/48000]\n",
            "loss: 0.402311  [12800/48000]\n",
            "loss: 0.363061  [19200/48000]\n",
            "loss: 0.436606  [25600/48000]\n",
            "loss: 0.402739  [32000/48000]\n",
            "loss: 0.452085  [38400/48000]\n",
            "loss: 0.442081  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.407553  [    0/48000]\n",
            "loss: 0.418265  [ 6400/48000]\n",
            "loss: 0.335637  [12800/48000]\n",
            "loss: 0.312752  [19200/48000]\n",
            "loss: 0.382538  [25600/48000]\n",
            "loss: 0.355544  [32000/48000]\n",
            "loss: 0.388304  [38400/48000]\n",
            "loss: 0.391502  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.364322  [    0/48000]\n",
            "loss: 0.383794  [ 6400/48000]\n",
            "loss: 0.290483  [12800/48000]\n",
            "loss: 0.283900  [19200/48000]\n",
            "loss: 0.334038  [25600/48000]\n",
            "loss: 0.313608  [32000/48000]\n",
            "loss: 0.355188  [38400/48000]\n",
            "loss: 0.360020  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.337462  [    0/48000]\n",
            "loss: 0.352213  [ 6400/48000]\n",
            "loss: 0.271837  [12800/48000]\n",
            "loss: 0.248659  [19200/48000]\n",
            "loss: 0.302715  [25600/48000]\n",
            "loss: 0.290675  [32000/48000]\n",
            "loss: 0.329269  [38400/48000]\n",
            "loss: 0.328179  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.301532  [    0/48000]\n",
            "loss: 0.322142  [ 6400/48000]\n",
            "loss: 0.242966  [12800/48000]\n",
            "loss: 0.230053  [19200/48000]\n",
            "loss: 0.295570  [25600/48000]\n",
            "loss: 0.269854  [32000/48000]\n",
            "loss: 0.294661  [38400/48000]\n",
            "loss: 0.306541  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.278749  [    0/48000]\n",
            "loss: 0.314016  [ 6400/48000]\n",
            "loss: 0.228973  [12800/48000]\n",
            "loss: 0.223578  [19200/48000]\n",
            "loss: 0.261676  [25600/48000]\n",
            "loss: 0.259087  [32000/48000]\n",
            "loss: 0.284409  [38400/48000]\n",
            "loss: 0.284677  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 92.0%, Avg loss: 0.288517 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=tanh, weight_decay=0, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.327936  [    0/48000]\n",
            "loss: 2.204319  [ 6400/48000]\n",
            "loss: 2.055952  [12800/48000]\n",
            "loss: 1.865152  [19200/48000]\n",
            "loss: 1.755445  [25600/48000]\n",
            "loss: 1.607375  [32000/48000]\n",
            "loss: 1.455747  [38400/48000]\n",
            "loss: 1.389900  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.321900  [    0/48000]\n",
            "loss: 1.222725  [ 6400/48000]\n",
            "loss: 1.063584  [12800/48000]\n",
            "loss: 0.983356  [19200/48000]\n",
            "loss: 0.977976  [25600/48000]\n",
            "loss: 0.859987  [32000/48000]\n",
            "loss: 0.814061  [38400/48000]\n",
            "loss: 0.850454  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.800216  [    0/48000]\n",
            "loss: 0.777287  [ 6400/48000]\n",
            "loss: 0.657069  [12800/48000]\n",
            "loss: 0.617056  [19200/48000]\n",
            "loss: 0.656752  [25600/48000]\n",
            "loss: 0.573797  [32000/48000]\n",
            "loss: 0.589947  [38400/48000]\n",
            "loss: 0.652688  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.593628  [    0/48000]\n",
            "loss: 0.587844  [ 6400/48000]\n",
            "loss: 0.485465  [12800/48000]\n",
            "loss: 0.463989  [19200/48000]\n",
            "loss: 0.511377  [25600/48000]\n",
            "loss: 0.459425  [32000/48000]\n",
            "loss: 0.466457  [38400/48000]\n",
            "loss: 0.555108  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.495977  [    0/48000]\n",
            "loss: 0.477584  [ 6400/48000]\n",
            "loss: 0.383507  [12800/48000]\n",
            "loss: 0.386969  [19200/48000]\n",
            "loss: 0.422569  [25600/48000]\n",
            "loss: 0.381188  [32000/48000]\n",
            "loss: 0.418778  [38400/48000]\n",
            "loss: 0.479377  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.422294  [    0/48000]\n",
            "loss: 0.422177  [ 6400/48000]\n",
            "loss: 0.327193  [12800/48000]\n",
            "loss: 0.326662  [19200/48000]\n",
            "loss: 0.381702  [25600/48000]\n",
            "loss: 0.341489  [32000/48000]\n",
            "loss: 0.365355  [38400/48000]\n",
            "loss: 0.441851  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.381679  [    0/48000]\n",
            "loss: 0.376855  [ 6400/48000]\n",
            "loss: 0.279891  [12800/48000]\n",
            "loss: 0.285110  [19200/48000]\n",
            "loss: 0.338760  [25600/48000]\n",
            "loss: 0.299856  [32000/48000]\n",
            "loss: 0.337475  [38400/48000]\n",
            "loss: 0.409984  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.352058  [    0/48000]\n",
            "loss: 0.334305  [ 6400/48000]\n",
            "loss: 0.252666  [12800/48000]\n",
            "loss: 0.274603  [19200/48000]\n",
            "loss: 0.320138  [25600/48000]\n",
            "loss: 0.273498  [32000/48000]\n",
            "loss: 0.316242  [38400/48000]\n",
            "loss: 0.376855  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.318211  [    0/48000]\n",
            "loss: 0.304380  [ 6400/48000]\n",
            "loss: 0.223732  [12800/48000]\n",
            "loss: 0.247597  [19200/48000]\n",
            "loss: 0.279238  [25600/48000]\n",
            "loss: 0.255858  [32000/48000]\n",
            "loss: 0.294767  [38400/48000]\n",
            "loss: 0.350550  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.298812  [    0/48000]\n",
            "loss: 0.276454  [ 6400/48000]\n",
            "loss: 0.203775  [12800/48000]\n",
            "loss: 0.231127  [19200/48000]\n",
            "loss: 0.261314  [25600/48000]\n",
            "loss: 0.245685  [32000/48000]\n",
            "loss: 0.281917  [38400/48000]\n",
            "loss: 0.332816  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.280035  [    0/48000]\n",
            "loss: 0.263715  [ 6400/48000]\n",
            "loss: 0.191664  [12800/48000]\n",
            "loss: 0.213061  [19200/48000]\n",
            "loss: 0.248404  [25600/48000]\n",
            "loss: 0.221353  [32000/48000]\n",
            "loss: 0.277227  [38400/48000]\n",
            "loss: 0.299716  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.263501  [    0/48000]\n",
            "loss: 0.244130  [ 6400/48000]\n",
            "loss: 0.171356  [12800/48000]\n",
            "loss: 0.194116  [19200/48000]\n",
            "loss: 0.239984  [25600/48000]\n",
            "loss: 0.200359  [32000/48000]\n",
            "loss: 0.264132  [38400/48000]\n",
            "loss: 0.301008  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.244643  [    0/48000]\n",
            "loss: 0.229847  [ 6400/48000]\n",
            "loss: 0.163431  [12800/48000]\n",
            "loss: 0.182333  [19200/48000]\n",
            "loss: 0.217593  [25600/48000]\n",
            "loss: 0.186649  [32000/48000]\n",
            "loss: 0.247658  [38400/48000]\n",
            "loss: 0.275854  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.235564  [    0/48000]\n",
            "loss: 0.225705  [ 6400/48000]\n",
            "loss: 0.148569  [12800/48000]\n",
            "loss: 0.181313  [19200/48000]\n",
            "loss: 0.203790  [25600/48000]\n",
            "loss: 0.179697  [32000/48000]\n",
            "loss: 0.240061  [38400/48000]\n",
            "loss: 0.262446  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.230455  [    0/48000]\n",
            "loss: 0.203304  [ 6400/48000]\n",
            "loss: 0.146715  [12800/48000]\n",
            "loss: 0.169852  [19200/48000]\n",
            "loss: 0.191335  [25600/48000]\n",
            "loss: 0.172823  [32000/48000]\n",
            "loss: 0.237482  [38400/48000]\n",
            "loss: 0.252535  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.225069  [    0/48000]\n",
            "loss: 0.189634  [ 6400/48000]\n",
            "loss: 0.136961  [12800/48000]\n",
            "loss: 0.158385  [19200/48000]\n",
            "loss: 0.185085  [25600/48000]\n",
            "loss: 0.178965  [32000/48000]\n",
            "loss: 0.229082  [38400/48000]\n",
            "loss: 0.238887  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.204881  [    0/48000]\n",
            "loss: 0.180430  [ 6400/48000]\n",
            "loss: 0.125251  [12800/48000]\n",
            "loss: 0.154024  [19200/48000]\n",
            "loss: 0.171428  [25600/48000]\n",
            "loss: 0.167326  [32000/48000]\n",
            "loss: 0.215167  [38400/48000]\n",
            "loss: 0.224383  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.205975  [    0/48000]\n",
            "loss: 0.191093  [ 6400/48000]\n",
            "loss: 0.120738  [12800/48000]\n",
            "loss: 0.152952  [19200/48000]\n",
            "loss: 0.167272  [25600/48000]\n",
            "loss: 0.160628  [32000/48000]\n",
            "loss: 0.202789  [38400/48000]\n",
            "loss: 0.217243  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.192438  [    0/48000]\n",
            "loss: 0.172429  [ 6400/48000]\n",
            "loss: 0.116440  [12800/48000]\n",
            "loss: 0.155079  [19200/48000]\n",
            "loss: 0.165970  [25600/48000]\n",
            "loss: 0.158294  [32000/48000]\n",
            "loss: 0.195137  [38400/48000]\n",
            "loss: 0.203197  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.194426  [    0/48000]\n",
            "loss: 0.166542  [ 6400/48000]\n",
            "loss: 0.110493  [12800/48000]\n",
            "loss: 0.138762  [19200/48000]\n",
            "loss: 0.154735  [25600/48000]\n",
            "loss: 0.140240  [32000/48000]\n",
            "loss: 0.180257  [38400/48000]\n",
            "loss: 0.181139  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 93.4%, Avg loss: 0.222499 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=tanh, weight_decay=0.0001, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.267227  [    0/48000]\n",
            "loss: 2.188692  [ 6400/48000]\n",
            "loss: 2.006046  [12800/48000]\n",
            "loss: 1.828806  [19200/48000]\n",
            "loss: 1.718969  [25600/48000]\n",
            "loss: 1.547526  [32000/48000]\n",
            "loss: 1.403744  [38400/48000]\n",
            "loss: 1.362023  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.240044  [    0/48000]\n",
            "loss: 1.223222  [ 6400/48000]\n",
            "loss: 1.049584  [12800/48000]\n",
            "loss: 0.942812  [19200/48000]\n",
            "loss: 0.972254  [25600/48000]\n",
            "loss: 0.853828  [32000/48000]\n",
            "loss: 0.801718  [38400/48000]\n",
            "loss: 0.838947  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.765164  [    0/48000]\n",
            "loss: 0.787880  [ 6400/48000]\n",
            "loss: 0.645647  [12800/48000]\n",
            "loss: 0.579206  [19200/48000]\n",
            "loss: 0.671732  [25600/48000]\n",
            "loss: 0.578149  [32000/48000]\n",
            "loss: 0.566942  [38400/48000]\n",
            "loss: 0.649331  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.568546  [    0/48000]\n",
            "loss: 0.596608  [ 6400/48000]\n",
            "loss: 0.456573  [12800/48000]\n",
            "loss: 0.424514  [19200/48000]\n",
            "loss: 0.533363  [25600/48000]\n",
            "loss: 0.457834  [32000/48000]\n",
            "loss: 0.450404  [38400/48000]\n",
            "loss: 0.541252  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.468538  [    0/48000]\n",
            "loss: 0.488951  [ 6400/48000]\n",
            "loss: 0.373310  [12800/48000]\n",
            "loss: 0.339365  [19200/48000]\n",
            "loss: 0.440892  [25600/48000]\n",
            "loss: 0.377896  [32000/48000]\n",
            "loss: 0.383030  [38400/48000]\n",
            "loss: 0.478079  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.402962 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=tanh, weight_decay=0.0001, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.353828  [    0/48000]\n",
            "loss: 2.172854  [ 6400/48000]\n",
            "loss: 2.044030  [12800/48000]\n",
            "loss: 1.874339  [19200/48000]\n",
            "loss: 1.742286  [25600/48000]\n",
            "loss: 1.579222  [32000/48000]\n",
            "loss: 1.441045  [38400/48000]\n",
            "loss: 1.370385  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.293465  [    0/48000]\n",
            "loss: 1.244037  [ 6400/48000]\n",
            "loss: 1.096488  [12800/48000]\n",
            "loss: 0.972145  [19200/48000]\n",
            "loss: 0.980481  [25600/48000]\n",
            "loss: 0.873358  [32000/48000]\n",
            "loss: 0.848359  [38400/48000]\n",
            "loss: 0.866206  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.781305  [    0/48000]\n",
            "loss: 0.806202  [ 6400/48000]\n",
            "loss: 0.692819  [12800/48000]\n",
            "loss: 0.591970  [19200/48000]\n",
            "loss: 0.683861  [25600/48000]\n",
            "loss: 0.594014  [32000/48000]\n",
            "loss: 0.592353  [38400/48000]\n",
            "loss: 0.658488  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.581236  [    0/48000]\n",
            "loss: 0.630314  [ 6400/48000]\n",
            "loss: 0.513070  [12800/48000]\n",
            "loss: 0.418123  [19200/48000]\n",
            "loss: 0.530936  [25600/48000]\n",
            "loss: 0.461183  [32000/48000]\n",
            "loss: 0.471763  [38400/48000]\n",
            "loss: 0.558892  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.479205  [    0/48000]\n",
            "loss: 0.524671  [ 6400/48000]\n",
            "loss: 0.403962  [12800/48000]\n",
            "loss: 0.333640  [19200/48000]\n",
            "loss: 0.449267  [25600/48000]\n",
            "loss: 0.394145  [32000/48000]\n",
            "loss: 0.399200  [38400/48000]\n",
            "loss: 0.487551  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.408480  [    0/48000]\n",
            "loss: 0.450352  [ 6400/48000]\n",
            "loss: 0.335308  [12800/48000]\n",
            "loss: 0.284769  [19200/48000]\n",
            "loss: 0.386750  [25600/48000]\n",
            "loss: 0.353093  [32000/48000]\n",
            "loss: 0.360292  [38400/48000]\n",
            "loss: 0.433251  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.372427  [    0/48000]\n",
            "loss: 0.410163  [ 6400/48000]\n",
            "loss: 0.287232  [12800/48000]\n",
            "loss: 0.249126  [19200/48000]\n",
            "loss: 0.353639  [25600/48000]\n",
            "loss: 0.305918  [32000/48000]\n",
            "loss: 0.315374  [38400/48000]\n",
            "loss: 0.393149  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.332665  [    0/48000]\n",
            "loss: 0.368290  [ 6400/48000]\n",
            "loss: 0.250321  [12800/48000]\n",
            "loss: 0.223961  [19200/48000]\n",
            "loss: 0.326142  [25600/48000]\n",
            "loss: 0.284262  [32000/48000]\n",
            "loss: 0.303369  [38400/48000]\n",
            "loss: 0.361857  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.309325  [    0/48000]\n",
            "loss: 0.349001  [ 6400/48000]\n",
            "loss: 0.222283  [12800/48000]\n",
            "loss: 0.214397  [19200/48000]\n",
            "loss: 0.283597  [25600/48000]\n",
            "loss: 0.263391  [32000/48000]\n",
            "loss: 0.279255  [38400/48000]\n",
            "loss: 0.340120  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.290235  [    0/48000]\n",
            "loss: 0.306041  [ 6400/48000]\n",
            "loss: 0.202067  [12800/48000]\n",
            "loss: 0.188841  [19200/48000]\n",
            "loss: 0.259230  [25600/48000]\n",
            "loss: 0.271061  [32000/48000]\n",
            "loss: 0.266738  [38400/48000]\n",
            "loss: 0.316731  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 92.0%, Avg loss: 0.288190 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=tanh, weight_decay=0.0001, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.358709  [    0/48000]\n",
            "loss: 2.173498  [ 6400/48000]\n",
            "loss: 2.035127  [12800/48000]\n",
            "loss: 1.862398  [19200/48000]\n",
            "loss: 1.736383  [25600/48000]\n",
            "loss: 1.581347  [32000/48000]\n",
            "loss: 1.445350  [38400/48000]\n",
            "loss: 1.368228  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.278305  [    0/48000]\n",
            "loss: 1.192932  [ 6400/48000]\n",
            "loss: 1.082238  [12800/48000]\n",
            "loss: 0.959759  [19200/48000]\n",
            "loss: 0.974418  [25600/48000]\n",
            "loss: 0.864470  [32000/48000]\n",
            "loss: 0.828922  [38400/48000]\n",
            "loss: 0.869287  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.788710  [    0/48000]\n",
            "loss: 0.740115  [ 6400/48000]\n",
            "loss: 0.685869  [12800/48000]\n",
            "loss: 0.598793  [19200/48000]\n",
            "loss: 0.663736  [25600/48000]\n",
            "loss: 0.587673  [32000/48000]\n",
            "loss: 0.617745  [38400/48000]\n",
            "loss: 0.678062  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.593606  [    0/48000]\n",
            "loss: 0.565549  [ 6400/48000]\n",
            "loss: 0.499849  [12800/48000]\n",
            "loss: 0.426883  [19200/48000]\n",
            "loss: 0.522774  [25600/48000]\n",
            "loss: 0.443536  [32000/48000]\n",
            "loss: 0.492961  [38400/48000]\n",
            "loss: 0.589748  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.493150  [    0/48000]\n",
            "loss: 0.460064  [ 6400/48000]\n",
            "loss: 0.389119  [12800/48000]\n",
            "loss: 0.334287  [19200/48000]\n",
            "loss: 0.428689  [25600/48000]\n",
            "loss: 0.372476  [32000/48000]\n",
            "loss: 0.415833  [38400/48000]\n",
            "loss: 0.516590  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.436399  [    0/48000]\n",
            "loss: 0.404424  [ 6400/48000]\n",
            "loss: 0.318532  [12800/48000]\n",
            "loss: 0.277672  [19200/48000]\n",
            "loss: 0.374182  [25600/48000]\n",
            "loss: 0.322085  [32000/48000]\n",
            "loss: 0.372227  [38400/48000]\n",
            "loss: 0.466898  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.379423  [    0/48000]\n",
            "loss: 0.360508  [ 6400/48000]\n",
            "loss: 0.280838  [12800/48000]\n",
            "loss: 0.260788  [19200/48000]\n",
            "loss: 0.338947  [25600/48000]\n",
            "loss: 0.294158  [32000/48000]\n",
            "loss: 0.332130  [38400/48000]\n",
            "loss: 0.435648  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.335705  [    0/48000]\n",
            "loss: 0.333854  [ 6400/48000]\n",
            "loss: 0.253003  [12800/48000]\n",
            "loss: 0.233439  [19200/48000]\n",
            "loss: 0.303963  [25600/48000]\n",
            "loss: 0.258074  [32000/48000]\n",
            "loss: 0.311021  [38400/48000]\n",
            "loss: 0.413100  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.315765  [    0/48000]\n",
            "loss: 0.303249  [ 6400/48000]\n",
            "loss: 0.237247  [12800/48000]\n",
            "loss: 0.204435  [19200/48000]\n",
            "loss: 0.283838  [25600/48000]\n",
            "loss: 0.231282  [32000/48000]\n",
            "loss: 0.281709  [38400/48000]\n",
            "loss: 0.387984  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.289540  [    0/48000]\n",
            "loss: 0.277043  [ 6400/48000]\n",
            "loss: 0.211598  [12800/48000]\n",
            "loss: 0.180772  [19200/48000]\n",
            "loss: 0.252383  [25600/48000]\n",
            "loss: 0.216896  [32000/48000]\n",
            "loss: 0.271111  [38400/48000]\n",
            "loss: 0.355186  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.265780  [    0/48000]\n",
            "loss: 0.255779  [ 6400/48000]\n",
            "loss: 0.190101  [12800/48000]\n",
            "loss: 0.179320  [19200/48000]\n",
            "loss: 0.243948  [25600/48000]\n",
            "loss: 0.202402  [32000/48000]\n",
            "loss: 0.256220  [38400/48000]\n",
            "loss: 0.346967  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.258919  [    0/48000]\n",
            "loss: 0.250871  [ 6400/48000]\n",
            "loss: 0.189234  [12800/48000]\n",
            "loss: 0.161625  [19200/48000]\n",
            "loss: 0.230688  [25600/48000]\n",
            "loss: 0.190207  [32000/48000]\n",
            "loss: 0.240885  [38400/48000]\n",
            "loss: 0.328239  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.247793  [    0/48000]\n",
            "loss: 0.216982  [ 6400/48000]\n",
            "loss: 0.180570  [12800/48000]\n",
            "loss: 0.160189  [19200/48000]\n",
            "loss: 0.202252  [25600/48000]\n",
            "loss: 0.178527  [32000/48000]\n",
            "loss: 0.226840  [38400/48000]\n",
            "loss: 0.317103  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.229033  [    0/48000]\n",
            "loss: 0.206887  [ 6400/48000]\n",
            "loss: 0.169617  [12800/48000]\n",
            "loss: 0.145804  [19200/48000]\n",
            "loss: 0.203895  [25600/48000]\n",
            "loss: 0.165995  [32000/48000]\n",
            "loss: 0.216546  [38400/48000]\n",
            "loss: 0.309596  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.230545  [    0/48000]\n",
            "loss: 0.200262  [ 6400/48000]\n",
            "loss: 0.171812  [12800/48000]\n",
            "loss: 0.130256  [19200/48000]\n",
            "loss: 0.195817  [25600/48000]\n",
            "loss: 0.166652  [32000/48000]\n",
            "loss: 0.204922  [38400/48000]\n",
            "loss: 0.292914  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.220315  [    0/48000]\n",
            "loss: 0.182239  [ 6400/48000]\n",
            "loss: 0.160216  [12800/48000]\n",
            "loss: 0.128763  [19200/48000]\n",
            "loss: 0.180312  [25600/48000]\n",
            "loss: 0.163930  [32000/48000]\n",
            "loss: 0.218056  [38400/48000]\n",
            "loss: 0.282693  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.211489  [    0/48000]\n",
            "loss: 0.178532  [ 6400/48000]\n",
            "loss: 0.148019  [12800/48000]\n",
            "loss: 0.123559  [19200/48000]\n",
            "loss: 0.159710  [25600/48000]\n",
            "loss: 0.157064  [32000/48000]\n",
            "loss: 0.190735  [38400/48000]\n",
            "loss: 0.271239  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.200036  [    0/48000]\n",
            "loss: 0.171735  [ 6400/48000]\n",
            "loss: 0.143177  [12800/48000]\n",
            "loss: 0.120730  [19200/48000]\n",
            "loss: 0.147896  [25600/48000]\n",
            "loss: 0.144346  [32000/48000]\n",
            "loss: 0.185107  [38400/48000]\n",
            "loss: 0.261281  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.190156  [    0/48000]\n",
            "loss: 0.161311  [ 6400/48000]\n",
            "loss: 0.133165  [12800/48000]\n",
            "loss: 0.109380  [19200/48000]\n",
            "loss: 0.141686  [25600/48000]\n",
            "loss: 0.141248  [32000/48000]\n",
            "loss: 0.170496  [38400/48000]\n",
            "loss: 0.248285  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.180838  [    0/48000]\n",
            "loss: 0.148094  [ 6400/48000]\n",
            "loss: 0.123526  [12800/48000]\n",
            "loss: 0.102001  [19200/48000]\n",
            "loss: 0.135127  [25600/48000]\n",
            "loss: 0.129015  [32000/48000]\n",
            "loss: 0.166113  [38400/48000]\n",
            "loss: 0.224594  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 93.6%, Avg loss: 0.220858 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=tanh, weight_decay=0.001, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.296207  [    0/48000]\n",
            "loss: 2.168712  [ 6400/48000]\n",
            "loss: 2.000142  [12800/48000]\n",
            "loss: 1.841722  [19200/48000]\n",
            "loss: 1.725256  [25600/48000]\n",
            "loss: 1.573284  [32000/48000]\n",
            "loss: 1.457493  [38400/48000]\n",
            "loss: 1.366876  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.262168  [    0/48000]\n",
            "loss: 1.265258  [ 6400/48000]\n",
            "loss: 1.074337  [12800/48000]\n",
            "loss: 0.954950  [19200/48000]\n",
            "loss: 0.977383  [25600/48000]\n",
            "loss: 0.874861  [32000/48000]\n",
            "loss: 0.817737  [38400/48000]\n",
            "loss: 0.848361  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.760920  [    0/48000]\n",
            "loss: 0.809726  [ 6400/48000]\n",
            "loss: 0.659589  [12800/48000]\n",
            "loss: 0.592813  [19200/48000]\n",
            "loss: 0.659732  [25600/48000]\n",
            "loss: 0.601964  [32000/48000]\n",
            "loss: 0.577956  [38400/48000]\n",
            "loss: 0.655497  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.553336  [    0/48000]\n",
            "loss: 0.619165  [ 6400/48000]\n",
            "loss: 0.482914  [12800/48000]\n",
            "loss: 0.430817  [19200/48000]\n",
            "loss: 0.510122  [25600/48000]\n",
            "loss: 0.465554  [32000/48000]\n",
            "loss: 0.472613  [38400/48000]\n",
            "loss: 0.554375  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.443027  [    0/48000]\n",
            "loss: 0.505033  [ 6400/48000]\n",
            "loss: 0.370224  [12800/48000]\n",
            "loss: 0.354401  [19200/48000]\n",
            "loss: 0.428271  [25600/48000]\n",
            "loss: 0.384145  [32000/48000]\n",
            "loss: 0.408136  [38400/48000]\n",
            "loss: 0.485810  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 89.8%, Avg loss: 0.404450 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=tanh, weight_decay=0.001, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302098  [    0/48000]\n",
            "loss: 2.158442  [ 6400/48000]\n",
            "loss: 2.010776  [12800/48000]\n",
            "loss: 1.840985  [19200/48000]\n",
            "loss: 1.724070  [25600/48000]\n",
            "loss: 1.556705  [32000/48000]\n",
            "loss: 1.410229  [38400/48000]\n",
            "loss: 1.378460  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.261178  [    0/48000]\n",
            "loss: 1.199361  [ 6400/48000]\n",
            "loss: 1.058968  [12800/48000]\n",
            "loss: 0.946761  [19200/48000]\n",
            "loss: 0.944987  [25600/48000]\n",
            "loss: 0.829174  [32000/48000]\n",
            "loss: 0.788183  [38400/48000]\n",
            "loss: 0.855218  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.756035  [    0/48000]\n",
            "loss: 0.763380  [ 6400/48000]\n",
            "loss: 0.655802  [12800/48000]\n",
            "loss: 0.590967  [19200/48000]\n",
            "loss: 0.652411  [25600/48000]\n",
            "loss: 0.564846  [32000/48000]\n",
            "loss: 0.554287  [38400/48000]\n",
            "loss: 0.656240  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.552343  [    0/48000]\n",
            "loss: 0.589365  [ 6400/48000]\n",
            "loss: 0.484693  [12800/48000]\n",
            "loss: 0.439426  [19200/48000]\n",
            "loss: 0.498463  [25600/48000]\n",
            "loss: 0.444740  [32000/48000]\n",
            "loss: 0.447840  [38400/48000]\n",
            "loss: 0.549877  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.456262  [    0/48000]\n",
            "loss: 0.479859  [ 6400/48000]\n",
            "loss: 0.389684  [12800/48000]\n",
            "loss: 0.345318  [19200/48000]\n",
            "loss: 0.415085  [25600/48000]\n",
            "loss: 0.379602  [32000/48000]\n",
            "loss: 0.394653  [38400/48000]\n",
            "loss: 0.483306  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.392011  [    0/48000]\n",
            "loss: 0.419071  [ 6400/48000]\n",
            "loss: 0.332736  [12800/48000]\n",
            "loss: 0.299434  [19200/48000]\n",
            "loss: 0.370872  [25600/48000]\n",
            "loss: 0.328296  [32000/48000]\n",
            "loss: 0.342373  [38400/48000]\n",
            "loss: 0.451844  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.362627  [    0/48000]\n",
            "loss: 0.370743  [ 6400/48000]\n",
            "loss: 0.291835  [12800/48000]\n",
            "loss: 0.264494  [19200/48000]\n",
            "loss: 0.333616  [25600/48000]\n",
            "loss: 0.304662  [32000/48000]\n",
            "loss: 0.296055  [38400/48000]\n",
            "loss: 0.415281  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.323506  [    0/48000]\n",
            "loss: 0.334409  [ 6400/48000]\n",
            "loss: 0.259389  [12800/48000]\n",
            "loss: 0.235749  [19200/48000]\n",
            "loss: 0.314181  [25600/48000]\n",
            "loss: 0.285096  [32000/48000]\n",
            "loss: 0.284702  [38400/48000]\n",
            "loss: 0.387135  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.302803  [    0/48000]\n",
            "loss: 0.298493  [ 6400/48000]\n",
            "loss: 0.243134  [12800/48000]\n",
            "loss: 0.219925  [19200/48000]\n",
            "loss: 0.284603  [25600/48000]\n",
            "loss: 0.267619  [32000/48000]\n",
            "loss: 0.251332  [38400/48000]\n",
            "loss: 0.360072  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.283240  [    0/48000]\n",
            "loss: 0.270529  [ 6400/48000]\n",
            "loss: 0.208429  [12800/48000]\n",
            "loss: 0.209495  [19200/48000]\n",
            "loss: 0.278545  [25600/48000]\n",
            "loss: 0.242843  [32000/48000]\n",
            "loss: 0.240901  [38400/48000]\n",
            "loss: 0.347826  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 91.9%, Avg loss: 0.288387 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=tanh, weight_decay=0.001, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.323562  [    0/48000]\n",
            "loss: 2.157212  [ 6400/48000]\n",
            "loss: 1.995368  [12800/48000]\n",
            "loss: 1.869704  [19200/48000]\n",
            "loss: 1.712349  [25600/48000]\n",
            "loss: 1.586593  [32000/48000]\n",
            "loss: 1.444534  [38400/48000]\n",
            "loss: 1.399163  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.283848  [    0/48000]\n",
            "loss: 1.187065  [ 6400/48000]\n",
            "loss: 1.050379  [12800/48000]\n",
            "loss: 0.984809  [19200/48000]\n",
            "loss: 0.928060  [25600/48000]\n",
            "loss: 0.858045  [32000/48000]\n",
            "loss: 0.837843  [38400/48000]\n",
            "loss: 0.873252  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.776025  [    0/48000]\n",
            "loss: 0.719941  [ 6400/48000]\n",
            "loss: 0.651034  [12800/48000]\n",
            "loss: 0.602090  [19200/48000]\n",
            "loss: 0.620622  [25600/48000]\n",
            "loss: 0.577575  [32000/48000]\n",
            "loss: 0.599812  [38400/48000]\n",
            "loss: 0.650875  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.575263  [    0/48000]\n",
            "loss: 0.529530  [ 6400/48000]\n",
            "loss: 0.476598  [12800/48000]\n",
            "loss: 0.449110  [19200/48000]\n",
            "loss: 0.484465  [25600/48000]\n",
            "loss: 0.456248  [32000/48000]\n",
            "loss: 0.504439  [38400/48000]\n",
            "loss: 0.548051  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.476410  [    0/48000]\n",
            "loss: 0.433902  [ 6400/48000]\n",
            "loss: 0.386930  [12800/48000]\n",
            "loss: 0.375021  [19200/48000]\n",
            "loss: 0.409792  [25600/48000]\n",
            "loss: 0.398035  [32000/48000]\n",
            "loss: 0.418464  [38400/48000]\n",
            "loss: 0.480567  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.409106  [    0/48000]\n",
            "loss: 0.394999  [ 6400/48000]\n",
            "loss: 0.317235  [12800/48000]\n",
            "loss: 0.318886  [19200/48000]\n",
            "loss: 0.356645  [25600/48000]\n",
            "loss: 0.342463  [32000/48000]\n",
            "loss: 0.375423  [38400/48000]\n",
            "loss: 0.443690  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.374177  [    0/48000]\n",
            "loss: 0.344489  [ 6400/48000]\n",
            "loss: 0.291897  [12800/48000]\n",
            "loss: 0.282067  [19200/48000]\n",
            "loss: 0.320664  [25600/48000]\n",
            "loss: 0.316301  [32000/48000]\n",
            "loss: 0.348419  [38400/48000]\n",
            "loss: 0.399025  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.344134  [    0/48000]\n",
            "loss: 0.326389  [ 6400/48000]\n",
            "loss: 0.256627  [12800/48000]\n",
            "loss: 0.268771  [19200/48000]\n",
            "loss: 0.292284  [25600/48000]\n",
            "loss: 0.295867  [32000/48000]\n",
            "loss: 0.316727  [38400/48000]\n",
            "loss: 0.360793  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.309669  [    0/48000]\n",
            "loss: 0.302840  [ 6400/48000]\n",
            "loss: 0.238927  [12800/48000]\n",
            "loss: 0.246247  [19200/48000]\n",
            "loss: 0.261478  [25600/48000]\n",
            "loss: 0.257470  [32000/48000]\n",
            "loss: 0.288368  [38400/48000]\n",
            "loss: 0.338421  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.301001  [    0/48000]\n",
            "loss: 0.280910  [ 6400/48000]\n",
            "loss: 0.231677  [12800/48000]\n",
            "loss: 0.232065  [19200/48000]\n",
            "loss: 0.247710  [25600/48000]\n",
            "loss: 0.244819  [32000/48000]\n",
            "loss: 0.263215  [38400/48000]\n",
            "loss: 0.316925  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.266762  [    0/48000]\n",
            "loss: 0.254636  [ 6400/48000]\n",
            "loss: 0.207485  [12800/48000]\n",
            "loss: 0.205451  [19200/48000]\n",
            "loss: 0.226035  [25600/48000]\n",
            "loss: 0.232164  [32000/48000]\n",
            "loss: 0.265769  [38400/48000]\n",
            "loss: 0.280593  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.261917  [    0/48000]\n",
            "loss: 0.219534  [ 6400/48000]\n",
            "loss: 0.195629  [12800/48000]\n",
            "loss: 0.195729  [19200/48000]\n",
            "loss: 0.210687  [25600/48000]\n",
            "loss: 0.229280  [32000/48000]\n",
            "loss: 0.247279  [38400/48000]\n",
            "loss: 0.276466  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.245683  [    0/48000]\n",
            "loss: 0.218310  [ 6400/48000]\n",
            "loss: 0.187472  [12800/48000]\n",
            "loss: 0.192201  [19200/48000]\n",
            "loss: 0.203314  [25600/48000]\n",
            "loss: 0.213373  [32000/48000]\n",
            "loss: 0.228572  [38400/48000]\n",
            "loss: 0.251849  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.234509  [    0/48000]\n",
            "loss: 0.215516  [ 6400/48000]\n",
            "loss: 0.166698  [12800/48000]\n",
            "loss: 0.181310  [19200/48000]\n",
            "loss: 0.176441  [25600/48000]\n",
            "loss: 0.195864  [32000/48000]\n",
            "loss: 0.223988  [38400/48000]\n",
            "loss: 0.235936  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.215316  [    0/48000]\n",
            "loss: 0.199201  [ 6400/48000]\n",
            "loss: 0.160523  [12800/48000]\n",
            "loss: 0.171826  [19200/48000]\n",
            "loss: 0.180701  [25600/48000]\n",
            "loss: 0.181750  [32000/48000]\n",
            "loss: 0.212052  [38400/48000]\n",
            "loss: 0.221259  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.203813  [    0/48000]\n",
            "loss: 0.189883  [ 6400/48000]\n",
            "loss: 0.145947  [12800/48000]\n",
            "loss: 0.164160  [19200/48000]\n",
            "loss: 0.163237  [25600/48000]\n",
            "loss: 0.180835  [32000/48000]\n",
            "loss: 0.212663  [38400/48000]\n",
            "loss: 0.198804  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.207718  [    0/48000]\n",
            "loss: 0.182806  [ 6400/48000]\n",
            "loss: 0.145114  [12800/48000]\n",
            "loss: 0.159928  [19200/48000]\n",
            "loss: 0.151780  [25600/48000]\n",
            "loss: 0.176477  [32000/48000]\n",
            "loss: 0.197550  [38400/48000]\n",
            "loss: 0.191525  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.198563  [    0/48000]\n",
            "loss: 0.167695  [ 6400/48000]\n",
            "loss: 0.138924  [12800/48000]\n",
            "loss: 0.151711  [19200/48000]\n",
            "loss: 0.151310  [25600/48000]\n",
            "loss: 0.168197  [32000/48000]\n",
            "loss: 0.185041  [38400/48000]\n",
            "loss: 0.179853  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.185484  [    0/48000]\n",
            "loss: 0.167668  [ 6400/48000]\n",
            "loss: 0.136458  [12800/48000]\n",
            "loss: 0.144460  [19200/48000]\n",
            "loss: 0.145432  [25600/48000]\n",
            "loss: 0.158353  [32000/48000]\n",
            "loss: 0.182080  [38400/48000]\n",
            "loss: 0.168903  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.178639  [    0/48000]\n",
            "loss: 0.154427  [ 6400/48000]\n",
            "loss: 0.122823  [12800/48000]\n",
            "loss: 0.141576  [19200/48000]\n",
            "loss: 0.133266  [25600/48000]\n",
            "loss: 0.151207  [32000/48000]\n",
            "loss: 0.173260  [38400/48000]\n",
            "loss: 0.161798  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 93.6%, Avg loss: 0.218371 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=leaky_relu, weight_decay=0, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 4.586716  [    0/48000]\n",
            "loss: 0.558063  [ 6400/48000]\n",
            "loss: 0.241471  [12800/48000]\n",
            "loss: 0.310082  [19200/48000]\n",
            "loss: 0.389416  [25600/48000]\n",
            "loss: 0.218064  [32000/48000]\n",
            "loss: 0.215581  [38400/48000]\n",
            "loss: 0.230590  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.302754  [    0/48000]\n",
            "loss: 0.225167  [ 6400/48000]\n",
            "loss: 0.104166  [12800/48000]\n",
            "loss: 0.160937  [19200/48000]\n",
            "loss: 0.202806  [25600/48000]\n",
            "loss: 0.120428  [32000/48000]\n",
            "loss: 0.095682  [38400/48000]\n",
            "loss: 0.171804  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.245783  [    0/48000]\n",
            "loss: 0.171602  [ 6400/48000]\n",
            "loss: 0.070214  [12800/48000]\n",
            "loss: 0.105584  [19200/48000]\n",
            "loss: 0.122386  [25600/48000]\n",
            "loss: 0.085286  [32000/48000]\n",
            "loss: 0.054676  [38400/48000]\n",
            "loss: 0.134397  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.214620  [    0/48000]\n",
            "loss: 0.143960  [ 6400/48000]\n",
            "loss: 0.047614  [12800/48000]\n",
            "loss: 0.073730  [19200/48000]\n",
            "loss: 0.093590  [25600/48000]\n",
            "loss: 0.065966  [32000/48000]\n",
            "loss: 0.036695  [38400/48000]\n",
            "loss: 0.106815  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.189119  [    0/48000]\n",
            "loss: 0.124381  [ 6400/48000]\n",
            "loss: 0.031770  [12800/48000]\n",
            "loss: 0.054257  [19200/48000]\n",
            "loss: 0.076269  [25600/48000]\n",
            "loss: 0.052707  [32000/48000]\n",
            "loss: 0.027484  [38400/48000]\n",
            "loss: 0.085461  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 0.134041 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=leaky_relu, weight_decay=0, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 7.027926  [    0/48000]\n",
            "loss: 0.498686  [ 6400/48000]\n",
            "loss: 0.245184  [12800/48000]\n",
            "loss: 0.331196  [19200/48000]\n",
            "loss: 0.296168  [25600/48000]\n",
            "loss: 0.257509  [32000/48000]\n",
            "loss: 0.207203  [38400/48000]\n",
            "loss: 0.177199  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.213319  [    0/48000]\n",
            "loss: 0.169968  [ 6400/48000]\n",
            "loss: 0.144519  [12800/48000]\n",
            "loss: 0.147914  [19200/48000]\n",
            "loss: 0.177137  [25600/48000]\n",
            "loss: 0.139253  [32000/48000]\n",
            "loss: 0.090542  [38400/48000]\n",
            "loss: 0.113626  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.165113  [    0/48000]\n",
            "loss: 0.119672  [ 6400/48000]\n",
            "loss: 0.114854  [12800/48000]\n",
            "loss: 0.098149  [19200/48000]\n",
            "loss: 0.126891  [25600/48000]\n",
            "loss: 0.110819  [32000/48000]\n",
            "loss: 0.054840  [38400/48000]\n",
            "loss: 0.083351  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.138698  [    0/48000]\n",
            "loss: 0.095160  [ 6400/48000]\n",
            "loss: 0.089483  [12800/48000]\n",
            "loss: 0.065324  [19200/48000]\n",
            "loss: 0.104184  [25600/48000]\n",
            "loss: 0.099499  [32000/48000]\n",
            "loss: 0.041631  [38400/48000]\n",
            "loss: 0.064890  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.118227  [    0/48000]\n",
            "loss: 0.080358  [ 6400/48000]\n",
            "loss: 0.070172  [12800/48000]\n",
            "loss: 0.050794  [19200/48000]\n",
            "loss: 0.080765  [25600/48000]\n",
            "loss: 0.088701  [32000/48000]\n",
            "loss: 0.033739  [38400/48000]\n",
            "loss: 0.050734  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.104971  [    0/48000]\n",
            "loss: 0.067239  [ 6400/48000]\n",
            "loss: 0.050532  [12800/48000]\n",
            "loss: 0.038606  [19200/48000]\n",
            "loss: 0.060976  [25600/48000]\n",
            "loss: 0.080727  [32000/48000]\n",
            "loss: 0.028550  [38400/48000]\n",
            "loss: 0.040390  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.090354  [    0/48000]\n",
            "loss: 0.055174  [ 6400/48000]\n",
            "loss: 0.034561  [12800/48000]\n",
            "loss: 0.031256  [19200/48000]\n",
            "loss: 0.047122  [25600/48000]\n",
            "loss: 0.070672  [32000/48000]\n",
            "loss: 0.024624  [38400/48000]\n",
            "loss: 0.032244  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.077477  [    0/48000]\n",
            "loss: 0.047386  [ 6400/48000]\n",
            "loss: 0.023470  [12800/48000]\n",
            "loss: 0.025251  [19200/48000]\n",
            "loss: 0.036525  [25600/48000]\n",
            "loss: 0.059707  [32000/48000]\n",
            "loss: 0.020992  [38400/48000]\n",
            "loss: 0.028282  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.068287  [    0/48000]\n",
            "loss: 0.041426  [ 6400/48000]\n",
            "loss: 0.016942  [12800/48000]\n",
            "loss: 0.020366  [19200/48000]\n",
            "loss: 0.029897  [25600/48000]\n",
            "loss: 0.047870  [32000/48000]\n",
            "loss: 0.018182  [38400/48000]\n",
            "loss: 0.025661  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.059065  [    0/48000]\n",
            "loss: 0.036208  [ 6400/48000]\n",
            "loss: 0.013240  [12800/48000]\n",
            "loss: 0.017544  [19200/48000]\n",
            "loss: 0.024357  [25600/48000]\n",
            "loss: 0.038883  [32000/48000]\n",
            "loss: 0.015804  [38400/48000]\n",
            "loss: 0.022832  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.105543 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=leaky_relu, weight_decay=0, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 4.148140  [    0/48000]\n",
            "loss: 0.495027  [ 6400/48000]\n",
            "loss: 0.265405  [12800/48000]\n",
            "loss: 0.362834  [19200/48000]\n",
            "loss: 0.304471  [25600/48000]\n",
            "loss: 0.249218  [32000/48000]\n",
            "loss: 0.205375  [38400/48000]\n",
            "loss: 0.202862  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.207352  [    0/48000]\n",
            "loss: 0.177447  [ 6400/48000]\n",
            "loss: 0.120637  [12800/48000]\n",
            "loss: 0.202509  [19200/48000]\n",
            "loss: 0.165741  [25600/48000]\n",
            "loss: 0.142223  [32000/48000]\n",
            "loss: 0.082316  [38400/48000]\n",
            "loss: 0.121683  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.151917  [    0/48000]\n",
            "loss: 0.109779  [ 6400/48000]\n",
            "loss: 0.073279  [12800/48000]\n",
            "loss: 0.141465  [19200/48000]\n",
            "loss: 0.123767  [25600/48000]\n",
            "loss: 0.102097  [32000/48000]\n",
            "loss: 0.051481  [38400/48000]\n",
            "loss: 0.093347  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.129735  [    0/48000]\n",
            "loss: 0.078258  [ 6400/48000]\n",
            "loss: 0.053026  [12800/48000]\n",
            "loss: 0.096940  [19200/48000]\n",
            "loss: 0.091781  [25600/48000]\n",
            "loss: 0.072796  [32000/48000]\n",
            "loss: 0.036828  [38400/48000]\n",
            "loss: 0.077151  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.117759  [    0/48000]\n",
            "loss: 0.059301  [ 6400/48000]\n",
            "loss: 0.039360  [12800/48000]\n",
            "loss: 0.071704  [19200/48000]\n",
            "loss: 0.066654  [25600/48000]\n",
            "loss: 0.052588  [32000/48000]\n",
            "loss: 0.027834  [38400/48000]\n",
            "loss: 0.066280  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.106300  [    0/48000]\n",
            "loss: 0.046336  [ 6400/48000]\n",
            "loss: 0.030942  [12800/48000]\n",
            "loss: 0.056075  [19200/48000]\n",
            "loss: 0.049638  [25600/48000]\n",
            "loss: 0.039773  [32000/48000]\n",
            "loss: 0.021303  [38400/48000]\n",
            "loss: 0.055972  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.094275  [    0/48000]\n",
            "loss: 0.036799  [ 6400/48000]\n",
            "loss: 0.024568  [12800/48000]\n",
            "loss: 0.044400  [19200/48000]\n",
            "loss: 0.039400  [25600/48000]\n",
            "loss: 0.031578  [32000/48000]\n",
            "loss: 0.017321  [38400/48000]\n",
            "loss: 0.048516  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.083879  [    0/48000]\n",
            "loss: 0.029547  [ 6400/48000]\n",
            "loss: 0.020867  [12800/48000]\n",
            "loss: 0.036278  [19200/48000]\n",
            "loss: 0.032336  [25600/48000]\n",
            "loss: 0.025413  [32000/48000]\n",
            "loss: 0.014208  [38400/48000]\n",
            "loss: 0.041524  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.072910  [    0/48000]\n",
            "loss: 0.025498  [ 6400/48000]\n",
            "loss: 0.018938  [12800/48000]\n",
            "loss: 0.029934  [19200/48000]\n",
            "loss: 0.027816  [25600/48000]\n",
            "loss: 0.020756  [32000/48000]\n",
            "loss: 0.012271  [38400/48000]\n",
            "loss: 0.036021  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.063102  [    0/48000]\n",
            "loss: 0.022279  [ 6400/48000]\n",
            "loss: 0.017158  [12800/48000]\n",
            "loss: 0.024933  [19200/48000]\n",
            "loss: 0.024221  [25600/48000]\n",
            "loss: 0.016779  [32000/48000]\n",
            "loss: 0.011078  [38400/48000]\n",
            "loss: 0.031626  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.053981  [    0/48000]\n",
            "loss: 0.020001  [ 6400/48000]\n",
            "loss: 0.015510  [12800/48000]\n",
            "loss: 0.021630  [19200/48000]\n",
            "loss: 0.021270  [25600/48000]\n",
            "loss: 0.014568  [32000/48000]\n",
            "loss: 0.009740  [38400/48000]\n",
            "loss: 0.027869  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.044695  [    0/48000]\n",
            "loss: 0.018003  [ 6400/48000]\n",
            "loss: 0.014236  [12800/48000]\n",
            "loss: 0.019574  [19200/48000]\n",
            "loss: 0.018424  [25600/48000]\n",
            "loss: 0.012322  [32000/48000]\n",
            "loss: 0.008891  [38400/48000]\n",
            "loss: 0.024850  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.035467  [    0/48000]\n",
            "loss: 0.016202  [ 6400/48000]\n",
            "loss: 0.012170  [12800/48000]\n",
            "loss: 0.016866  [19200/48000]\n",
            "loss: 0.016660  [25600/48000]\n",
            "loss: 0.011254  [32000/48000]\n",
            "loss: 0.008218  [38400/48000]\n",
            "loss: 0.021852  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.028573  [    0/48000]\n",
            "loss: 0.014432  [ 6400/48000]\n",
            "loss: 0.010699  [12800/48000]\n",
            "loss: 0.014921  [19200/48000]\n",
            "loss: 0.015609  [25600/48000]\n",
            "loss: 0.010006  [32000/48000]\n",
            "loss: 0.007512  [38400/48000]\n",
            "loss: 0.019558  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.022922  [    0/48000]\n",
            "loss: 0.013090  [ 6400/48000]\n",
            "loss: 0.009235  [12800/48000]\n",
            "loss: 0.013346  [19200/48000]\n",
            "loss: 0.014228  [25600/48000]\n",
            "loss: 0.009205  [32000/48000]\n",
            "loss: 0.006831  [38400/48000]\n",
            "loss: 0.017469  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.019070  [    0/48000]\n",
            "loss: 0.011662  [ 6400/48000]\n",
            "loss: 0.008126  [12800/48000]\n",
            "loss: 0.011813  [19200/48000]\n",
            "loss: 0.012821  [25600/48000]\n",
            "loss: 0.008737  [32000/48000]\n",
            "loss: 0.006150  [38400/48000]\n",
            "loss: 0.015683  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.015993  [    0/48000]\n",
            "loss: 0.010620  [ 6400/48000]\n",
            "loss: 0.006726  [12800/48000]\n",
            "loss: 0.010639  [19200/48000]\n",
            "loss: 0.011953  [25600/48000]\n",
            "loss: 0.007982  [32000/48000]\n",
            "loss: 0.005420  [38400/48000]\n",
            "loss: 0.014209  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.014069  [    0/48000]\n",
            "loss: 0.009795  [ 6400/48000]\n",
            "loss: 0.005916  [12800/48000]\n",
            "loss: 0.009646  [19200/48000]\n",
            "loss: 0.010871  [25600/48000]\n",
            "loss: 0.007559  [32000/48000]\n",
            "loss: 0.004799  [38400/48000]\n",
            "loss: 0.012799  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.012406  [    0/48000]\n",
            "loss: 0.008972  [ 6400/48000]\n",
            "loss: 0.005141  [12800/48000]\n",
            "loss: 0.008871  [19200/48000]\n",
            "loss: 0.009974  [25600/48000]\n",
            "loss: 0.007068  [32000/48000]\n",
            "loss: 0.004294  [38400/48000]\n",
            "loss: 0.011567  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.011193  [    0/48000]\n",
            "loss: 0.008331  [ 6400/48000]\n",
            "loss: 0.004489  [12800/48000]\n",
            "loss: 0.008158  [19200/48000]\n",
            "loss: 0.009256  [25600/48000]\n",
            "loss: 0.006771  [32000/48000]\n",
            "loss: 0.003911  [38400/48000]\n",
            "loss: 0.010609  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.107464 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=leaky_relu, weight_decay=0.0001, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.275041  [    0/48000]\n",
            "loss: 0.573930  [ 6400/48000]\n",
            "loss: 0.294391  [12800/48000]\n",
            "loss: 0.295882  [19200/48000]\n",
            "loss: 0.363341  [25600/48000]\n",
            "loss: 0.238708  [32000/48000]\n",
            "loss: 0.160837  [38400/48000]\n",
            "loss: 0.274303  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.205652  [    0/48000]\n",
            "loss: 0.240273  [ 6400/48000]\n",
            "loss: 0.114453  [12800/48000]\n",
            "loss: 0.149959  [19200/48000]\n",
            "loss: 0.198327  [25600/48000]\n",
            "loss: 0.128124  [32000/48000]\n",
            "loss: 0.067597  [38400/48000]\n",
            "loss: 0.192544  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.177273  [    0/48000]\n",
            "loss: 0.184084  [ 6400/48000]\n",
            "loss: 0.069620  [12800/48000]\n",
            "loss: 0.074556  [19200/48000]\n",
            "loss: 0.135189  [25600/48000]\n",
            "loss: 0.094303  [32000/48000]\n",
            "loss: 0.047971  [38400/48000]\n",
            "loss: 0.151382  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.154548  [    0/48000]\n",
            "loss: 0.147472  [ 6400/48000]\n",
            "loss: 0.046559  [12800/48000]\n",
            "loss: 0.053424  [19200/48000]\n",
            "loss: 0.107519  [25600/48000]\n",
            "loss: 0.080752  [32000/48000]\n",
            "loss: 0.039744  [38400/48000]\n",
            "loss: 0.117052  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.133144  [    0/48000]\n",
            "loss: 0.115594  [ 6400/48000]\n",
            "loss: 0.037238  [12800/48000]\n",
            "loss: 0.041679  [19200/48000]\n",
            "loss: 0.085804  [25600/48000]\n",
            "loss: 0.064957  [32000/48000]\n",
            "loss: 0.033642  [38400/48000]\n",
            "loss: 0.092934  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 0.134058 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=leaky_relu, weight_decay=0.0001, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.507593  [    0/48000]\n",
            "loss: 0.511140  [ 6400/48000]\n",
            "loss: 0.244924  [12800/48000]\n",
            "loss: 0.266960  [19200/48000]\n",
            "loss: 0.323460  [25600/48000]\n",
            "loss: 0.252507  [32000/48000]\n",
            "loss: 0.239517  [38400/48000]\n",
            "loss: 0.189260  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.205277  [    0/48000]\n",
            "loss: 0.201603  [ 6400/48000]\n",
            "loss: 0.124445  [12800/48000]\n",
            "loss: 0.133769  [19200/48000]\n",
            "loss: 0.149798  [25600/48000]\n",
            "loss: 0.140029  [32000/48000]\n",
            "loss: 0.135727  [38400/48000]\n",
            "loss: 0.109896  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.182231  [    0/48000]\n",
            "loss: 0.142102  [ 6400/48000]\n",
            "loss: 0.096130  [12800/48000]\n",
            "loss: 0.075659  [19200/48000]\n",
            "loss: 0.083673  [25600/48000]\n",
            "loss: 0.105297  [32000/48000]\n",
            "loss: 0.087749  [38400/48000]\n",
            "loss: 0.079303  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.159660  [    0/48000]\n",
            "loss: 0.114591  [ 6400/48000]\n",
            "loss: 0.074363  [12800/48000]\n",
            "loss: 0.049098  [19200/48000]\n",
            "loss: 0.048990  [25600/48000]\n",
            "loss: 0.084855  [32000/48000]\n",
            "loss: 0.059209  [38400/48000]\n",
            "loss: 0.058407  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.142195  [    0/48000]\n",
            "loss: 0.097233  [ 6400/48000]\n",
            "loss: 0.053153  [12800/48000]\n",
            "loss: 0.035162  [19200/48000]\n",
            "loss: 0.034183  [25600/48000]\n",
            "loss: 0.067403  [32000/48000]\n",
            "loss: 0.039704  [38400/48000]\n",
            "loss: 0.047576  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.125971  [    0/48000]\n",
            "loss: 0.079859  [ 6400/48000]\n",
            "loss: 0.039227  [12800/48000]\n",
            "loss: 0.027607  [19200/48000]\n",
            "loss: 0.024778  [25600/48000]\n",
            "loss: 0.054385  [32000/48000]\n",
            "loss: 0.028790  [38400/48000]\n",
            "loss: 0.039646  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.109335  [    0/48000]\n",
            "loss: 0.063918  [ 6400/48000]\n",
            "loss: 0.027335  [12800/48000]\n",
            "loss: 0.022138  [19200/48000]\n",
            "loss: 0.017776  [25600/48000]\n",
            "loss: 0.045601  [32000/48000]\n",
            "loss: 0.020862  [38400/48000]\n",
            "loss: 0.033615  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.092886  [    0/48000]\n",
            "loss: 0.051100  [ 6400/48000]\n",
            "loss: 0.020602  [12800/48000]\n",
            "loss: 0.018858  [19200/48000]\n",
            "loss: 0.013107  [25600/48000]\n",
            "loss: 0.035510  [32000/48000]\n",
            "loss: 0.016547  [38400/48000]\n",
            "loss: 0.028758  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.077036  [    0/48000]\n",
            "loss: 0.042616  [ 6400/48000]\n",
            "loss: 0.017312  [12800/48000]\n",
            "loss: 0.016377  [19200/48000]\n",
            "loss: 0.010208  [25600/48000]\n",
            "loss: 0.026569  [32000/48000]\n",
            "loss: 0.013793  [38400/48000]\n",
            "loss: 0.024919  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.062417  [    0/48000]\n",
            "loss: 0.036382  [ 6400/48000]\n",
            "loss: 0.014271  [12800/48000]\n",
            "loss: 0.014665  [19200/48000]\n",
            "loss: 0.008793  [25600/48000]\n",
            "loss: 0.020177  [32000/48000]\n",
            "loss: 0.011732  [38400/48000]\n",
            "loss: 0.021329  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 0.103077 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=leaky_relu, weight_decay=0.0001, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.201190  [    0/48000]\n",
            "loss: 0.636803  [ 6400/48000]\n",
            "loss: 0.260773  [12800/48000]\n",
            "loss: 0.357693  [19200/48000]\n",
            "loss: 0.208159  [25600/48000]\n",
            "loss: 0.222486  [32000/48000]\n",
            "loss: 0.151792  [38400/48000]\n",
            "loss: 0.221417  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.196669  [    0/48000]\n",
            "loss: 0.287700  [ 6400/48000]\n",
            "loss: 0.148627  [12800/48000]\n",
            "loss: 0.213767  [19200/48000]\n",
            "loss: 0.113856  [25600/48000]\n",
            "loss: 0.162759  [32000/48000]\n",
            "loss: 0.088192  [38400/48000]\n",
            "loss: 0.172928  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.166366  [    0/48000]\n",
            "loss: 0.216564  [ 6400/48000]\n",
            "loss: 0.118221  [12800/48000]\n",
            "loss: 0.149740  [19200/48000]\n",
            "loss: 0.083458  [25600/48000]\n",
            "loss: 0.143312  [32000/48000]\n",
            "loss: 0.060201  [38400/48000]\n",
            "loss: 0.137428  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.152957  [    0/48000]\n",
            "loss: 0.174631  [ 6400/48000]\n",
            "loss: 0.095013  [12800/48000]\n",
            "loss: 0.110975  [19200/48000]\n",
            "loss: 0.070941  [25600/48000]\n",
            "loss: 0.127703  [32000/48000]\n",
            "loss: 0.049554  [38400/48000]\n",
            "loss: 0.108369  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.140603  [    0/48000]\n",
            "loss: 0.148603  [ 6400/48000]\n",
            "loss: 0.071301  [12800/48000]\n",
            "loss: 0.082137  [19200/48000]\n",
            "loss: 0.062799  [25600/48000]\n",
            "loss: 0.110087  [32000/48000]\n",
            "loss: 0.042244  [38400/48000]\n",
            "loss: 0.079492  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.129899  [    0/48000]\n",
            "loss: 0.126790  [ 6400/48000]\n",
            "loss: 0.052494  [12800/48000]\n",
            "loss: 0.063078  [19200/48000]\n",
            "loss: 0.057229  [25600/48000]\n",
            "loss: 0.095233  [32000/48000]\n",
            "loss: 0.034141  [38400/48000]\n",
            "loss: 0.056942  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.115906  [    0/48000]\n",
            "loss: 0.109554  [ 6400/48000]\n",
            "loss: 0.039344  [12800/48000]\n",
            "loss: 0.048171  [19200/48000]\n",
            "loss: 0.047459  [25600/48000]\n",
            "loss: 0.079843  [32000/48000]\n",
            "loss: 0.028965  [38400/48000]\n",
            "loss: 0.040441  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.103525  [    0/48000]\n",
            "loss: 0.091708  [ 6400/48000]\n",
            "loss: 0.032563  [12800/48000]\n",
            "loss: 0.038283  [19200/48000]\n",
            "loss: 0.039516  [25600/48000]\n",
            "loss: 0.065918  [32000/48000]\n",
            "loss: 0.023719  [38400/48000]\n",
            "loss: 0.029049  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.093009  [    0/48000]\n",
            "loss: 0.077740  [ 6400/48000]\n",
            "loss: 0.027099  [12800/48000]\n",
            "loss: 0.029865  [19200/48000]\n",
            "loss: 0.031299  [25600/48000]\n",
            "loss: 0.054034  [32000/48000]\n",
            "loss: 0.020360  [38400/48000]\n",
            "loss: 0.022298  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.082415  [    0/48000]\n",
            "loss: 0.066868  [ 6400/48000]\n",
            "loss: 0.023941  [12800/48000]\n",
            "loss: 0.022503  [19200/48000]\n",
            "loss: 0.026691  [25600/48000]\n",
            "loss: 0.042058  [32000/48000]\n",
            "loss: 0.016988  [38400/48000]\n",
            "loss: 0.018364  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.072817  [    0/48000]\n",
            "loss: 0.056577  [ 6400/48000]\n",
            "loss: 0.022388  [12800/48000]\n",
            "loss: 0.017980  [19200/48000]\n",
            "loss: 0.021787  [25600/48000]\n",
            "loss: 0.033009  [32000/48000]\n",
            "loss: 0.014934  [38400/48000]\n",
            "loss: 0.015581  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.064419  [    0/48000]\n",
            "loss: 0.048873  [ 6400/48000]\n",
            "loss: 0.020813  [12800/48000]\n",
            "loss: 0.014860  [19200/48000]\n",
            "loss: 0.018287  [25600/48000]\n",
            "loss: 0.026091  [32000/48000]\n",
            "loss: 0.013008  [38400/48000]\n",
            "loss: 0.013616  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.055109  [    0/48000]\n",
            "loss: 0.041119  [ 6400/48000]\n",
            "loss: 0.019273  [12800/48000]\n",
            "loss: 0.012104  [19200/48000]\n",
            "loss: 0.015719  [25600/48000]\n",
            "loss: 0.020640  [32000/48000]\n",
            "loss: 0.011623  [38400/48000]\n",
            "loss: 0.012017  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.046878  [    0/48000]\n",
            "loss: 0.036093  [ 6400/48000]\n",
            "loss: 0.018378  [12800/48000]\n",
            "loss: 0.010550  [19200/48000]\n",
            "loss: 0.013021  [25600/48000]\n",
            "loss: 0.016798  [32000/48000]\n",
            "loss: 0.010176  [38400/48000]\n",
            "loss: 0.010768  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.040316  [    0/48000]\n",
            "loss: 0.030605  [ 6400/48000]\n",
            "loss: 0.016301  [12800/48000]\n",
            "loss: 0.008885  [19200/48000]\n",
            "loss: 0.011005  [25600/48000]\n",
            "loss: 0.014263  [32000/48000]\n",
            "loss: 0.008615  [38400/48000]\n",
            "loss: 0.009489  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.033917  [    0/48000]\n",
            "loss: 0.026757  [ 6400/48000]\n",
            "loss: 0.014515  [12800/48000]\n",
            "loss: 0.007782  [19200/48000]\n",
            "loss: 0.009414  [25600/48000]\n",
            "loss: 0.012354  [32000/48000]\n",
            "loss: 0.007636  [38400/48000]\n",
            "loss: 0.008522  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.028498  [    0/48000]\n",
            "loss: 0.023380  [ 6400/48000]\n",
            "loss: 0.012540  [12800/48000]\n",
            "loss: 0.006767  [19200/48000]\n",
            "loss: 0.008329  [25600/48000]\n",
            "loss: 0.011086  [32000/48000]\n",
            "loss: 0.006790  [38400/48000]\n",
            "loss: 0.007761  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.024646  [    0/48000]\n",
            "loss: 0.020619  [ 6400/48000]\n",
            "loss: 0.011205  [12800/48000]\n",
            "loss: 0.006124  [19200/48000]\n",
            "loss: 0.007385  [25600/48000]\n",
            "loss: 0.010003  [32000/48000]\n",
            "loss: 0.006130  [38400/48000]\n",
            "loss: 0.007134  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.021244  [    0/48000]\n",
            "loss: 0.018474  [ 6400/48000]\n",
            "loss: 0.009697  [12800/48000]\n",
            "loss: 0.005443  [19200/48000]\n",
            "loss: 0.006625  [25600/48000]\n",
            "loss: 0.009319  [32000/48000]\n",
            "loss: 0.005516  [38400/48000]\n",
            "loss: 0.006557  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.018582  [    0/48000]\n",
            "loss: 0.016641  [ 6400/48000]\n",
            "loss: 0.008547  [12800/48000]\n",
            "loss: 0.004976  [19200/48000]\n",
            "loss: 0.005797  [25600/48000]\n",
            "loss: 0.008627  [32000/48000]\n",
            "loss: 0.004930  [38400/48000]\n",
            "loss: 0.006073  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 0.104724 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "New Best model: {'num_layers': 4, 'layer_size': 512, 'activation': 'leaky_relu', 'weight_decay': 0.0001, 'epochs': 20} with accuracy 0.9715\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=leaky_relu, weight_decay=0.001, epochs=5\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 4.414824  [    0/48000]\n",
            "loss: 0.675635  [ 6400/48000]\n",
            "loss: 0.340706  [12800/48000]\n",
            "loss: 0.370533  [19200/48000]\n",
            "loss: 0.211342  [25600/48000]\n",
            "loss: 0.119817  [32000/48000]\n",
            "loss: 0.183721  [38400/48000]\n",
            "loss: 0.263107  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.252044  [    0/48000]\n",
            "loss: 0.233097  [ 6400/48000]\n",
            "loss: 0.157702  [12800/48000]\n",
            "loss: 0.160953  [19200/48000]\n",
            "loss: 0.103783  [25600/48000]\n",
            "loss: 0.053866  [32000/48000]\n",
            "loss: 0.083587  [38400/48000]\n",
            "loss: 0.169843  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.219408  [    0/48000]\n",
            "loss: 0.172037  [ 6400/48000]\n",
            "loss: 0.105813  [12800/48000]\n",
            "loss: 0.087674  [19200/48000]\n",
            "loss: 0.072375  [25600/48000]\n",
            "loss: 0.040401  [32000/48000]\n",
            "loss: 0.052238  [38400/48000]\n",
            "loss: 0.122956  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.191363  [    0/48000]\n",
            "loss: 0.134125  [ 6400/48000]\n",
            "loss: 0.069701  [12800/48000]\n",
            "loss: 0.056752  [19200/48000]\n",
            "loss: 0.058035  [25600/48000]\n",
            "loss: 0.035351  [32000/48000]\n",
            "loss: 0.041041  [38400/48000]\n",
            "loss: 0.092253  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.169877  [    0/48000]\n",
            "loss: 0.110316  [ 6400/48000]\n",
            "loss: 0.050005  [12800/48000]\n",
            "loss: 0.042891  [19200/48000]\n",
            "loss: 0.047722  [25600/48000]\n",
            "loss: 0.030658  [32000/48000]\n",
            "loss: 0.034113  [38400/48000]\n",
            "loss: 0.070879  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.1%, Avg loss: 0.127239 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=leaky_relu, weight_decay=0.001, epochs=10\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 3.924651  [    0/48000]\n",
            "loss: 0.591870  [ 6400/48000]\n",
            "loss: 0.212772  [12800/48000]\n",
            "loss: 0.308740  [19200/48000]\n",
            "loss: 0.298577  [25600/48000]\n",
            "loss: 0.236904  [32000/48000]\n",
            "loss: 0.189626  [38400/48000]\n",
            "loss: 0.241063  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.192372  [    0/48000]\n",
            "loss: 0.236654  [ 6400/48000]\n",
            "loss: 0.077219  [12800/48000]\n",
            "loss: 0.163555  [19200/48000]\n",
            "loss: 0.126363  [25600/48000]\n",
            "loss: 0.118713  [32000/48000]\n",
            "loss: 0.115253  [38400/48000]\n",
            "loss: 0.170497  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.149075  [    0/48000]\n",
            "loss: 0.171387  [ 6400/48000]\n",
            "loss: 0.062000  [12800/48000]\n",
            "loss: 0.095371  [19200/48000]\n",
            "loss: 0.081032  [25600/48000]\n",
            "loss: 0.086804  [32000/48000]\n",
            "loss: 0.084881  [38400/48000]\n",
            "loss: 0.114541  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.135990  [    0/48000]\n",
            "loss: 0.134147  [ 6400/48000]\n",
            "loss: 0.049340  [12800/48000]\n",
            "loss: 0.062568  [19200/48000]\n",
            "loss: 0.064615  [25600/48000]\n",
            "loss: 0.069615  [32000/48000]\n",
            "loss: 0.068394  [38400/48000]\n",
            "loss: 0.075789  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.121542  [    0/48000]\n",
            "loss: 0.113907  [ 6400/48000]\n",
            "loss: 0.039282  [12800/48000]\n",
            "loss: 0.050437  [19200/48000]\n",
            "loss: 0.054391  [25600/48000]\n",
            "loss: 0.056019  [32000/48000]\n",
            "loss: 0.056234  [38400/48000]\n",
            "loss: 0.055903  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.107818  [    0/48000]\n",
            "loss: 0.096794  [ 6400/48000]\n",
            "loss: 0.031685  [12800/48000]\n",
            "loss: 0.041069  [19200/48000]\n",
            "loss: 0.048852  [25600/48000]\n",
            "loss: 0.046663  [32000/48000]\n",
            "loss: 0.044626  [38400/48000]\n",
            "loss: 0.043081  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.094473  [    0/48000]\n",
            "loss: 0.081771  [ 6400/48000]\n",
            "loss: 0.027391  [12800/48000]\n",
            "loss: 0.036595  [19200/48000]\n",
            "loss: 0.041616  [25600/48000]\n",
            "loss: 0.038641  [32000/48000]\n",
            "loss: 0.035812  [38400/48000]\n",
            "loss: 0.034064  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.083627  [    0/48000]\n",
            "loss: 0.072685  [ 6400/48000]\n",
            "loss: 0.023097  [12800/48000]\n",
            "loss: 0.033092  [19200/48000]\n",
            "loss: 0.036364  [25600/48000]\n",
            "loss: 0.032612  [32000/48000]\n",
            "loss: 0.029203  [38400/48000]\n",
            "loss: 0.028672  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.075142  [    0/48000]\n",
            "loss: 0.064454  [ 6400/48000]\n",
            "loss: 0.020247  [12800/48000]\n",
            "loss: 0.029840  [19200/48000]\n",
            "loss: 0.031357  [25600/48000]\n",
            "loss: 0.026891  [32000/48000]\n",
            "loss: 0.024127  [38400/48000]\n",
            "loss: 0.024659  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.066728  [    0/48000]\n",
            "loss: 0.059179  [ 6400/48000]\n",
            "loss: 0.018311  [12800/48000]\n",
            "loss: 0.026792  [19200/48000]\n",
            "loss: 0.027501  [25600/48000]\n",
            "loss: 0.022418  [32000/48000]\n",
            "loss: 0.019732  [38400/48000]\n",
            "loss: 0.021293  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.107752 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "Training with: Layers=4, Batch_Size=512, Activation=leaky_relu, weight_decay=0.001, epochs=20\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.559811  [    0/48000]\n",
            "loss: 0.615557  [ 6400/48000]\n",
            "loss: 0.405260  [12800/48000]\n",
            "loss: 0.408423  [19200/48000]\n",
            "loss: 0.356533  [25600/48000]\n",
            "loss: 0.223628  [32000/48000]\n",
            "loss: 0.120362  [38400/48000]\n",
            "loss: 0.281636  [44800/48000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.210830  [    0/48000]\n",
            "loss: 0.258828  [ 6400/48000]\n",
            "loss: 0.184840  [12800/48000]\n",
            "loss: 0.220194  [19200/48000]\n",
            "loss: 0.167731  [25600/48000]\n",
            "loss: 0.124628  [32000/48000]\n",
            "loss: 0.061025  [38400/48000]\n",
            "loss: 0.193651  [44800/48000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.173210  [    0/48000]\n",
            "loss: 0.198795  [ 6400/48000]\n",
            "loss: 0.134815  [12800/48000]\n",
            "loss: 0.148068  [19200/48000]\n",
            "loss: 0.095835  [25600/48000]\n",
            "loss: 0.099436  [32000/48000]\n",
            "loss: 0.051308  [38400/48000]\n",
            "loss: 0.139750  [44800/48000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.159152  [    0/48000]\n",
            "loss: 0.165670  [ 6400/48000]\n",
            "loss: 0.099524  [12800/48000]\n",
            "loss: 0.114792  [19200/48000]\n",
            "loss: 0.067812  [25600/48000]\n",
            "loss: 0.081859  [32000/48000]\n",
            "loss: 0.045552  [38400/48000]\n",
            "loss: 0.106812  [44800/48000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.141986  [    0/48000]\n",
            "loss: 0.138782  [ 6400/48000]\n",
            "loss: 0.074486  [12800/48000]\n",
            "loss: 0.090342  [19200/48000]\n",
            "loss: 0.052925  [25600/48000]\n",
            "loss: 0.065255  [32000/48000]\n",
            "loss: 0.037198  [38400/48000]\n",
            "loss: 0.081995  [44800/48000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.127904  [    0/48000]\n",
            "loss: 0.121299  [ 6400/48000]\n",
            "loss: 0.056576  [12800/48000]\n",
            "loss: 0.070876  [19200/48000]\n",
            "loss: 0.042381  [25600/48000]\n",
            "loss: 0.053381  [32000/48000]\n",
            "loss: 0.029740  [38400/48000]\n",
            "loss: 0.063195  [44800/48000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.113750  [    0/48000]\n",
            "loss: 0.105688  [ 6400/48000]\n",
            "loss: 0.043678  [12800/48000]\n",
            "loss: 0.056360  [19200/48000]\n",
            "loss: 0.034322  [25600/48000]\n",
            "loss: 0.042455  [32000/48000]\n",
            "loss: 0.024161  [38400/48000]\n",
            "loss: 0.050635  [44800/48000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.102760  [    0/48000]\n",
            "loss: 0.090975  [ 6400/48000]\n",
            "loss: 0.033242  [12800/48000]\n",
            "loss: 0.044938  [19200/48000]\n",
            "loss: 0.028808  [25600/48000]\n",
            "loss: 0.034788  [32000/48000]\n",
            "loss: 0.019840  [38400/48000]\n",
            "loss: 0.042499  [44800/48000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.091731  [    0/48000]\n",
            "loss: 0.077603  [ 6400/48000]\n",
            "loss: 0.026492  [12800/48000]\n",
            "loss: 0.035342  [19200/48000]\n",
            "loss: 0.024323  [25600/48000]\n",
            "loss: 0.028278  [32000/48000]\n",
            "loss: 0.017030  [38400/48000]\n",
            "loss: 0.035570  [44800/48000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.081787  [    0/48000]\n",
            "loss: 0.068609  [ 6400/48000]\n",
            "loss: 0.021853  [12800/48000]\n",
            "loss: 0.027347  [19200/48000]\n",
            "loss: 0.020945  [25600/48000]\n",
            "loss: 0.023585  [32000/48000]\n",
            "loss: 0.014264  [38400/48000]\n",
            "loss: 0.031340  [44800/48000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.072228  [    0/48000]\n",
            "loss: 0.060249  [ 6400/48000]\n",
            "loss: 0.018559  [12800/48000]\n",
            "loss: 0.022331  [19200/48000]\n",
            "loss: 0.017374  [25600/48000]\n",
            "loss: 0.019421  [32000/48000]\n",
            "loss: 0.012126  [38400/48000]\n",
            "loss: 0.027931  [44800/48000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.063796  [    0/48000]\n",
            "loss: 0.052658  [ 6400/48000]\n",
            "loss: 0.016267  [12800/48000]\n",
            "loss: 0.018505  [19200/48000]\n",
            "loss: 0.014621  [25600/48000]\n",
            "loss: 0.016762  [32000/48000]\n",
            "loss: 0.010751  [38400/48000]\n",
            "loss: 0.025069  [44800/48000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.053964  [    0/48000]\n",
            "loss: 0.046098  [ 6400/48000]\n",
            "loss: 0.014307  [12800/48000]\n",
            "loss: 0.015501  [19200/48000]\n",
            "loss: 0.012899  [25600/48000]\n",
            "loss: 0.014176  [32000/48000]\n",
            "loss: 0.009784  [38400/48000]\n",
            "loss: 0.021843  [44800/48000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.044137  [    0/48000]\n",
            "loss: 0.040492  [ 6400/48000]\n",
            "loss: 0.012256  [12800/48000]\n",
            "loss: 0.013573  [19200/48000]\n",
            "loss: 0.011314  [25600/48000]\n",
            "loss: 0.012186  [32000/48000]\n",
            "loss: 0.008923  [38400/48000]\n",
            "loss: 0.019138  [44800/48000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.035910  [    0/48000]\n",
            "loss: 0.035136  [ 6400/48000]\n",
            "loss: 0.010564  [12800/48000]\n",
            "loss: 0.012101  [19200/48000]\n",
            "loss: 0.010058  [25600/48000]\n",
            "loss: 0.010598  [32000/48000]\n",
            "loss: 0.008243  [38400/48000]\n",
            "loss: 0.017170  [44800/48000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.028411  [    0/48000]\n",
            "loss: 0.031550  [ 6400/48000]\n",
            "loss: 0.009219  [12800/48000]\n",
            "loss: 0.010715  [19200/48000]\n",
            "loss: 0.009026  [25600/48000]\n",
            "loss: 0.009551  [32000/48000]\n",
            "loss: 0.007610  [38400/48000]\n",
            "loss: 0.015391  [44800/48000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.022488  [    0/48000]\n",
            "loss: 0.028172  [ 6400/48000]\n",
            "loss: 0.008001  [12800/48000]\n",
            "loss: 0.009670  [19200/48000]\n",
            "loss: 0.007865  [25600/48000]\n",
            "loss: 0.008681  [32000/48000]\n",
            "loss: 0.007223  [38400/48000]\n",
            "loss: 0.013807  [44800/48000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.018239  [    0/48000]\n",
            "loss: 0.024914  [ 6400/48000]\n",
            "loss: 0.007062  [12800/48000]\n",
            "loss: 0.008716  [19200/48000]\n",
            "loss: 0.007224  [25600/48000]\n",
            "loss: 0.007998  [32000/48000]\n",
            "loss: 0.006650  [38400/48000]\n",
            "loss: 0.012271  [44800/48000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.015466  [    0/48000]\n",
            "loss: 0.022555  [ 6400/48000]\n",
            "loss: 0.006077  [12800/48000]\n",
            "loss: 0.007948  [19200/48000]\n",
            "loss: 0.006462  [25600/48000]\n",
            "loss: 0.007496  [32000/48000]\n",
            "loss: 0.006170  [38400/48000]\n",
            "loss: 0.011062  [44800/48000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.013473  [    0/48000]\n",
            "loss: 0.020652  [ 6400/48000]\n",
            "loss: 0.005394  [12800/48000]\n",
            "loss: 0.007164  [19200/48000]\n",
            "loss: 0.005861  [25600/48000]\n",
            "loss: 0.007013  [32000/48000]\n",
            "loss: 0.005691  [38400/48000]\n",
            "loss: 0.009987  [44800/48000]\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 0.099309 \n",
            "\n",
            "\n",
            "-------------------------------\n",
            "New Best model: {'num_layers': 4, 'layer_size': 512, 'activation': 'leaky_relu', 'weight_decay': 0.001, 'epochs': 20} with accuracy 0.9719166666666667\n",
            "Best Hyperparameters: {'num_layers': 4, 'layer_size': 512, 'activation': 'leaky_relu', 'weight_decay': 0.001, 'epochs': 20} with accuracy 0.9719166666666667\n"
          ]
        }
      ],
      "source": [
        "best_accuracy = 0\n",
        "best_hyperparams = {}\n",
        "\n",
        "# Iterate over each combination of hyperparameters\n",
        "for num_layers in hyperparameters['num_layers']:\n",
        "    for layer_size in hyperparameters['layer_size']:\n",
        "        for activation in hyperparameters['activation']:\n",
        "            for weight_decay in hyperparameters['weight_decay']:\n",
        "                for epochs in hyperparameters['epochs']:\n",
        "                    print(f\"\\n-------------------------------\")\n",
        "                    print(f\"Training with: Layers={num_layers}, Batch_Size={layer_size}, Activation={activation}, weight_decay={weight_decay}, epochs={epochs}\")\n",
        "                    model = NeuralNetwork(num_layers, layer_size, activation).to(device)\n",
        "                    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n",
        "                    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "                    for epoch in range(epochs):\n",
        "                        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "                        train(train_dataloader, model, loss_fn, optimizer)\n",
        "                    accuracy = test(vali_dataloader, model, loss_fn)\n",
        "\n",
        "                    if accuracy > best_accuracy:\n",
        "                        best_accuracy = accuracy\n",
        "                        best_hyperparams = {\n",
        "                            'num_layers': num_layers,\n",
        "                            'layer_size': layer_size,\n",
        "                            'activation': activation,\n",
        "                            'weight_decay': weight_decay,\n",
        "                            'epochs': epochs\n",
        "                        }\n",
        "                        print(f\"\\n-------------------------------\")\n",
        "                        print(f\"New Best model: {best_hyperparams} with accuracy {best_accuracy}\")\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_hyperparams} with accuracy {best_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE_WS8qB3UR_",
        "outputId": "1e5557f9-0c5b-4837-eb40-0a95b1d79397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "loss: 5.395525  [    0/48000]\n",
            "loss: 0.484484  [ 6400/48000]\n",
            "loss: 0.278851  [12800/48000]\n",
            "loss: 0.276068  [19200/48000]\n",
            "loss: 0.409283  [25600/48000]\n",
            "loss: 0.189501  [32000/48000]\n",
            "loss: 0.201966  [38400/48000]\n",
            "loss: 0.194260  [44800/48000]\n",
            "Epoch 2/20\n",
            "loss: 0.273310  [    0/48000]\n",
            "loss: 0.221075  [ 6400/48000]\n",
            "loss: 0.147200  [12800/48000]\n",
            "loss: 0.148058  [19200/48000]\n",
            "loss: 0.244193  [25600/48000]\n",
            "loss: 0.111907  [32000/48000]\n",
            "loss: 0.100699  [38400/48000]\n",
            "loss: 0.142313  [44800/48000]\n",
            "Epoch 3/20\n",
            "loss: 0.232457  [    0/48000]\n",
            "loss: 0.162505  [ 6400/48000]\n",
            "loss: 0.100193  [12800/48000]\n",
            "loss: 0.076236  [19200/48000]\n",
            "loss: 0.160131  [25600/48000]\n",
            "loss: 0.081535  [32000/48000]\n",
            "loss: 0.063013  [38400/48000]\n",
            "loss: 0.110658  [44800/48000]\n",
            "Epoch 4/20\n",
            "loss: 0.199585  [    0/48000]\n",
            "loss: 0.132268  [ 6400/48000]\n",
            "loss: 0.069686  [12800/48000]\n",
            "loss: 0.049720  [19200/48000]\n",
            "loss: 0.116422  [25600/48000]\n",
            "loss: 0.059757  [32000/48000]\n",
            "loss: 0.045869  [38400/48000]\n",
            "loss: 0.086401  [44800/48000]\n",
            "Epoch 5/20\n",
            "loss: 0.168962  [    0/48000]\n",
            "loss: 0.111395  [ 6400/48000]\n",
            "loss: 0.050277  [12800/48000]\n",
            "loss: 0.036611  [19200/48000]\n",
            "loss: 0.091463  [25600/48000]\n",
            "loss: 0.042442  [32000/48000]\n",
            "loss: 0.038425  [38400/48000]\n",
            "loss: 0.068730  [44800/48000]\n",
            "Epoch 6/20\n",
            "loss: 0.142629  [    0/48000]\n",
            "loss: 0.092793  [ 6400/48000]\n",
            "loss: 0.035581  [12800/48000]\n",
            "loss: 0.028309  [19200/48000]\n",
            "loss: 0.071787  [25600/48000]\n",
            "loss: 0.031175  [32000/48000]\n",
            "loss: 0.030600  [38400/48000]\n",
            "loss: 0.053734  [44800/48000]\n",
            "Epoch 7/20\n",
            "loss: 0.120329  [    0/48000]\n",
            "loss: 0.078366  [ 6400/48000]\n",
            "loss: 0.026039  [12800/48000]\n",
            "loss: 0.021752  [19200/48000]\n",
            "loss: 0.059216  [25600/48000]\n",
            "loss: 0.024602  [32000/48000]\n",
            "loss: 0.026050  [38400/48000]\n",
            "loss: 0.045400  [44800/48000]\n",
            "Epoch 8/20\n",
            "loss: 0.104667  [    0/48000]\n",
            "loss: 0.068342  [ 6400/48000]\n",
            "loss: 0.020108  [12800/48000]\n",
            "loss: 0.018466  [19200/48000]\n",
            "loss: 0.048489  [25600/48000]\n",
            "loss: 0.020032  [32000/48000]\n",
            "loss: 0.021866  [38400/48000]\n",
            "loss: 0.039719  [44800/48000]\n",
            "Epoch 9/20\n",
            "loss: 0.088211  [    0/48000]\n",
            "loss: 0.059514  [ 6400/48000]\n",
            "loss: 0.016634  [12800/48000]\n",
            "loss: 0.015617  [19200/48000]\n",
            "loss: 0.038986  [25600/48000]\n",
            "loss: 0.016222  [32000/48000]\n",
            "loss: 0.018554  [38400/48000]\n",
            "loss: 0.035706  [44800/48000]\n",
            "Epoch 10/20\n",
            "loss: 0.074465  [    0/48000]\n",
            "loss: 0.052109  [ 6400/48000]\n",
            "loss: 0.014301  [12800/48000]\n",
            "loss: 0.013826  [19200/48000]\n",
            "loss: 0.032479  [25600/48000]\n",
            "loss: 0.014370  [32000/48000]\n",
            "loss: 0.015420  [38400/48000]\n",
            "loss: 0.031734  [44800/48000]\n",
            "Epoch 11/20\n",
            "loss: 0.064103  [    0/48000]\n",
            "loss: 0.044771  [ 6400/48000]\n",
            "loss: 0.012431  [12800/48000]\n",
            "loss: 0.011954  [19200/48000]\n",
            "loss: 0.028215  [25600/48000]\n",
            "loss: 0.012435  [32000/48000]\n",
            "loss: 0.013134  [38400/48000]\n",
            "loss: 0.028701  [44800/48000]\n",
            "Epoch 12/20\n",
            "loss: 0.054855  [    0/48000]\n",
            "loss: 0.039602  [ 6400/48000]\n",
            "loss: 0.011065  [12800/48000]\n",
            "loss: 0.010805  [19200/48000]\n",
            "loss: 0.024484  [25600/48000]\n",
            "loss: 0.011166  [32000/48000]\n",
            "loss: 0.011562  [38400/48000]\n",
            "loss: 0.025693  [44800/48000]\n",
            "Epoch 13/20\n",
            "loss: 0.048797  [    0/48000]\n",
            "loss: 0.035077  [ 6400/48000]\n",
            "loss: 0.009694  [12800/48000]\n",
            "loss: 0.009948  [19200/48000]\n",
            "loss: 0.020909  [25600/48000]\n",
            "loss: 0.009971  [32000/48000]\n",
            "loss: 0.009739  [38400/48000]\n",
            "loss: 0.023238  [44800/48000]\n",
            "Epoch 14/20\n",
            "loss: 0.041095  [    0/48000]\n",
            "loss: 0.031417  [ 6400/48000]\n",
            "loss: 0.008922  [12800/48000]\n",
            "loss: 0.008850  [19200/48000]\n",
            "loss: 0.017777  [25600/48000]\n",
            "loss: 0.009370  [32000/48000]\n",
            "loss: 0.008704  [38400/48000]\n",
            "loss: 0.020896  [44800/48000]\n",
            "Epoch 15/20\n",
            "loss: 0.034666  [    0/48000]\n",
            "loss: 0.027601  [ 6400/48000]\n",
            "loss: 0.008092  [12800/48000]\n",
            "loss: 0.008188  [19200/48000]\n",
            "loss: 0.015687  [25600/48000]\n",
            "loss: 0.008588  [32000/48000]\n",
            "loss: 0.007817  [38400/48000]\n",
            "loss: 0.018564  [44800/48000]\n",
            "Epoch 16/20\n",
            "loss: 0.028875  [    0/48000]\n",
            "loss: 0.024885  [ 6400/48000]\n",
            "loss: 0.007431  [12800/48000]\n",
            "loss: 0.007510  [19200/48000]\n",
            "loss: 0.013178  [25600/48000]\n",
            "loss: 0.007852  [32000/48000]\n",
            "loss: 0.007071  [38400/48000]\n",
            "loss: 0.016549  [44800/48000]\n",
            "Epoch 17/20\n",
            "loss: 0.025056  [    0/48000]\n",
            "loss: 0.022709  [ 6400/48000]\n",
            "loss: 0.006719  [12800/48000]\n",
            "loss: 0.006778  [19200/48000]\n",
            "loss: 0.011614  [25600/48000]\n",
            "loss: 0.007378  [32000/48000]\n",
            "loss: 0.006483  [38400/48000]\n",
            "loss: 0.014636  [44800/48000]\n",
            "Epoch 18/20\n",
            "loss: 0.022074  [    0/48000]\n",
            "loss: 0.020111  [ 6400/48000]\n",
            "loss: 0.006111  [12800/48000]\n",
            "loss: 0.006403  [19200/48000]\n",
            "loss: 0.010215  [25600/48000]\n",
            "loss: 0.006881  [32000/48000]\n",
            "loss: 0.005986  [38400/48000]\n",
            "loss: 0.013223  [44800/48000]\n",
            "Epoch 19/20\n",
            "loss: 0.019652  [    0/48000]\n",
            "loss: 0.018661  [ 6400/48000]\n",
            "loss: 0.005676  [12800/48000]\n",
            "loss: 0.005849  [19200/48000]\n",
            "loss: 0.009279  [25600/48000]\n",
            "loss: 0.006466  [32000/48000]\n",
            "loss: 0.005496  [38400/48000]\n",
            "loss: 0.011877  [44800/48000]\n",
            "Epoch 20/20\n",
            "loss: 0.017504  [    0/48000]\n",
            "loss: 0.016991  [ 6400/48000]\n",
            "loss: 0.005158  [12800/48000]\n",
            "loss: 0.005528  [19200/48000]\n",
            "loss: 0.008336  [25600/48000]\n",
            "loss: 0.006153  [32000/48000]\n",
            "loss: 0.005240  [38400/48000]\n",
            "loss: 0.011000  [44800/48000]\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork(\n",
        "    num_layers=best_hyperparams['num_layers'],\n",
        "    layer_size=best_hyperparams['layer_size'],\n",
        "    activation=best_hyperparams['activation']\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=best_hyperparams['weight_decay'])\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "epochs = best_hyperparams['epochs']\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57mHLDsDq6_6",
        "outputId": "b8f9c7e5-e250-42df-9b56-0fac31339f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results: \n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 0.105325 \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9691666666666666"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Validation Results: \")\n",
        "test(vali_dataloader, model, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uowfGUW9XVt"
      },
      "outputs": [],
      "source": [
        "test_tensor = torch.tensor(test_features).to(device).float()\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  raw_pred = model(test_tensor)\n",
        "  pred = np.argmax(raw_pred.cpu().numpy(), axis = 1)\n",
        "\n",
        "\n",
        "np.savetxt(\"/content/drive/MyDrive/CS_584/hw2_Miner2.txt\", pred, fmt= \"%d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fjBLR0w2cjM"
      },
      "source": [
        "### Question: What is your final hyperparameter setting? How do you tune them? What choices have you tried?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XfwgKzC2cjM"
      },
      "source": [
        "**Final Hyperparameter Setting** :\n",
        "\n",
        "- Number of layers: 4\n",
        "- Batch Size: 512\n",
        "- Activation: leaky_relu\n",
        "- Weight Decay: 0.01\n",
        "- Epochs: 20\n",
        "\n",
        "**Choices Tried** :\n",
        "- 'num_layers': [2, 3, 4]\n",
        "- 'layer_size': [128, 256, 512]\n",
        "- 'activation': ['relu', 'sigmoid', 'tanh', 'leaky_relu']  \n",
        "- 'weight_decay': [0, 1e-4, 1e-3]\n",
        "- 'epochs' (Number of epochs): [5, 10, 20]  \n",
        "\n",
        "**How do you tune them** :\n",
        "- I am implementing hyperparameter tuning for a neural network using grid search approach. I have defined a range of options for no. of layers, size of each layer, activation functions, weight decay values and epochs.\n",
        "- For each combination of these hyperparameters, it constructs a new neural network model, trains it on the dataset and evaluates its performance on a validation set.\n",
        "- This process records the combination that yields the highest validation accuracy, treating it as the best set of hyperparameters.\n",
        "- After iterating through all combinations and identifying the best hyperparameters, we train on the test data and find the accuracy of the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LysW8dE52cjN"
      },
      "source": [
        "### Question: your username in Miner2 and the score&ranking of your submission in Miner2 (at the time of answering this question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvg0JXBf2cjN"
      },
      "source": [
        "\n",
        "- **Username**: Knock\n",
        "- **Score**: 0.97\n",
        "- **Ranking**: 46"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
